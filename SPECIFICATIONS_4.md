# üîß Sp√©cifications Techniques - Ensemble 4

## üéØ T√¢che #21 : T√©l√©m√©trie Structur√©e Am√©lior√©e

### Contexte
Le syst√®me de logging actuel utilise `console.log()` basique, ce qui ne permet pas une exploitation efficace des logs en production. Nous devons mettre en place un syst√®me de t√©l√©m√©trie structur√©e avec persistance, redaction, sampling et tracing.

### Objectif
Remplacer tous les `console.log()` par un `LoggerService` centralis√© qui produit des logs JSON structur√©s et production-ready.

### Sp√©cifications Techniques D√©taill√©es

#### 1. Structure des Logs
```typescript
interface LogEntry {
  timestamp: string; // ISO 8601
  level: 'DEBUG' | 'INFO' | 'WARN' | 'ERROR';
  service: string; // Nom du service √©metteur
  message: string; // Message humainement lisible
  data?: Record<string, any>; // M√©tadonn√©es structur√©es
  error?: {
    message: string;
    stack?: string;
  };
  correlationId?: string; // Pour le tracing distribu√©
}
```

#### 2. LoggerService Centralis√©
```typescript
class LoggerService {
  // Variables d'environnement avec fallback pour le navigateur
  private readonly IS_PRODUCTION = import.meta.env?.PROD || false;
  private readonly MIN_LOG_LEVEL: LogLevel = 
    (import.meta.env?.VITE_LOG_LEVEL as LogLevel) || 'INFO';

  private readonly LOG_LEVEL_PRIORITY: Record<LogLevel, number> = {
    'DEBUG': 0,
    'INFO': 1,
    'WARN': 2,
    'ERROR': 3
  };

  // Buffer pour la persistance des logs
  private logBuffer: any[] = [];
  private readonly MAX_BUFFER_SIZE = 100;
  private readonly FLUSH_INTERVAL = 10000; // 10s
  private correlationId: string | null = null;

  // Sampling pour les logs haute fr√©quence
  private logCounts: Map<string, { count: number, lastLogged: number }> = new Map();
  private readonly SAMPLE_RATE: Record<LogLevel, number> = {
    'DEBUG': 0.1,  // Log seulement 10% des DEBUG
    'INFO': 1.0,   // Log tous les INFO
    'WARN': 1.0,   // Log tous les WARN
    'ERROR': 1.0   // Log TOUJOURS les erreurs
  };

  // Redaction des donn√©es sensibles
  private readonly SENSITIVE_KEYS = ['password', 'token', 'apiKey', 'secret', 'creditCard'];
  private readonly EMAIL_REGEX = /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g;

  constructor() {
    // Flush p√©riodique
    setInterval(() => this.flushLogs(), this.FLUSH_INTERVAL);

    // Flush avant fermeture de la page
    if (typeof window !== 'undefined') {
      window.addEventListener('beforeunload', () => this.flushLogs());
    }
  }

  public setCorrelationId(id: string): void {
    this.correlationId = id;
  }

  public clearCorrelationId(): void {
    this.correlationId = null;
  }

  public debug(service: string, message: string, data?: Record<string, any>): void {
    if (this.shouldLog('DEBUG')) {
      this.log('DEBUG', { service, message, data });
    }
  }

  public info(service: string, message: string, data?: Record<string, any>): void {
    if (this.shouldLog('INFO')) {
      this.log('INFO', { service, message, data });
    }
  }

  public warn(service: string, message: string, data?: Record<string, any>): void {
    if (this.shouldLog('WARN')) {
      this.log('WARN', { service, message, data });
    }
  }

  public error(service: string, message: string, error?: Error, data?: Record<string, any>): void {
    if (this.shouldLog('ERROR')) {
      this.log('ERROR', { 
        service, 
        message, 
        data,
        error: error ? { message: error.message, stack: error.stack } : undefined
      });
    }
  }

  private shouldLog(level: LogLevel): boolean {
    return this.LOG_LEVEL_PRIORITY[level] >= 
           this.LOG_LEVEL_PRIORITY[this.MIN_LOG_LEVEL];
  }

  private shouldSample(level: LogLevel, service: string, message: string): boolean {
    const key = `${service}:${message}`;
    const now = Date.now();
    const stats = this.logCounts.get(key);

    // Toujours logger les erreurs
    if (level === 'ERROR') return true;

    if (!stats) {
      this.logCounts.set(key, { count: 1, lastLogged: now });
      return true;
    }

    stats.count++;

    // Si on a d√©j√† logg√© ce message il y a moins de 1s, sample
    if (now - stats.lastLogged < 1000) {
      return Math.random() < this.SAMPLE_RATE[level];
    }

    stats.lastLogged = now;
    return true;
  }

  private redactSensitiveData(obj: any): any {
    if (typeof obj !== 'object' || obj === null) return obj;

    if (Array.isArray(obj)) {
      return obj.map(item => this.redactSensitiveData(item));
    }

    const redacted: any = {};
    for (const [key, value] of Object.entries(obj)) {
      // Redact les cl√©s sensibles
      if (this.SENSITIVE_KEYS.some(k => key.toLowerCase().includes(k))) {
        redacted[key] = '***REDACTED***';
        continue;
      }

      // Redact les emails dans les strings
      if (typeof value === 'string') {
        redacted[key] = value.replace(this.EMAIL_REGEX, '***@***.***');
      } else {
        redacted[key] = this.redactSensitiveData(value);
      }
    }
    return redacted;
  }

  private log(level: LogLevel, payload: LogPayload): void {
    // Sampling
    if (!this.shouldSample(level, payload.service, payload.message)) {
      return; // Skip ce log
    }

    const logEntry = {
      timestamp: new Date().toISOString(),
      level,
      correlationId: this.correlationId,
      ...payload,
      data: payload.data ? this.redactSensitiveData(payload.data) : undefined
    };

    // Affiche dans la console en dev
    if (!this.IS_PRODUCTION) {
      console.log(JSON.stringify(logEntry, null, 2));
    }

    // Buffer pour persistance
    this.logBuffer.push(logEntry);

    // Flush imm√©diat si erreur critique ou buffer plein
    if (level === 'ERROR' || this.logBuffer.length >= this.MAX_BUFFER_SIZE) {
      this.flushLogs();
    }
  }

  private async flushLogs(): Promise<void> {
    if (this.logBuffer.length === 0) return;

    const logsToFlush = [...this.logBuffer];
    this.logBuffer = [];

    try {
      // Sauvegarde dans OPFS avec retry exponentiel
      const timestamp = Date.now();
      const filename = `logs/session-${timestamp}.json`;
      await this.saveWithRetry(filename, JSON.stringify(logsToFlush));

      // En prod : envoie √† un service externe
      // await fetch('https://logs.kensho.ai/ingest', {
      //   method: 'POST',
      //   body: JSON.stringify(logsToFlush)
      // });
    } catch (error) {
      console.error('[LoggerService] √âchec du flush:', error);
      // Restaure les logs dans le buffer pour retry
      this.logBuffer.unshift(...logsToFlush);
    }
  }

  // Retry exponentiel avec fallback vers in-memory storage
  private async saveWithRetry(filename: string, data: string, attempt: number = 0): Promise<void> {
    const maxRetries = 3;
    const delay = Math.pow(2, attempt) * 1000; // 1s, 2s, 4s

    try {
      await storageManager.saveFile(filename, data);
    } catch (error) {
      if (attempt < maxRetries) {
        console.warn(`[LoggerService] Retry ${attempt + 1}/${maxRetries} dans ${delay}ms`);
        await new Promise(resolve => setTimeout(resolve, delay));
        return this.saveWithRetry(filename, data, attempt + 1);
      } else {
        // Fallback vers in-memory storage
        console.error('[LoggerService] Fallback vers in-memory storage');
        // Stockage temporaire en m√©moire
        this.inMemoryStorage.set(filename, data);
        throw error;
      }
    }
  }

  // M√©thode pour r√©cup√©rer les logs historiques
  public async getHistoricalLogs(
    filters?: { level?: LogLevel, service?: string, since?: number, correlationId?: string }
  ): Promise<any[]> {
    const logFiles = await storageManager.listFiles('logs/');
    const allLogs: any[] = [];

    for (const file of logFiles) {
      try {
        const content = await storageManager.readFile(file);
        const logs = JSON.parse(content);
        allLogs.push(...logs);
      } catch (error) {
        console.error(`[LoggerService] Erreur lors de la lecture du fichier ${file}:`, error);
        // Essayer depuis le stockage temporaire
        const tempContent = this.inMemoryStorage.get(file);
        if (tempContent) {
          try {
            const logs = JSON.parse(tempContent);
            allLogs.push(...logs);
          } catch (parseError) {
            console.error(`[LoggerService] Erreur de parsing du contenu temporaire:`, parseError);
          }
        }
      }
    }

    // Applique les filtres
    return allLogs.filter(log => {
      if (filters?.level && log.level !== filters.level) return false;
      if (filters?.service && log.service !== filters.service) return false;
      if (filters?.since && new Date(log.timestamp).getTime() < filters.since) return false;
      if (filters?.correlationId && log.correlationId !== filters.correlationId) return false;
      return true;
    });
  }

  // M√©triques agr√©g√©es
  private metrics = {
    byLevel: { DEBUG: 0, INFO: 0, WARN: 0, ERROR: 0 },
    byService: new Map<string, number>(),
    errorRate: 0
  };

  public getMetrics(): any {
    return { ...this.metrics };
  }

  // AlertManager pour le syst√®me d'alerting
  private alertRules: AlertRule[] = [];
  private alertSilenceMap: Map<string, number> = new Map(); // silenceUntil timestamp

  public addAlertRule(rule: AlertRule): void {
    this.alertRules.push(rule);
  }

  public async checkAlerts(): Promise<void> {
    const metrics = this.getMetrics();
    const now = Date.now();

    for (const rule of this.alertRules) {
      // V√©rifier si l'alerte est en silence
      const silenceUntil = this.alertSilenceMap.get(rule.id) || 0;
      if (now < silenceUntil) continue;

      // √âvaluer la r√®gle
      if (rule.condition(metrics)) {
        // D√©clencher l'alerte
        await this.triggerAlert(rule, metrics);
        
        // Mettre en silence pour √©viter le spam
        if (rule.silenceDuration) {
          this.alertSilenceMap.set(rule.id, now + rule.silenceDuration);
        }
      }
    }
  }

  private async triggerAlert(rule: AlertRule, metrics: any): Promise<void> {
    const alertPayload = {
      level: 'ALERT',
      ruleId: rule.id,
      serviceName: rule.serviceName,
      message: rule.message,
      metrics: metrics,
      timestamp: new Date().toISOString()
    };

    // Envoyer √† un webhook ou afficher une notification
    if (rule.webhookUrl) {
      try {
        await fetch(rule.webhookUrl, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(alertPayload)
        });
      } catch (error) {
        console.error('[LoggerService] √âchec de l\'envoi de l\'alerte:', error);
      }
    }

    // Afficher une notification dans l'UI
    if (typeof window !== 'undefined' && window.dispatchEvent) {
      window.dispatchEvent(new CustomEvent('logger-alert', { detail: alertPayload }));
    }
  }
}

// Types pour l'AlertManager
interface AlertRule {
  id: string;
  serviceName: string;
  condition: (metrics: any) => boolean;
  message: string;
  webhookUrl?: string;
  silenceDuration?: number; // en millisecondes
}
```

#### 3. Int√©gration dans les Services Existants
Tous les services doivent utiliser le LoggerService centralis√© :

```typescript
// Exemple d'int√©gration dans DialoguePlugin
class DialoguePlugin {
  private logger = new LoggerService();

  async process(prompt: string): Promise<string> {
    const correlationId = uuidv4();
    this.logger.setCorrelationId(correlationId);

    try {
      this.logger.info('DialoguePlugin', 'D√©but du traitement', { prompt: prompt.substring(0, 50) });
      
      const plan = await router.createPlan(prompt);
      this.logger.debug('DialoguePlugin', 'Plan cr√©√©', { planSteps: plan.steps.length });
      
      const result = await taskExecutor.executePlan(plan);
      this.logger.info('DialoguePlugin', 'Fin du traitement');
      
      return result;
    } catch (error) {
      this.logger.error('DialoguePlugin', '√âchec du traitement', error, { prompt: prompt.substring(0, 50) });
      throw error;
    } finally {
      this.logger.clearCorrelationId();
    }
  }
}

// Exemple d'int√©gration dans TaskExecutor
class TaskExecutor {
  private logger = new LoggerService();

  async executeTask(task: Task): Promise<any> {
    this.logger.info('TaskExecutor', `Ex√©cution de la t√¢che ${task.type}`, { taskId: task.id });
    
    try {
      const result = await this.executeTaskInternal(task);
      this.logger.info('TaskExecutor', `T√¢che termin√©e avec succ√®s`, { taskId: task.id });
      return result;
    } catch (error) {
      this.logger.error('TaskExecutor', `√âchec de la t√¢che ${task.type}`, error, { taskId: task.id });
      throw error;
    }
  }
}
```

#### 4. Composant UI pour Visualiser les Logs
```tsx
// LogViewer.tsx
import React, { useState, useEffect } from 'react';

function LogViewer() {
  const [logs, setLogs] = useState<any[]>([]);
  const [filter, setFilter] = useState({ level: 'ALL', service: 'ALL' });

  useEffect(() => {
    const interval = setInterval(async () => {
      const historicalLogs = await logger.getHistoricalLogs(
        filter.level !== 'ALL' ? { level: filter.level as LogLevel } : undefined
      );
      setLogs(historicalLogs.slice(-100)); // Derniers 100 logs
    }, 1000);

    return () => clearInterval(interval);
  }, [filter]);

  return (
    <div className="log-viewer">
      <div className="log-filters">
        <select onChange={e => setFilter({ ...filter, level: e.target.value })}>
          <option value="ALL">All Levels</option>
          <option value="ERROR">Errors</option>
          <option value="WARN">Warnings</option>
          <option value="INFO">Info</option>
          <option value="DEBUG">Debug</option>
        </select>
        
        <select onChange={e => setFilter({ ...filter, service: e.target.value })}>
          <option value="ALL">All Services</option>
          <option value="DialoguePlugin">DialoguePlugin</option>
          <option value="TaskExecutor">TaskExecutor</option>
          <option value="Router">Router</option>
        </select>
      </div>

      <div className="log-list">
        {logs.map((log, i) => (
          <div key={i} className={`log-entry log-${log.level.toLowerCase()}`}>
            <span className="timestamp">[{log.timestamp}]</span>
            <span className="level">{log.level}</span>
            <span className="service">{log.service}</span>
            <span className="message">{log.message}</span>
            {log.correlationId && <span className="correlation-id">CID: {log.correlationId}</span>}
          </div>
        ))}
      </div>
    </div>
  );
}
```

#### 5. Configuration Environnement
Fichiers `.env` :

```bash
# .env.development
VITE_LOG_LEVEL=DEBUG

# .env.production
VITE_LOG_LEVEL=WARN
```

### R√©sultats Attendus
1. Tous les services utilisent le LoggerService centralis√©
2. Logs structur√©s en JSON avec tous les champs requis
3. Persistance des logs dans OPFS avec retry exponentiel
4. Redaction automatique des donn√©es sensibles
5. Sampling intelligent pour les logs haute fr√©quence
6. Tracing distribu√© avec correlationId
7. M√©triques agr√©g√©es disponibles
8. Syst√®me d'alerting avec AlertManager
9. Composant UI pour visualiser les logs
10. Configuration par environnement fonctionnelle

## üéØ T√¢che #22 : Am√©liorations du RuntimeManager

### Objectif
Am√©liorer le RuntimeManager avec du versioning de graphes, du feedback utilisateur pendant compilation, un cache m√©moire observable et du warming planifi√©.

### Sp√©cifications Techniques D√©taill√©es

#### 1. Versioning des Graphes
```typescript
// Header JSON standardis√© pour les graphes
interface GraphHeader {
  version: string;          // Version du graphe (ex: "1.2.3")
  modelName: string;        // Nom du mod√®le
  schemaHash: string;       // Hash du sch√©ma pour v√©rification d'int√©grit√©
  generatedAt: string;      // Timestamp de g√©n√©ration ISO 8601
  dependencies?: string[];  // D√©pendances du graphe
}

class RuntimeManager {
  private readonly GRAPH_VERSION = '1.0.0';
  
  // Utilisation du header comme cl√© de cache
  private getGraphCacheKey(modelKey: string): string {
    return `${modelKey}@${this.GRAPH_VERSION}`;
  }
  
  // Nettoyage automatique des graphes obsol√®tes au boot
  public async cleanupObsoleteGraphs(): Promise<void> {
    const allGraphs = await storageManager.listGraphs();
    for (const graphKey of allGraphs) {
      const [modelName, version] = graphKey.split('@');
      if (version !== this.GRAPH_VERSION) {
        await storageManager.deleteGraph(graphKey);
        logger.info('RuntimeManager', `Graphe obsol√®te supprim√©: ${graphKey}`);
      }
    }
  }
}
```

#### 2. Feedback Utilisateur Pendant Compilation
```typescript
class RuntimeManager {
  // Timeline simul√©e d√©terministe
  private readonly COMPILATION_STAGES = [
    { name: 'parsing', duration: 200 },
    { name: 'linking', duration: 300 },
    { name: 'optimizing', duration: 500 },
    { name: 'compiling', duration: 500 }
  ];

  public async compileModel(modelKey: string): Promise<void> {
    const correlationId = uuidv4();
    logger.setCorrelationId(correlationId);
    
    try {
      logger.info('RuntimeManager', 'D√©but de la compilation du mod√®le', { modelKey });
      
      // √âmettre des √©v√©nements de progression
      for (const stage of this.COMPILATION_STAGES) {
        // √âmettre un √©v√©nement de progression
        this.emitProgressEvent(stage.name, 'started');
        
        // Simuler le traitement avec une dur√©e d√©terministe
        await new Promise(resolve => setTimeout(resolve, stage.duration));
        
        // √âmettre un √©v√©nement de progression
        this.emitProgressEvent(stage.name, 'completed');
      }
      
      logger.info('RuntimeManager', 'Compilation du mod√®le termin√©e', { modelKey });
    } catch (error) {
      logger.error('RuntimeManager', '√âchec de la compilation du mod√®le', error, { modelKey });
      throw error;
    } finally {
      logger.clearCorrelationId();
    }
  }
  
  private emitProgressEvent(stage: string, status: 'started' | 'completed'): void {
    if (typeof window !== 'undefined' && window.dispatchEvent) {
      window.dispatchEvent(new CustomEvent('compilation-progress', {
        detail: { stage, status, timestamp: Date.now() }
      }));
    }
  }
}
```

#### 3. Cache M√©moire Observable
```typescript
class LRUCache<T> {
  private cache: Map<string, { value: T, timestamp: number }> = new Map();
  private readonly maxSize: number;
  private hits: number = 0;
  private misses: number = 0;

  constructor(maxSize: number = 100) {
    this.maxSize = maxSize;
  }

  public get(key: string): T | undefined {
    const entry = this.cache.get(key);
    if (entry) {
      this.hits++;
      // Mettre √† jour le timestamp pour LRU
      entry.timestamp = Date.now();
      return entry.value;
    }
    this.misses++;
    return undefined;
  }

  public set(key: string, value: T): void {
    // √âviction LRU si n√©cessaire
    if (this.cache.size >= this.maxSize) {
      let oldestKey = '';
      let oldestTimestamp = Infinity;
      
      for (const [k, v] of this.cache.entries()) {
        if (v.timestamp < oldestTimestamp) {
          oldestTimestamp = v.timestamp;
          oldestKey = k;
        }
      }
      
      if (oldestKey) {
        this.cache.delete(oldestKey);
        logger.info('LRUCache', `√âviction du graphe: ${oldestKey}`);
      }
    }
    
    this.cache.set(key, { value, timestamp: Date.now() });
  }

  public getStats(): { size: number, hits: number, misses: number, hitRate: number } {
    const total = this.hits + this.misses;
    const hitRate = total > 0 ? this.hits / total : 0;
    
    return {
      size: this.cache.size,
      hits: this.hits,
      misses: this.misses,
      hitRate: parseFloat(hitRate.toFixed(4))
    };
  }

  public clear(): void {
    this.cache.clear();
    this.hits = 0;
    this.misses = 0;
  }
}

class RuntimeManager {
  private graphCache = new LRUCache<CompiledGraph>(50); // Cache de 50 graphes max
  
  public getCacheStats(): ReturnType<LRUCache<any>['getStats']> {
    return this.graphCache.getStats();
  }
}
```

#### 4. Warming Planifi√©
```typescript
class RuntimeManager {
  private warmingScheduler: WorkerScheduler;
  private readonly WARMING_IDLE_TIMEOUT = 30000; // 30 secondes

  constructor() {
    this.warmingScheduler = new WorkerScheduler({
      priority: 'low', // Priorit√© basse pour ne pas bloquer l'UI
      idleCallback: true, // Utiliser requestIdleCallback
      maxConcurrency: 2 // Limiter la concurrence
    });
  }

  // Warming bas√© sur les metrics d'utilisation
  public async scheduleStrategicWarming(): Promise<void> {
    try {
      // Obtenir les mod√®les les plus fr√©quemment utilis√©s
      const topModels = await this.getModelUsageMetrics();
      
      for (const model of topModels.slice(0, 5)) { // Top 5 mod√®les
        // Planifier le warming avec une priorit√© basse
        this.warmingScheduler.schedule(async () => {
          await this.preWarmModel(model.modelKey);
        }, { priority: 'low' });
      }
      
      logger.info('RuntimeManager', 'Warming strat√©gique planifi√©', { 
        modelCount: Math.min(topModels.length, 5) 
      });
    } catch (error) {
      logger.error('RuntimeManager', '√âchec de la planification du warming strat√©gique', error);
    }
  }

  // Pr√©-warming d'un mod√®le
  private async preWarmModel(modelKey: string): Promise<void> {
    try {
      logger.info('RuntimeManager', 'D√©but du pr√©-warming du mod√®le', { modelKey });
      
      // Charger et compiler le mod√®le
      const compiledGraph = await this.loadAndCompileModel(modelKey);
      
      // Stocker dans le cache compress√©
      const compressedGraph = await this.compressGraph(compiledGraph);
      const cacheKey = this.getGraphCacheKey(modelKey);
      await storageManager.saveCompressedGraph(cacheKey, compressedGraph);
      
      logger.info('RuntimeManager', 'Mod√®le pr√©-warm√© avec succ√®s', { modelKey });
    } catch (error) {
      logger.error('RuntimeManager', '√âchec du pr√©-warming du mod√®le', error, { modelKey });
    }
  }

  // Compression du graphe pour le stockage temporaire
  private async compressGraph(graph: CompiledGraph): Promise<Blob> {
    const jsonString = JSON.stringify(graph);
    const stream = new ReadableStream({
      start(controller) {
        controller.enqueue(new TextEncoder().encode(jsonString));
        controller.close();
      }
    });
    
    const compressedStream = stream.pipeThrough(new CompressionStream('gzip'));
    return await new Response(compressedStream).blob();
  }

  // Obtention des metrics d'utilisation des mod√®les
  private async getModelUsageMetrics(): Promise<{ modelKey: string; usageCount: number }[]> {
    // Simulation - en r√©alit√©, cela viendrait des logs/metrics
    return [
      { modelKey: 'llama-3.2-1b', usageCount: 1250 },
      { modelKey: 'mistral-7b', usageCount: 980 },
      { modelKey: 'phi-3-mini', usageCount: 750 },
      { modelKey: 'gemma-2b', usageCount: 620 },
      { modelKey: 'qwen-1.8b', usageCount: 450 }
    ];
  }
}

// Scheduler pour le warming planifi√©
class WorkerScheduler {
  private tasks: ScheduledTask[] = [];
  private readonly options: SchedulerOptions;

  constructor(options: SchedulerOptions) {
    this.options = options;
  }

  public schedule(task: () => Promise<void>, options: TaskOptions): void {
    const scheduledTask: ScheduledTask = {
      task,
      priority: options.priority || 'normal',
      scheduledAt: Date.now()
    };

    this.tasks.push(scheduledTask);
    this.processNextTask();
  }

  private async processNextTask(): Promise<void> {
    if (this.tasks.length === 0) return;

    // Trier par priorit√©
    this.tasks.sort((a, b) => {
      const priorityOrder = { 'high': 0, 'normal': 1, 'low': 2 };
      return priorityOrder[a.priority] - priorityOrder[b.priority];
    });

    const nextTask = this.tasks.shift();
    if (!nextTask) return;

    try {
      if (this.options.idleCallback && typeof requestIdleCallback !== 'undefined') {
        // Utiliser requestIdleCallback pour le browser
        await new Promise<void>((resolve) => {
          requestIdleCallback(async () => {
            await nextTask.task();
            resolve();
          }, { timeout: this.options.idleTimeout || 1000 });
        });
      } else {
        // Ex√©cuter directement
        await nextTask.task();
      }
    } catch (error) {
      logger.error('WorkerScheduler', '√âchec de l\'ex√©cution de la t√¢che', error);
    }
  }
}

interface SchedulerOptions {
  priority: 'high' | 'normal' | 'low';
  idleCallback?: boolean;
  idleTimeout?: number;
  maxConcurrency?: number;
}

interface TaskOptions {
  priority: 'high' | 'normal' | 'low';
}

interface ScheduledTask {
  task: () => Promise<void>;
  priority: 'high' | 'normal' | 'low';
  scheduledAt: number;
}
```

### R√©sultats Attendus pour la T√¢che #22
1. Versioning des graphes avec header standardis√© et nettoyage automatique
2. Feedback utilisateur pendant compilation avec timeline simul√©e
3. Cache m√©moire observable avec statistiques d'utilisation
4. Warming planifi√© bas√© sur les metrics d'utilisation
5. Compression des graphes pour le stockage temporaire
6. Scheduler intelligent pour le warming avec priorit√©s

## üéØ T√¢che #23 : Suite de Benchmark

### Objectif
Cr√©er un script de benchmark (npm run benchmark) qui ex√©cute une s√©rie de sc√©narios standardis√©s sur notre "Usine Vide" et mesure des m√©triques de performance cl√©s. Ce script doit pouvoir simuler diff√©rentes configurations mat√©rielles pour √©valuer la performance sur un √©ventail de "devices".

### Sp√©cifications Techniques D√©taill√©es

#### 1. DeviceSimulator
Ce module nous permettra de "tromper" notre ResourceManager pour qu'il croie tourner sur un appareil diff√©rent.

```typescript
// src/core/kernel/monitoring/DeviceSimulator.ts

import { resourceManager, DeviceStatus } from '../ResourceManager';

export type DeviceProfile = 'LOW_END_MOBILE' | 'MID_RANGE_TABLET' | 'HIGH_END_DESKTOP';

const PROFILES: Record<DeviceProfile, Partial<DeviceStatus>> = {
  'LOW_END_MOBILE': {
    cpu: { hardwareConcurrency: 2 },
    memory: { jsHeapSizeLimit: 2 * 1024**3, usageRatio: 0.8 },
    network: { effectiveType: '3g' },
    battery: { level: 0.4, isCharging: false }
  },
  'MID_RANGE_TABLET': {
    cpu: { hardwareConcurrency: 4 },
    memory: { jsHeapSizeLimit: 4 * 1024**3, usageRatio: 0.6 },
    network: { effectiveType: '4g' },
    battery: { level: 0.7, isCharging: false }
  },
  'HIGH_END_DESKTOP': {
    cpu: { hardwareConcurrency: 16 },
    memory: { jsHeapSizeLimit: 16 * 1024**3, usageRatio: 0.3 },
    network: { effectiveType: '4g' },
    battery: { isCharging: true, level: 1 }
  }
};

export function simulateDevice(profile: DeviceProfile): void {
  const status = PROFILES[profile];
  // "Monkey-patch" la m√©thode getStatus pour qu'elle retourne notre profil simul√©
  resourceManager.getStatus = async () => {
    return { ...resourceManager.getInitialStatus(), ...status } as DeviceStatus;
  };
  console.log(`\n[Benchmark] üì± Simulation du device: ${profile}`);
}
```

#### 2. Script de Benchmark
Ce script sera √† la racine du projet et ex√©cutera nos sc√©narios.

```typescript
// benchmark.ts (√† la racine)

import { dialoguePlugin } from './src/core/plugins/DialoguePlugin';
import { simulateDevice, DeviceProfile } from './src/core/kernel/monitoring/DeviceSimulator';
import { logger } from './src/core/kernel/monitoring/LoggerService';

// D√©sactive les logs JSON pour un affichage plus propre du benchmark
logger.info = () => {};
logger.warn = () => {};
logger.error = () => {};

const SCENARIOS = {
  'Dialogue Simple': "Explique le concept de l'open source en une phrase.",
  'T√¢che de Code': "√âcris une fonction javascript qui inverse une cha√Æne de caract√®res.",
  'Requ√™te Complexe (Parall√®le)': "√âcris un po√®me sur la lune et donne-moi le code d'une fonction qui calcule la factorielle."
};

async function runBenchmarkForProfile(profile: DeviceProfile) {
  simulateDevice(profile);
  
  const results: Record<string, number> = {};

  for (const [name, prompt] of Object.entries(SCENARIOS)) {
    const startTime = performance.now();
    
    // On appelle process, mais on ne se soucie pas de la r√©ponse, juste du temps
    await dialoguePlugin.process(prompt);
    
    const duration = performance.now() - startTime;
    results[name] = Math.round(duration);
  }

  return results;
}

async function runAllBenchmarks() {
  console.log("üìä === D√âBUT DE LA SUITE DE BENCHMARKS === üìä");

  const allResults: Record<string, any> = {};

  allResults['LOW_END_MOBILE'] = await runBenchmarkForProfile('LOW_END_MOBILE');
  allResults['MID_RANGE_TABLET'] = await runBenchmarkForProfile('MID_RANGE_TABLET');
  allResults['HIGH_END_DESKTOP'] = await runBenchmarkForProfile('HIGH_END_DESKTOP');

  console.log("\n\nüìà === R√âSULTATS FINAUX (en ms) === üìà");
  console.table(allResults);
  console.log("\n‚úÖ Suite de benchmarks termin√©e.");
}

runAllBenchmarks();
```

#### 3. Configuration du package.json
Ajout du script de benchmark dans le package.json :

```json
// package.json
{
  "scripts": {
    "benchmark": "ts-node benchmark.ts"
  }
}
```

### R√©sultats Attendus pour la T√¢che #23
1. Cr√©ation du DeviceSimulator pour simuler diff√©rentes configurations mat√©rielles
2. Script de benchmark complet avec sc√©narios standardis√©s
3. Int√©gration du script dans package.json
4. Mesure objective des temps d'ex√©cution bout en bout
5. Simulation multi-device pour √©valuer l'adaptation du Router
6. D√©tection de r√©gression gr√¢ce aux benchmarks r√©guliers
7. Distinction claire entre cold start et warm start pour des mesures pr√©cises
8. Instrumentation du RuntimeManager et StorageManager pour les m√©triques de compilation/loading
9. Baselines diff√©renci√©es pour cold et warm start avec seuils de r√©gression
