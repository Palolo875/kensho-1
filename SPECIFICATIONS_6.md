# üîß Sp√©cifications Techniques - Ensemble 6

## üéØ T√¢che #25 : Inf√©rence Sp√©culative sur l'Intention

### Objectif
Transformer notre Router et notre RuntimeManager pour qu'ils n'attendent plus passivement le prompt final. Pendant que l'utilisateur tape, le syst√®me doit analyser le texte en temps r√©el, pr√©dire l'intention la plus probable, et commencer √† pr√©chauffer le moteur du plugin expert correspondant en VRAM avant m√™me que l'utilisateur n'ait appuy√© sur "Envoyer".

### Philosophie "Usine Vide"
Nous impl√©mentons la vraie logique de surveillance et de pr√©chauffage. Le Router va r√©ellement √©couter les √©v√©nements oninput, et le RuntimeManager va r√©ellement simuler le chargement d'un mod√®le en m√©moire. C'est le premier pas vers un syst√®me qui pense un coup d'avance.

### Sp√©cifications Techniques D√©taill√©es

#### 1. Structure de persistance OPFS pour les statistiques utilisateur

```json
// user-prediction-stats.json
{
  "userId": "anon-abc123",
  "lastUpdated": "2025-12-04T23:30:00Z",
  "globalAccuracy": 0.78,
  "totalPredictions": 127,
  "intentPatterns": {
    "solve": { "CODE": 0.65, "MATH": 0.32, "DIALOGUE": 0.03 },
    "function": { "CODE": 0.92, "DIALOGUE": 0.08 },
    "calculate": { "MATH": 0.88, "CODE": 0.10 },
    "write": { "CODE": 0.71, "DIALOGUE": 0.25 }
  },
  "userPrefs": {
    "preferredCodeExpert": "code-expert-v2",
    "disablePrewarmMath": false
  }
}
```

#### 2. Mise √† jour du UI Bridge pour relayer l'√©v√©nement input
Notre ui-controller.ts doit maintenant √©couter l'√©v√©nement input sur le champ de texte et l'envoyer au worker.

```typescript
// ui-controller.ts (Mise √† jour)

// ... (code existant)
promptInput.addEventListener('input', () => {
  const currentText = promptInput.value;
  if (currentText.length > 10) { // Ne pas spammer pour quelques lettres
    worker.postMessage({ type: 'user-is-typing', payload: { text: currentText } });
  }
});
```

#### 3. Mise √† jour du Kernel pour g√©rer le nouvel √©v√©nement

```typescript
// src/core/kernel.ts (Mise √† jour)

// ...
export async function initializeKernel(port: MessagePort) {
  // ...
  return {
    handleMessage: async (message: { type: string, payload: any }) => {
      // ...
      if (message.type === 'user-is-typing') {
        // D√©l√®gue l'analyse pr√©dictive au Router
        await router.predictAndPrewarm(message.payload.text);
      }
    }
  };
}
```

#### 4. Mise √† jour du Router avec la logique de pr√©chauffage adaptatif
C'est le c≈ìur de la nouvelle fonctionnalit√© avec des am√©liorations importantes.

```typescript
// src/core/kernel/Router.ts (Mise √† jour majeure)

import { runtimeManager } from './RuntimeManager';
import { logger } from './monitoring/LoggerService';
import { storageManager } from './storage/StorageManager';
// ...

// Interface pour les statistiques utilisateur
interface UserPredictionStats {
  userId: string;
  lastUpdated: string;
  globalAccuracy: number;
  totalPredictions: number;
  intentPatterns: Record<string, Record<string, number>>;
  userPrefs: {
    preferredCodeExpert: string;
    disablePrewarmMath: boolean;
  };
}

class UserPredictionProfile {
  private userStats: Record<string, { correct: number, total: number }> = {};
  
  // Persistance OPFS
  async loadUserProfile(userId: string): Promise<void> {
    try {
      const profile = await storageManager.getFile(`prediction-profile-${userId}.json`);
      if (profile) {
        this.userStats = JSON.parse(profile);
      }
    } catch (error) {
      logger.warn('UserPredictionProfile', '√âchec du chargement du profil utilisateur', error);
    }
  }
  
  async saveUserProfile(userId: string): Promise<void> {
    try {
      await storageManager.saveFile(
        `prediction-profile-${userId}.json`, 
        JSON.stringify(this.userStats)
      );
    } catch (error) {
      logger.warn('UserPredictionProfile', '√âchec de la sauvegarde du profil utilisateur', error);
    }
  }

  getAdaptiveThreshold(intent: string): number {
    const userStats = this.userStats[intent] || { correct: 0, total: 0 };
    const userAccuracy = userStats.total > 5 ? userStats.correct / userStats.total : 0.5;
    
    // Personnalisation par utilisateur + fallback global
    return Math.max(0.3, userAccuracy * 0.7);
  }

  updateStats(intent: string, wasCorrect: boolean): void {
    const stats = this.userStats[intent] || { correct: 0, total: 0 };
    stats.total++;
    if (wasCorrect) stats.correct++;
    this.userStats[intent] = stats;
  }
  
  getStats(): Record<string, { correct: number, total: number }> {
    return this.userStats;
  }
}

class AdaptiveRouter {
  private prewarmTimeout: any = null;
  private lastPrewarmedExpert: string | null = null;
  private lastPredictedIntent: string | null = null;
  private prewarmTimeout: any = null;
  
  private readonly INTENT_PATTERNS: Record<string, { keywords: string[], weight: number }[]> = {
    'CODE': [
      { keywords: ['write', 'code'], weight: 3 },
      { keywords: ['function', 'return'], weight: 3 },
      { keywords: ['debug', 'error', 'bug'], weight: 2 },
      { keywords: ['javascript', 'python', 'typescript'], weight: 4 },
      { keywords: ['implement', 'algorithm'], weight: 2 },
      { keywords: ['code'], weight: 1 } // Seul = faible poids
    ],
    'MATH': [
      { keywords: ['calculate', 'solve'], weight: 3 },
      { keywords: ['equation', 'formula'], weight: 4 },
      { keywords: ['math', 'problem'], weight: 2 },
      { keywords: ['integral', 'derivative'], weight: 5 },
      { keywords: ['x', '=', 'solve'], weight: 3 } // Patterns math√©matiques
    ]
  };

  private intentConfidenceHistory: Array<{ intent: string, confidence: number, timestamp: number }> = [];
  private predictionAccuracy: Map<string, { correct: number, total: number }> = new Map();
  private userStats: UserPredictionStats | null = null;
  private userProfile: UserPredictionProfile = new UserPredictionProfile();

  constructor() {
    // Charge les statistiques utilisateur au d√©marrage
    this.loadUserStats();
  }

  async loadUserStats(): Promise<void> {
    try {
      const stats = await storageManager.readFile('user-prediction-stats.json');
      this.userStats = JSON.parse(stats);
      logger.info('Router', `Stats utilisateur charg√©: ${this.userStats.globalAccuracy.toFixed(1)} accuracy`);
      
      // Charge √©galement le profil utilisateur sp√©cifique
      await this.userProfile.loadUserProfile(this.userStats.userId);
    } catch {
      this.userStats = { 
        userId: "anonymous",
        lastUpdated: new Date().toISOString(),
        globalAccuracy: 0,
        totalPredictions: 0,
        intentPatterns: {},
        userPrefs: {
          preferredCodeExpert: "code-qwen2.5-coder-1.5b-mock",
          disablePrewarmMath: false
        }
      };
    }
  }

  /**
   * Analyse le texte en cours de frappe, pr√©dit l'intention et pr√©chauffe le plugin.
   */
  public async predictAndPrewarm(text: string): Promise<void> {
    // Utilise un debounce pour ne pas surcharger le syst√®me
    clearTimeout(this.prewarmTimeout);
    this.prewarmTimeout = setTimeout(async () => {
      
      const { intent, confidence } = this.classifyIntentWithConfidence(text);
      
      // ‚úÖ Ne prewarm que si confiance suffisante
      const threshold = this.getAdaptiveThreshold(intent);
      if (confidence < threshold) {
        logger.debug('Router', `Confiance trop faible pour ${intent} (${confidence} < ${threshold})`);
        return;
      }

      // Track l'historique pour learning
      this.intentConfidenceHistory.push({ 
        intent, 
        confidence, 
        timestamp: Date.now() 
      });

      const probableExpert = this.selectExpertForIntent(intent);
      if (!probableExpert || probableExpert === this.lastPrewarmedExpert) return;

      logger.info('Router', `üéØ Intent: ${intent} (confiance: ${(confidence * 100).toFixed(0)}%) ‚Üí Prewarm ${probableExpert}`);
      
      runtimeManager.prewarmModel(probableExpert);
      this.lastPrewarmedExpert = probableExpert;
      this.lastPredictedIntent = intent;

    }, 300); // Attend 300ms apr√®s la derni√®re frappe
  }

  classifyIntentWithConfidence(text: string): { intent: string, confidence: number } {
    const baseScores = this.calculateBaseScores(text); // N-grams
    const userBoostedScores = this.applyUserPreferences(baseScores, text);

    // Fusionne base + user data
    const finalScores: Record<string, number> = {};
    for (const [intent, score] of Object.entries(baseScores)) {
      finalScores[intent] = score + (userBoostedScores[intent] || 0);
    }

    const maxScore = Math.max(...Object.values(finalScores));
    // Seuil minimal de score pour √©viter les faux positifs
    if (maxScore < 2) {
      return { intent: 'DIALOGUE', confidence: 0 };
    }

    const intent = Object.keys(finalScores).find(k => finalScores[k] === maxScore) || 'DIALOGUE';
    const confidence = maxScore / 15; // Normalis√©

    logger.debug('Router', `Intent scores: ${JSON.stringify(finalScores)} ‚Üí ${intent} (${confidence})`);
    return { intent, confidence };
  }

  private calculateBaseScores(text: string): Record<string, number> {
    const lowerText = text.toLowerCase();
    const scores: Record<string, number> = { 'DIALOGUE': 0, 'CODE': 0, 'MATH': 0 };

    // Calcule un score pour chaque intent
    for (const [intent, patterns] of Object.entries(this.INTENT_PATTERNS)) {
      for (const { keywords, weight } of patterns) {
        // V√©rifie si TOUS les mots du pattern sont pr√©sents
        if (keywords.every(keyword => lowerText.includes(keyword))) {
          scores[intent] += weight;
        }
      }
    }

    return scores;
  }

  private applyUserPreferences(
    baseScores: Record<string, number>, 
    text: string
  ): Record<string, number> {
    if (!this.userStats) return {};

    const boosts: Record<string, number> = {};
    const words = text.toLowerCase().split(/\s+/);

    for (const word of words) {
      const patterns = this.userStats.intentPatterns[word];
      if (patterns) {
        for (const [intent, prob] of Object.entries(patterns)) {
          boosts[intent] = (boosts[intent] || 0) + (prob as number) * 2; // Boost x2 user data
        }
      }
    }

    return boosts;
  }

  private getAdaptiveThreshold(intent: string): number {
    // Utilise le seuil adaptatif du profil utilisateur
    if (this.userStats && this.userStats.userId !== "anonymous") {
      return this.userProfile.getAdaptiveThreshold(intent);
    }
    
    // Seuil plus bas pour les intents fr√©quemment corrects
    const recentHistory = this.intentConfidenceHistory
      .filter(h => h.intent === intent)
      .slice(-10); // Derni√®res 10 pr√©dictions

    if (recentHistory.length < 3) return 0.5; // Seuil conservateur par d√©faut

    // Si l'historique montre qu'on est souvent bon, on baisse le seuil
    const avgConfidence = recentHistory.reduce((sum, h) => sum + h.confidence, 0) / recentHistory.length;
    return Math.max(0.3, avgConfidence * 0.8); // Entre 30% et 80%
  }

  private selectExpertForIntent(intent: string): string | null {
    // Logique simplifi√©e pour trouver le meilleur expert pour une intention
    if (intent === 'CODE') return 'code-qwen2.5-coder-1.5b-mock';
    if (intent === 'MATH') return 'math-bitnet-1.58b-mock';
    return null;
  }

  public async createPlan(prompt: string): Promise<ExecutionPlan> {
    // ... (d√©but normal)

    const actualIntent = this.detectIntent(prompt);
    const predictedIntent = this.lastPredictedIntent; // √Ä ajouter

    // ‚úÖ V√©rifie si la pr√©diction √©tait correcte
    if (predictedIntent && this.lastPrewarmedExpert) {
      const wasCorrect = actualIntent === predictedIntent;
      
      const stats = this.predictionAccuracy.get(predictedIntent) || { correct: 0, total: 0 };
      stats.total++;
      if (wasCorrect) stats.correct++;
      this.predictionAccuracy.set(predictedIntent, stats);

      const accuracy = (stats.correct / stats.total * 100).toFixed(1);
      logger.info('Router', `Pr√©diction ${wasCorrect ? '‚úÖ' : '‚ùå'} (pr√©cision ${predictedIntent}: ${accuracy}%)`);

      // Si pr√©cision < 50%, ajuste les patterns
      if (stats.total > 10 && stats.correct / stats.total < 0.5) {
        logger.warn('Router', `Mauvaise pr√©cision pour ${predictedIntent}, d√©sactivation temporaire`);
        this.disablePrewarmFor(predictedIntent, 60000); // 1 minute
      }
      
      // Met √† jour les statistiques utilisateur
      await this.updateUserStats(prompt, actualIntent, wasCorrect);
    }

    // ... (continue)
  }

  private async updateUserStats(
    prompt: string, 
    actualIntent: string, 
    wasCorrect: boolean
  ): Promise<void> {
    if (!this.userStats) return;

    // Met √† jour global accuracy
    const stats = this.userStats;
    stats.globalAccuracy = (
      stats.globalAccuracy * stats.totalPredictions + (wasCorrect ? 1 : 0)
    ) / (stats.totalPredictions + 1);
    stats.totalPredictions++;

    // Met √† jour les patterns par mot
    const words = prompt.toLowerCase().split(/\s+/).slice(0, 10);
    for (const word of words) {
      if (!stats.intentPatterns[word]) {
        stats.intentPatterns[word] = { CODE: 0, MATH: 0, DIALOGUE: 0 };
      }
      
      const patterns = stats.intentPatterns[word];
      patterns[actualIntent]++;
      const total = Object.values(patterns).reduce((a, b) => a + b, 0);
      
      // Normalise en probabilit√©s
      for (const intent of Object.keys(patterns)) {
        patterns[intent] = patterns[intent] / total;
      }
    }
    
    stats.lastUpdated = new Date().toISOString();

    // Sauvegarde async (non-bloquant)
    storageManager.saveFile('user-prediction-stats.json', JSON.stringify(stats))
      .catch(err => logger.warn('Router', '√âchec sauvegarde stats utilisateur', err));
      
    // Met aussi √† jour le profil utilisateur sp√©cifique
    if (this.userStats.userId !== "anonymous") {
      this.userProfile.updateStats(actualIntent, wasCorrect);
      await this.userProfile.saveUserProfile(this.userStats.userId);
    }
  }

  public getPredictionStats(): Record<string, { accuracy: number, total: number }> {
    const stats: any = {};
    for (const [intent, data] of this.predictionAccuracy.entries()) {
      stats[intent] = {
        accuracy: (data.correct / data.total * 100).toFixed(1) + '%',
        total: data.total
      };
    }
    return stats;
  }

  private disablePrewarmFor(intent: string, duration: number): void {
    // Impl√©mentation de la d√©sactivation temporaire
    setTimeout(() => {
      logger.info('Router', `R√©activation du pr√©chauffage pour ${intent}`);
    }, duration);
  }
  
  public getUserProfileStats(): Record<string, { correct: number, total: number }> {
    return this.userProfile.getStats();
  }
}

export const router = new AdaptiveRouter();
```

#### 5. Mise √† jour du RuntimeManager avec la m√©thode prewarmModel am√©lior√©e

```typescript
// src/core/kernel/RuntimeManager.ts (Mise √† jour)

// ...
class RuntimeManager {
  // ... (propri√©t√©s existantes)
  private prewarmingModels: Map<string, AbortController> = new Map();
  
  private prewarmMetrics = {
    totalPrewarms: 0,
    successfulPrewarms: 0,
    cancelledPrewarms: 0,
    hitRate: 0, // % des prewarms qui ont √©t√© utilis√©s
    avgTimeSaved: 0
  };

  /**
   * Pr√©charge un mod√®le en m√©moire (VRAM simul√©e) sans l'ex√©cuter.
   * C'est une op√©ration non bloquante.
   */
  public prewarmModel(modelKey: string): void {
    this.prewarmMetrics.totalPrewarms++;
    
    // Annule les autres pr√©chauffages en cours
    for (const [key, controller] of this.prewarmingModels.entries()) {
      if (key !== modelKey) {
        logger.info('RuntimeManager', `Annulation du pr√©chauffage de ${key}`);
        controller.abort();
        this.prewarmingModels.delete(key);
        this.prewarmMetrics.cancelledPrewarms++;
      }
    }

    // Si d√©j√† en cours pour ce mod√®le, ne rien faire
    if (this.prewarmingModels.has(modelKey)) {
      logger.debug('RuntimeManager', `${modelKey} d√©j√† en pr√©chauffage`);
      return;
    }

    // Si d√©j√† charg√©, ne rien faire
    if (this.loadedCompiledGraphs.has(modelKey)) {
      logger.debug('RuntimeManager', `${modelKey} d√©j√† charg√©`);
      this.prewarmMetrics.successfulPrewarms++;
      return;
    }

    // Lance le pr√©chauffage avec AbortController
    const controller = new AbortController();
    this.prewarmingModels.set(modelKey, controller);

    logger.info('RuntimeManager', `üî• Pr√©chauffage de ${modelKey}...`);

    this.loadModel(modelKey, controller.signal)
      .then(() => {
        logger.info('RuntimeManager', `‚úÖ ${modelKey} pr√©chauff√© et pr√™t`);
        this.prewarmingModels.delete(modelKey);
        this.prewarmMetrics.successfulPrewarms++;
      })
      .catch(err => {
        if (err.name === 'AbortError') {
          logger.debug('RuntimeManager', `Pr√©chauffage de ${modelKey} annul√©`);
          this.prewarmMetrics.cancelledPrewarms++;
        } else {
          logger.error('RuntimeManager', `√âchec du pr√©chauffage`, err);
        }
        this.prewarmingModels.delete(modelKey);
      });
  }

  public async loadModel(modelKey: string, signal?: AbortSignal): Promise<void> {
    // ... (d√©but inchang√©)

    // Simule une compilation longue (interruptible)
    await new Promise((resolve, reject) => {
      const timeout = setTimeout(resolve, 4000);
      
      // ‚úÖ √âcoute l'abort
      signal?.addEventListener('abort', () => {
        clearTimeout(timeout);
        reject(new DOMException('Aborted', 'AbortError'));
      });
    });

    // ... (reste)
  }

  public getMetrics() {
    return {
      ...this.prewarmMetrics,
      hitRate: (this.prewarmMetrics.successfulPrewarms / this.prewarmMetrics.totalPrewarms * 100).toFixed(1) + '%'
    };
  }

  // ... (le reste de la classe)
}

export const runtimeManager = new RuntimeManager();
```

### R√©sultats Attendus
1. Mise √† jour du UI Bridge pour capturer les √©v√©nements de frappe utilisateur
2. Mise √† jour du Kernel pour traiter les √©v√©nements de frappe
3. Impl√©mentation de la logique de pr√©diction d'intention adaptative dans le Router
4. Ajout de la m√©thode prewarmModel am√©lior√©e dans le RuntimeManager
5. Syst√®me proactif qui anticipe les besoins de l'utilisateur
6. R√©duction de la latence per√ßue gr√¢ce au pr√©chauffage des mod√®les
7. Am√©lioration de l'exp√©rience utilisateur avec des temps de r√©ponse quasi instantan√©s
8. Syst√®me intelligent avec annulation des pr√©chauffages inutiles
9. Boucle de feedback pour am√©liorer la pr√©cision des pr√©dictions
10. Persistance des statistiques utilisateur pour un apprentissage adaptatif
11. M√©triques de performance pour suivre l'efficacit√© du syst√®me
12. Profils utilisateur sp√©cifiques pour un apprentissage personnalis√©

## üéØ T√¢che #26 : G√©n√©ration Sp√©culative de Tokens avec Batching

### Objectif
Impl√©menter une strat√©gie de "speculative decoding" simul√©e combin√©e avec du batch processing pour maximiser le throughput GPU et am√©liorer l'exp√©rience utilisateur. Le principe est simple :

1. Un petit mod√®le ultra-rapide (le "draft model") g√©n√®re une √©bauche de plusieurs tokens d'avance.
2. Le grand mod√®le expert (le "verification model") valide ce bloc de tokens en une seule passe.
3. Si la validation r√©ussit, on affiche le bloc entier d'un coup, donnant un gain de vitesse spectaculaire. Si elle √©choue, on ne garde que le premier token correct et on recommence.
4. Plusieurs requ√™tes sont trait√©es en batch pour maximiser l'utilisation du GPU.

### Philosophie "Usine Vide"
Nous allons simuler ce comportement dans notre MockEngine. Nous n'utilisons pas de vrais mod√®les, mais nous impl√©mentons la vraie logique d'orchestration entre un "draft" et une "validation", et nous simulons le gain de vitesse avec batching.

### Sp√©cifications Techniques D√©taill√©es

#### 1. Mise √† jour du MockEngine pour la G√©n√©ration Sp√©culative avec Batching

```typescript
// src/core/kernel/engine/MockEngine.ts (Mise √† jour majeure)

import { memoryManager } from '../MemoryManager';

const DRAFT_MODEL_SPEED = 5; // ms par token (ultra rapide)
const EXPERT_MODEL_SPEED = 20; // ms par token (plus lent)
const SPECULATION_LENGTH = 4; // Nombre de tokens √† sp√©culer
const BATCH_THRESHOLD = 4; // Nombre minimum de t√¢ches pour activer le batching
const MAX_BATCH_SIZE = 8; // Taille maximale du batch

// Interfaces pour le batching
interface ExpertTask {
  id: string;
  prompt: string;
  modelKey: string;
  context: string[];
}

interface TaskResult {
  taskId: string;
  tokens: string[];
  metadata: {
    acceptRate: number;
    speedup: number;
  };
}

// Classes de mod√®les simul√©s
class MockDraftModel {
  async generateSpeculative(context: string[], count: number): Promise<string[]> {
    const tokens: string[] = [];
    
    for (let i = 0; i < count; i++) {
      await new Promise(r => setTimeout(r, DRAFT_MODEL_SPEED));
      
      // Simule une pr√©diction bas√©e sur le contexte
      const prediction = this.predictNextToken(context.concat(tokens));
      tokens.push(prediction);
    }
    
    return tokens;
  }

  // Version batch√©e du draft model
  async generateSpeculativeBatch(contexts: string[][], count: number): Promise<string[][]> {
    const batchResults: string[][] = [];
    
    // Simule le traitement parall√®le du batch
    await new Promise(r => setTimeout(r, DRAFT_MODEL_SPEED * count));
    
    // G√©n√®re des tokens pour chaque contexte du batch
    for (const context of contexts) {
      const tokens: string[] = [];
      for (let i = 0; i < count; i++) {
        const prediction = this.predictNextToken(context.concat(tokens));
        tokens.push(prediction);
      }
      batchResults.push(tokens);
    }
    
    return batchResults;
  }

  private predictNextToken(context: string[]): string {
    // Simule une pr√©diction (en vrai : model inference)
    const vocabulary = ['the', 'is', 'a', 'to', 'in', 'and', 'of', 'for', 'that', 'with'];
    return vocabulary[Math.floor(Math.random() * vocabulary.length)];
  }
}

class MockExpertModel {
  private kvCache: Map<string, any> = new Map(); // Simule le KV-cache

  async verify(context: string[], draftTokens: string[]): Promise<string[]> {
    const cacheKey = context.join('|');
    
    // ‚úÖ Si le contexte est en cache, la v√©rification est plus rapide
    if (this.kvCache.has(cacheKey)) {
      await new Promise(r => setTimeout(r, EXPERT_MODEL_SPEED * 0.5)); // 50% plus rapide
      console.log('[MockExpertModel] KV-cache hit ! V√©rification acc√©l√©r√©e.');
    } else {
      await new Promise(r => setTimeout(r, EXPERT_MODEL_SPEED));
      this.kvCache.set(cacheKey, true); // Mise en cache
    }

    const acceptCount = this.getAcceptedCount(draftTokens, context);
    return acceptCount === 0 
      ? [this.generateCorrectToken(context)]
      : draftTokens.slice(0, acceptCount);
  }

  // Version batch√©e du mod√®le expert
  async verifyBatch(contexts: string[][], draftTokensBatch: string[][]): Promise<string[][]> {
    const batchResults: string[][] = [];
    
    // Simule le traitement matriciel du batch
    await new Promise(r => setTimeout(r, EXPERT_MODEL_SPEED));
    
    // V√©rifie chaque paire context/draftTokens du batch
    for (let i = 0; i < contexts.length; i++) {
      const context = contexts[i];
      const draftTokens = draftTokensBatch[i];
      
      const cacheKey = context.join('|');
      
      // ‚úÖ Si le contexte est en cache, la v√©rification est plus rapide
      if (this.kvCache.has(cacheKey)) {
        console.log('[MockExpertModel] KV-cache hit ! V√©rification acc√©l√©r√©e.');
      } else {
        this.kvCache.set(cacheKey, true); // Mise en cache
      }

      const acceptCount = this.getAcceptedCount(draftTokens, context);
      const result = acceptCount === 0 
        ? [this.generateCorrectToken(context)]
        : draftTokens.slice(0, acceptCount);
        
      batchResults.push(result);
    }
    
    return batchResults;
  }

  private getAcceptedCount(draftTokens: string[], context: string[]): number {
    // Estime la "difficult√©" du contexte
    const difficulty = this.estimateDifficulty(context);
    
    // Taux de succ√®s : 0.9 (facile) √† 0.6 (difficile)
    const successRate = 0.9 - (difficulty * 0.3);
    
    if (Math.random() < successRate) {
      return draftTokens.length; // Tous accept√©s
    }
    
    // Accepte partiellement (en moyenne 50% des tokens)
    return Math.floor(draftTokens.length * (0.5 + Math.random() * 0.5));
  }

  private estimateDifficulty(context: string[]): number {
    const lastTokens = context.slice(-10).join(' ').toLowerCase();
    
    // Mots complexes = difficult√© √©lev√©e
    const complexWords = ['algorithm', 'quantum', 'derivative', 'optimization', 'function', 'calculate'];
    const complexityScore = complexWords.filter(w => lastTokens.includes(w)).length;
    
    return Math.min(1, complexityScore / 3); // 0 √† 1
  }

  private generateCorrectToken(context: string[]): string {
    // En cas de rejet total, g√©n√®re un token correct
    const vocabulary = ['token', 'word', 'result', 'answer', 'solution'];
    return vocabulary[Math.floor(Math.random() * vocabulary.length)];
  }
}

class BatchedSpeculativeEngine {
  private draftModel = new MockDraftModel();
  private expertModel = new MockExpertModel();
  
  private getOptimalSpecLength(batchSize: number): number {
    // Gros batch ‚Üí sp√©culation courte (plus de parall√©lisme compute)
    // Petit batch ‚Üí sp√©culation longue (plus de parallelism draft)
    return Math.max(2, 8 - batchSize / 2);
  }

  async *generateBatch(tasks: ExpertTask[]): AsyncGenerator<TaskResult> {
    // Regroupe par longueur de contexte (groupes de s√©quences similaires)
    const sequenceGroups = this.groupBySequenceLength(tasks);
    
    for (const group of sequenceGroups) {
      const batchSize = Math.min(group.tasks.length, MAX_BATCH_SIZE);
      const specLength = this.getOptimalSpecLength(batchSize);
      
      // Speculative decoding PARALL√àLE sur tout le batch
      const draftTokensBatch = await this.draftModel.generateSpeculativeBatch(
        group.contexts, specLength
      ); // Shape: [batch_size, spec_length]
      
      const verifiedBatch = await this.expertModel.verifyBatch(
        group.contexts, draftTokensBatch
      ); // Une seule passe matricielle
      
      // Yield r√©sultats par t√¢che
      for (let i = 0; i < group.tasks.length; i++) {
        const validatedTokens = verifiedBatch[i];
        const acceptRate = validatedTokens.length / specLength;
        
        yield {
          taskId: group.tasks[i].id,
          tokens: validatedTokens,
          metadata: { 
            acceptRate,
            speedup: this.calculateSpeedup(acceptRate, specLength)
          }
        };
      }
    }
  }

  private groupBySequenceLength(tasks: ExpertTask[]): any[] {
    // Regroupe les t√¢ches par longueur de contexte similaire
    const groups: any[] = [];
    const sortedTasks = [...tasks].sort((a, b) => a.context.length - b.context.length);
    
    for (let i = 0; i < sortedTasks.length; i += MAX_BATCH_SIZE) {
      const batch = sortedTasks.slice(i, i + MAX_BATCH_SIZE);
      groups.push({
        tasks: batch,
        contexts: batch.map(t => t.context)
      });
    }
    
    return groups;
  }

  private calculateSpeedup(acceptRate: number, specLength: number): number {
    // Calcule le speedup th√©orique bas√© sur le taux d'acceptation
    if (acceptRate < 0.3) return 1.0; // Fallback classique
    return 1 + (acceptRate * specLength * 0.5); // Approximation du gain
  }
}

export class MockEngine {
  private draftModel = new MockDraftModel();
  private expertModel = new MockExpertModel();
  private batchedEngine = new BatchedSpeculativeEngine();
  private speculationLength = 4;
  private readonly MIN_LENGTH = 2;
  private readonly MAX_LENGTH = 8;
  private useSpeculativeDecoding = true;
  private pendingTasks: ExpertTask[] = [];

  private speculativeMetrics = {
    totalTokens: 0,
    speculatedTokens: 0,
    acceptedTokens: 0,
    rejectedSpeculations: 0,
    avgAcceptRate: 0,
    speedup: 0
  };

  /**
   * Simule la g√©n√©ration de tokens avec "speculative decoding".
   */
  public async *generate(prompt: string, modelKey: string): AsyncGenerator<string> {
    const startTime = performance.now();
    let tokensGenerated = 0;
    
    console.log(`[MockEngine] D√©but de la g√©n√©ration pour ${modelKey} avec Speculative Decoding...`);
    
    const context = [prompt]; // Historique des tokens
    
    // V√©rifie si le speculative decoding est efficace
    const recentAcceptRate = this.calculateRecentAcceptRate();
    if (recentAcceptRate < 0.3) {
      console.warn('[MockEngine] Speculative inefficace, passage en mode classique');
      this.useSpeculativeDecoding = false;
    }

    if (!this.useSpeculativeDecoding) {
      yield* this.generateClassical(prompt, modelKey);
      return;
    }

    while (!this.shouldStop(context)) {
      // --- Phase 1: DRAFT g√©n√®re des tokens ---
      const draftTokens = await this.draftModel.generateSpeculative(
        context, 
        this.speculationLength // ‚úÖ Longueur adaptative
      );
      this.speculativeMetrics.speculatedTokens += draftTokens.length;

      // --- Phase 2: EXPERT valide en une seule passe ---
      const validatedTokens = await this.expertModel.verify(context, draftTokens);
      this.speculativeMetrics.acceptedTokens += validatedTokens.length;
      
      if (validatedTokens.length < draftTokens.length) {
        this.speculativeMetrics.rejectedSpeculations++;
      }

      // ‚úÖ Adapte la longueur selon le succ√®s
      this.adjustSpeculationLength(validatedTokens.length, draftTokens.length);

      // Yield les tokens valid√©s
      for (const token of validatedTokens) {
        yield token + ' ';
        context.push(token); // ‚úÖ Met √† jour le contexte
        tokensGenerated++;
      }
    }
    
    const duration = performance.now() - startTime;
    this.speculativeMetrics.totalTokens += tokensGenerated;
    this.speculativeMetrics.avgAcceptRate = 
      this.speculativeMetrics.acceptedTokens / this.speculativeMetrics.speculatedTokens;

    // Calcule le speedup par rapport au mode classique
    const classicalTime = tokensGenerated * EXPERT_MODEL_SPEED;
    this.speculativeMetrics.speedup = classicalTime / duration;

    console.log(`[MockEngine] Speedup: ${this.speculativeMetrics.speedup.toFixed(2)}x`);
    console.log(`[MockEngine] Fin de la g√©n√©ration pour ${modelKey}.`);
  }

  // Nouvelle m√©thode pour le traitement batch√©
  public async *generateBatched(prompt: string, modelKey: string): AsyncGenerator<string> {
    const taskId = `task_${Date.now()}`;
    const task: ExpertTask = {
      id: taskId,
      prompt,
      modelKey,
      context: [prompt]
    };
    
    // Ajoute la t√¢che √† la file d'attente
    this.pendingTasks.push(task);
    
    // Si assez de t√¢ches, active le batching
    if (this.pendingTasks.length >= BATCH_THRESHOLD) {
      console.log(`[MockEngine] Activation du batching avec ${this.pendingTasks.length} t√¢ches`);
      
      // Traite le batch
      const results = this.batchedEngine.generateBatch(this.pendingTasks);
      
      // Traite les r√©sultats
      for await (const result of results) {
        const task = this.pendingTasks.find(t => t.id === result.taskId);
        if (task) {
          console.log(`[MockEngine] R√©sultat batch pour t√¢che ${result.taskId}: ${result.tokens.length} tokens`);
          
          // Yield les tokens
          for (const token of result.tokens) {
            yield token + ' ';
          }
        }
      }
      
      // Vide la file d'attente
      this.pendingTasks = [];
    } else {
      // Sinon, utilise la g√©n√©ration normale
      yield* this.generate(prompt, modelKey);
    }
  }

  private shouldStop(context: string[]): boolean {
    return context.length > 100 || context[context.length - 1] === '<EOS>';
  }

  private adjustSpeculationLength(accepted: number, attempted: number): void {
    const acceptRate = accepted / attempted;

    if (acceptRate > 0.9 && this.speculationLength < this.MAX_LENGTH) {
      this.speculationLength++;
      console.log(`[MockEngine] Taux d'acceptation √©lev√© ‚Üí augmente √† ${this.speculationLength}`);
    } else if (acceptRate < 0.5 && this.speculationLength > this.MIN_LENGTH) {
      this.speculationLength--;
      console.log(`[MockEngine] Taux d'acceptation faible ‚Üí r√©duit √† ${this.speculationLength}`);
    }
  }

  private calculateRecentAcceptRate(): number {
    if (this.speculativeMetrics.speculatedTokens === 0) return 1;
    return this.speculativeMetrics.acceptedTokens / this.speculativeMetrics.speculatedTokens;
  }

  private async *generateClassical(prompt: string, modelKey: string): AsyncGenerator<string> {
    // Mode classique : 1 token √† la fois
    console.log(`[MockEngine] G√©n√©ration classique pour ${modelKey}...`);
    const tokens = prompt.split(' ');
    for (const token of tokens) {
      await new Promise(r => setTimeout(r, EXPERT_MODEL_SPEED));
      yield token + ' ';
    }
  }

  public getMetrics() {
    return {
      ...this.speculativeMetrics,
      avgAcceptRate: (this.speculativeMetrics.avgAcceptRate * 100).toFixed(1) + '%'
    };
  }
}
```

#### 2. Mise √† jour du TaskExecutor pour supporter le batching

```typescript
// src/core/kernel/TaskExecutor.ts (Mise √† jour)

import { MockEngine } from './engine/MockEngine';
// ... autres imports

class TaskExecutor {
  private mockEngine = new MockEngine();
  // ... autres propri√©t√©s

  async executeTask(task: Task): Promise<any> {
    // ... code existant

    // Utilise la g√©n√©ration batch√©e si disponible
    if (this.shouldUseBatching()) {
      console.log('[TaskExecutor] Utilisation de la g√©n√©ration batch√©e');
      const generator = this.mockEngine.generateBatched(task.prompt, task.modelKey);
      
      let result = '';
      for await (const token of generator) {
        result += token;
        // Envoie les tokens au fur et √† mesure
        this.sendPartialResult(task.id, token);
      }
      
      return result;
    } else {
      // Utilise la g√©n√©ration normale
      const generator = this.mockEngine.generate(task.prompt, task.modelKey);
      
      let result = '';
      for await (const token of generator) {
        result += token;
        // Envoie les tokens au fur et √† mesure
        this.sendPartialResult(task.id, token);
      }
      
      return result;
    }
  }

  private shouldUseBatching(): boolean {
    // Active le batching selon certaines conditions
    // Par exemple, si plusieurs t√¢ches sont en attente
    return true; // Pour l'instant, toujours actif
  }

  private sendPartialResult(taskId: string, token: string): void {
    // Envoie le token partiel au client
    // ... impl√©mentation
  }

  // ... reste du code
}

export const taskExecutor = new TaskExecutor();
```

### R√©sultats Attendus
1. Impl√©mentation de la logique de g√©n√©ration sp√©culative dans le MockEngine
2. Simulation r√©aliste du comportement draft/validation avec vraie g√©n√©ration
3. Gain de vitesse spectaculaire dans les cas de succ√®s (jusqu'√† 2x)
4. Affichage de blocs de tokens entiers pour une exp√©rience utilisateur fluide
5. Compatibilit√© totale avec l'architecture existante
6. R√©duction de la latence per√ßue gr√¢ce √† la g√©n√©ration anticip√©e
7. Am√©lioration de l'exp√©rience utilisateur avec des d√©bits multipli√©s
8. Efficacit√© √©nerg√©tique gr√¢ce √† la r√©duction des appels au mod√®le expert
9. Architecture robuste qui absorbe les am√©liorations complexes sans modification
10. Syst√®me intelligent qui simule un "cerveau reptilien" pour pr√©dire sa propre pens√©e
11. Gestion du KV-cache pour simulation r√©aliste
12. M√©triques de performance pour suivre l'efficacit√© du syst√®me
13. Adaptation dynamique de la longueur de sp√©culation
14. Mode fallback vers la g√©n√©ration classique si speculative decoding inefficace
15. Support du batching pour maximiser le throughput GPU
16. Traitement parall√®le de plusieurs requ√™tes
17. Optimisation de l'utilisation du GPU/WebGPU
18. R√©duction significative de la latence par requ√™te