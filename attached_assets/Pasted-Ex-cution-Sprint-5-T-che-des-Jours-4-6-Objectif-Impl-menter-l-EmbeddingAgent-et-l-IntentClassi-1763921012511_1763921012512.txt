Exécution : Sprint 5 - Tâche des Jours 4-6
Objectif : Implémenter l'EmbeddingAgent et l'IntentClassifier
Philosophie : "Spécialisation et Efficacité." Nous créons deux experts distincts : un pour transformer les mots en sens (vecteurs), et un pour comprendre l'intention derrière les mots.
Sous-Étape 1 : Implémentation de l'EmbeddingAgent (Jours 4-5)
C'est le "Mémoricien" de Kensho. Son seul rôle est de charger le modèle d'embedding et de l'exposer comme un service.
1. Création du Fichier de l'Agent :
Bash

mkdir -p src/agents/embedding
touch src/agents/embedding/index.ts

2. Code de l'EmbeddingAgent :
TypeScript

// src/agents/embedding/index.ts
import { runAgent } from '../../core/agent-system/defineAgent';
import { AgentRuntime } from '../../core/agent-system/AgentRuntime';
import { pipeline, Pipeline } from '@xenova/transformers';

const EMBEDDING_MODEL = 'Xenova/all-MiniLM-L6-v2';

runAgent({
    name: 'EmbeddingAgent',
    init: async (runtime: AgentRuntime) => {
        let extractor: Pipeline | null = null;

        async function getExtractor() {
            if (!extractor) {
                runtime.log('info', `[EmbeddingAgent] Chargement du modèle d'embedding: ${EMBEDDING_MODEL}...`);
                // On charge le modèle avec un suivi de progression
                extractor = await pipeline('feature-extraction', EMBEDDING_MODEL, {
                    progress_callback: (progress: any) => {
                        runtime.log('debug', `[EmbeddingAgent] Chargement: ${progress.file} (${Math.round(progress.progress)}%)`);
                    }
                });
                runtime.log('info', '[EmbeddingAgent] Modèle d\'embedding prêt.');
            }
            return extractor;
        }

        // Pré-chauffer le modèle en arrière-plan
        getExtractor();

        // --- Implémentation du Rate-Limiter (debounce) ---
        let embeddingQueue: { text: string, resolve: (value: number[]) => void, reject: (reason?: any) => void }[] = [];
        let isProcessing = false;

        const processQueue = async () => {
            if (isProcessing || embeddingQueue.length === 0) return;
            isProcessing = true;

            const itemsToProcess = [...embeddingQueue];
            embeddingQueue = [];

            try {
                const texts = itemsToProcess.map(item => item.text);
                runtime.log('debug', `[EmbeddingAgent] Traitement d'un batch de ${texts.length} textes.`);
                const extractorInstance = await getExtractor();
                
                // Le batching est géré nativement par la bibliothèque
                const embeddings = await extractorInstance(texts, { pooling: 'mean', normalize: true });

                // Résoudre chaque promesse
                embeddings.tolist().forEach((embedding: number[], i: number) => {
                    itemsToProcess[i].resolve(embedding);
                });

            } catch (error) {
                itemsToProcess.forEach(item => item.reject(error));
            } finally {
                isProcessing = false;
                // S'il y a de nouveaux éléments dans la file, relancer
                if (embeddingQueue.length > 0) {
                    processQueue();
                }
            }
        };

        // Déclencher le traitement toutes les 500ms s'il y a des éléments
        setInterval(processQueue, 500);

        runtime.registerMethod(
            'embed',
            (payload: { text: string }): Promise<number[]> => {
                return new Promise((resolve, reject) => {
                    embeddingQueue.push({ text: payload.text, resolve, reject });
                });
            }
        );
    }
});

    Commentaire :
        Le modèle est chargé de manière "lazy" à la première utilisation.
        Le rate-limiter est implémenté avec une file d'attente et un setInterval. Cela garantit que même si 50 requêtes arrivent en 100ms, elles seront traitées en un seul batch efficace 500ms plus tard, évitant de surcharger le thread.
        La bibliothèque @xenova/transformers gère nativement le batching, ce qui est très performant.

Sous-Étape 2 : Implémentation de l'IntentClassifier (Jour 6)
C'est le "gardien" qui oriente les requêtes de l'utilisateur. Nous implémentons l'approche "hot path" que nous avons validée.
1. Création du Fichier de l'Agent :
Bash

mkdir -p src/agents/intent-classifier
touch src/agents/intent-classifier/index.ts

2. Code de l'IntentClassifier :
TypeScript

// src/agents/intent-classifier/index.ts
import { runAgent } from '../../core/agent-system/defineAgent';
import { AgentRuntime } from '../../core/agent-system/AgentRuntime';

// Définition des types d'intention
export type Intent = 
    | { type: 'MEMORIZE', content: string, confidence: number }
    | { type: 'FORGET', content: string, scope: 'ALL' | 'NODE', confidence: number }
    | { type: 'CHAT', confidence: number };

// Patterns Regex pour le "hot path"
const INTENT_PATTERNS = {
    FORGET: [
        { pattern: /oublie tout sur (.+)/i, extract: '$1', scope: 'ALL', confidence: 0.99 },
        { pattern: /supprime (.+) de ta mémoire/i, extract: '$1', scope: 'NODE', confidence: 0.98 },
        { pattern: /efface (.+)/i, extract: '$1', scope: 'NODE', confidence: 0.95 },
    ],
    MEMORIZE: [
        { pattern: /retiens que (.+)/i, extract: '$1', confidence: 0.99 },
        { pattern: /souviens-toi que (.+)/i, extract: '$1', confidence: 0.99 },
        { pattern: /note que (.+)/i, extract: '$1', confidence: 0.95 },
        { pattern: /mon (.+) est (.+)/i, extract: '$1 est $2', confidence: 0.90 },
    ],
};

runAgent({
    name: 'IntentClassifierAgent',
    init: (runtime: AgentRuntime) => {
        // Pour ce sprint, nous n'implémentons pas le fallback LLM,
        // nous nous concentrons sur le hot path qui couvre 80% des cas.
        // Le fallback sera une tâche pour un sprint futur.

        runtime.registerMethod(
            'classify',
            async (payload: { text: string }): Promise<Intent> => {
                const text = payload.text.trim();

                // Hot Path : Regex
                for (const [intentType, rules] of Object.entries(INTENT_PATTERNS)) {
                    for (const rule of rules) {
                        const match = text.match(rule.pattern);
                        if (match) {
                            const content = rule.extract.replace(/\$(\d+)/g, (_, n) => match[parseInt(n)]);
                            runtime.log('debug', `[IntentClassifier] Intent détecté par Regex: ${intentType}`);
                            return {
                                type: intentType as 'MEMORIZE' | 'FORGET',
                                content: content.trim(),
                                scope: (rule as any).scope || 'NODE',
                                confidence: rule.confidence,
                            } as Intent;
                        }
                    }
                }

                // Si aucune regex ne correspond, c'est une simple conversation
                runtime.log('debug', `[IntentClassifier] Aucune intention spécifique détectée. Type: CHAT.`);
                return { type: 'CHAT', confidence: 0.5 };
            }
        );
    }
});

    Commentaire :
        L'implémentation est extrêmement rapide et légère, car elle n'utilise que des expressions régulières.
        Elle gère les différents verbes (retiens, note, oublie, efface) et même des structures de phrase simples (mon projet est X).
        Elle retourne un objet Intent structuré, prêt à être utilisé par l'OIE.

Conclusion de la Tâche
La tâche des Jours 4-6 est terminée. Kensho a maintenant les "sens" nécessaires pour comprendre et traiter le langage en vue de la mémorisation.

    Ce qui est fait :
        Un EmbeddingAgent robuste et performant, capable de traiter des requêtes en batch.
        Un IntentClassifier ultra-rapide qui couvre les cas d'usage les plus courants pour la gestion de la mémoire.
