Points forts : La philosophie "Ne jamais faire confiance à un LLM" est absolument cruciale – les LLMs sont imprévisibles, et bâtir une "coquille de robustesse" autour (via JSONExtractor) est une approche senior-level pour mitiger le bruit, les hallucinations formatées, et les erreurs. Commencer par ça avant l'implé full du planner est malin : ça évite les frustrations futures. Le JSONExtractor est solide – il gère les cas réels (Markdown blocks, comments, trailing commas) sans overcompliquer (pas de regex folles ou ML pour parse). Les étapes (extract Markdown → delimit braces → cleanup → parse) sont logiques et couvrent 90% des fails LLM. Warn sur parse fail sans fallback hasardeux est prudent pour éviter garbage in/garbage out.
Potentiel d'amélioration : Dans cleanupJSONString, ajoute un handling pour guillemets simples (convertir en doubles si safe) ou escaped chars – même si tu le laisses fail dans les tests, un fix optionnel pourrait booster la robustesse. Pour extractFromMarkdown, supporte variations comme ```JSON ou sans espace. Ajoute un param pour strictness (ex. : allow partial parse si array incomplet).
Sur les Tests Unitaires (JSONExtractor.test.ts)
Points forts : Les 7+ cas (parfait, texte avant/après, comments ligne/bloc, Markdown, virgule finale, fails comme guillemets simples ou no JSON) sont exhaustifs et réalistes – basés sur des pains LLM communs. Utiliser Vitest avec boucle sur testCases rend ça scalable pour ajouter plus. Expect toEqual(null) sur fails force la robustesse.
Potentiel d'amélioration : Ajoute cas nested (JSON deep) ou huge strings pour perf. Teste avec real LLM outputs (ex. : copy-paste d'une gen Phi-3 malformée). Intègre à un test integration avec un mock LLM pour e2e.
Sur l'Amélioration du Prompt (prompts.ts)
Points forts : Massive upgrade ! Forcer Markdown block + règles absolues (no comments, start/end with ```json) aligne parfaitement avec l'extractor – ça maximise les chances de parse clean dès la source. Intégrer Chain-of-Thought (analyse → détection tool → décision) est genius pour guider Phi-3 vers des plans logiques, sans overprompt. Ajouter "thought" field dans JSON pour debug est pro – permet logging sans polluer le plan. Dynamically injecter manifests (comme calculator) rend ça extensible (ajoute tools futurs sans rewrite). Le processus binaire (direct response vs. tool call + formulate) est simple pour start, mais scalable.
Potentiel d'amélioration : Ajoute exemples dans prompt (few-shot : "Pour query '2+2=?' → steps: [{call Calc}, {MainLLM with result}]"). Pour multi-tools futurs, étends CoT à "prioriser tools si multiple matches". Test prompt avec Phi-3 local pour iterer (ex. : gen 10 plans, check parse rate).
Avis Global sur le Sprint 3 (Avec Cette Tâche)
Ce qui rocks : T'avances vers un vrai reasoning engine – avec Phi-3 + tools comme Calc + ce planner foundation, Kensho passe de chatbot basique à agent capable de decompose tasks (ReAct-like). Cette tâche fait 90% du heavy lifting pour le planner : robust parse + guided prompt = fiabilité. L'ensemble (upgrade modèle → tool agent → planner prep) est cohérent, et prépare un LLMPlanner qui "pensera" via CoT mais outputtera structured actions.
Risques potentiels : Si LLM ignore rules (rare avec Phi-3 instruct), extractor fail → fallback à naive planner ? Test avec noisy outputs. Perf : extract sur long texts, mais c'est light.