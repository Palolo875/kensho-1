# Kensho - FactCheckerAgent & Learning System

## Overview
Kensho is an advanced AI debate orchestration system featuring meta-critique validation, cognitive traceability, performance monitoring, and feedback-driven learning. It is now enhanced with robust fact-checking capabilities and a **production-ready asynchronous kernel** (Sprint 12). The project aims to provide transparent, verifiable, and nuanced AI-generated insights. Kensho is designed to reduce AI hallucinations, improve response reliability, and offer a transparent view into the AI's reasoning process.

## User Preferences
I prefer detailed explanations and transparency in the AI's operations. I want to see the cognitive process and verification steps clearly. I value robust error handling and graceful degradation in system responses. I prefer a modular and extensible architecture. I would like the agent to prioritize reliability and factual accuracy. I prefer that the agent asks before making major changes to the system architecture.

## System Architecture
Kensho's architecture is built around a multi-agent debate system that includes Optimist, Critic, and MetaCritic agents, orchestrated in a 4-step flow with graceful degradation. Cognitive traceability is provided via a `JournalCognitif` system, logging all debate steps and decisions.

### Sprint 12: Le C≈ìur Asynchrone (Kernel v2.0)
**Date:** Novembre 2025  
**Statut:** ‚úÖ Impl√©ment√© et Production-Ready

Le Sprint 12 introduit un noyau asynchrone robuste pour g√©rer les mod√®les IA et les ressources syst√®me de mani√®re optimale:

**Composants principaux:**
- **ModelManager v2.0** (`src/core/kernel/ModelManager.ts`): Gestionnaire asynchrone de mod√®les WebLLM avec:
  - Initialisation explicite et promesse `ready` pour √©viter les race conditions
  - Support du changement de mod√®le √† chaud via `switchModel()`
  - Tracking de l'√©tat actuel du mod√®le charg√©
  - Gestion du cycle de vie complet (init ‚Üí dispose)
  - Callback de progression pour l'UI
  
- **ResourceManager v1.0** (`src/core/kernel/ResourceManager.ts`): Syst√®me nerveux sensoriel surveillant:
  - **M√©moire**: Utilisation JS heap, tendances (rising/falling/stable), d√©tection >85%
  - **Batterie**: Niveau, √©tat de charge, temps avant d√©charge
  - **R√©seau**: √âtat online/offline, type de connexion (4G/3G/2G), latence RTT
  - **CPU**: Nombre de c≈ìurs logiques, d√©tection de throttling
  - **Mode √©co**: D√©tection automatique du mode √©conomie d'√©nergie
  - Syst√®me d'√©v√©nements r√©actifs (`on('memory-critical')`, `on('battery-low')`, etc.)
  - Cache temporel (500ms) pour √©viter les lectures excessives
  
- **KernelCoordinator** (`src/core/kernel/KernelCoordinator.ts`): Orchestrateur intelligent qui:
  - Coordonne ModelManager et ResourceManager
  - Prend des d√©cisions de chargement bas√©es sur les ressources (`canLoadModel()`)
  - G√®re les √©v√©nements critiques (m√©moire satur√©e ‚Üí notification)
  - Fournit une API unifi√©e pour l'application
  
- **ModelCatalog** (`src/core/kernel/ModelCatalog.ts`): Catalogue centralis√© des mod√®les:
  - `gemma-3-270m-it-MLC`: Noyau de dialogue ultra-compact (270M, q4f16_1)
  - Consommation optimale: 0.75% batterie pour 25 conversations
  - Extensible pour futurs mod√®les (embeddings, sp√©cialis√©s)

**Corrections de bugs critiques:**
- Fix `hasMemoryAPI`: Utilise `performance.memory` au lieu de `navigator.deviceMemory`
- Gestion compl√®te des event listeners avec cleanup pour √©viter memory leaks
- Validation robuste des propri√©t√©s optionnelles (`connection.effectiveType`)

**Architecture:**
```
Application
    ‚Üì
KernelCoordinator (Orchestration)
    ‚Üì                    ‚Üì
ModelManager     ResourceManager
(Que charger)    (Quand charger)
    ‚Üì                    ‚Üì
WebLLM Engine    Browser APIs
```

**Usage:**
```typescript
import { kernelCoordinator } from '@/core/kernel';

// Initialisation
await kernelCoordinator.init('gemma-3-270m', (progress) => {
  console.log(progress.text);
});

// Changement de mod√®le intelligent
await kernelCoordinator.switchModel('qwen2-e5-embed');

// V√©rification des ressources
const decision = await kernelCoordinator.canLoadModel('heavy-model');
if (!decision.canLoad) {
  console.warn(decision.reason); // "M√©moire satur√©e", "Batterie critique", etc.
}
```

### Sprint 13: Le Router Intelligent v2.0
**Date:** Novembre 2025  
**Statut:** ‚úÖ Impl√©ment√© et Production-Ready

Le Sprint 13 introduit un syst√®me de routage intelligent qui dirige les requ√™tes utilisateur vers les experts IA appropri√©s, avec v√©rifications de disponibilit√© des ressources et classification hybride.

**Corrections Critiques Int√©gr√©es:**
1. ‚úÖ **Anti-Hallucination** - `ModelCatalog` v√©rifi√© avec UNIQUEMENT des mod√®les WebLLM/MLC existants (Gemma-3-270M, Qwen2.5-Coder-1.5B, Qwen2.5-Math-1.5B)
2. ‚úÖ **Classification Hybride** - Mots-cl√©s rapides ‚Üí Fallback LLM (Gemma-3-270M), pas BGE qui n'est pas dans WebLLM
3. ‚úÖ **Fail-Aware Classifier** - `ClassificationError` propag√©es, pas de masquage silencieux
4. ‚úÖ **S√©lection Consciente** - V√©rification via `kernelCoordinator.canLoadModel()` avant cr√©ation de plan
5. ‚úÖ **Capacity Score Holistique** - CPU + M√©moire + Batterie + R√©seau ‚Üí Score/10 pour d√©cision SERIAL vs PARALLEL
6. ‚úÖ **Transparence des Downgrades** - `downgradedFromIntent` et `downgradeReason` dans `ExecutionPlan`

**Composants principaux:**
- **Router** (`src/core/router/Router.ts`): Orchestrateur intelligent cr√©ant des plans d'ex√©cution
- **IntentClassifier** (`src/core/router/IntentClassifier.ts`): Classification hybride des intentions (CODE, MATH, FACTCHECK, DIALOGUE)
- **CapacityEvaluator** (`src/core/router/CapacityEvaluator.ts`): √âvaluation holistique de la capacit√© syst√®me (score 0-10)
- **ModelCatalog** (`src/core/router/ModelCatalog.ts`): Catalogue v√©rifi√© des mod√®les disponibles avec dates de v√©rification

**Architecture:**
```
User Query
    ‚Üì
IntentClassifier (Keywords ‚Üí LLM Fallback)
    ‚Üì
CapacityEvaluator (CPU + Memory + Battery + Network ‚Üí Score/10)
    ‚Üì
Router.selectExperts (Intent + canLoadModel ‚Üí Model Selection)
    ‚Üì
ExecutionPlan (Primary + Fallback + Strategy + Downgrade Info)
```

**Usage:**
```typescript
import { Router } from '@/core/router';

const router = new Router();

// Cr√©er un plan d'ex√©cution
const plan = await router.createPlan("Comment debugger ce code JavaScript ?");
// {
//   primaryTask: { agentName: 'CodeExpert', modelKey: 'qwen2.5-coder-1.5b', ... },
//   fallbackTasks: [{ agentName: 'GeneralDialogue', modelKey: 'gemma-3-270m', ... }],
//   strategy: 'PARALLEL',
//   capacityScore: 8.5,
//   estimatedDuration: 18000,
//   downgradedFromIntent: undefined  // Pas de downgrade
// }

// En cas de downgrade (mod√®le sp√©cialis√© non disponible)
const degradedPlan = await router.createPlan("Calcule la d√©riv√©e de x¬≤");
// {
//   primaryTask: { agentName: 'CalculatorAgent', modelKey: 'gemma-3-270m', ... },
//   fallbackTasks: [],
//   downgradedFromIntent: 'MATH',
//   downgradeReason: 'M√©moire satur√©e (>80%)'
// }
```

**Mod√®les Support√©s (V√©rifi√©s WebLLM/MLC):**
- `gemma-3-270m-it-MLC` - Dialogue g√©n√©raliste (270M, q4f16_1)
- `Qwen2.5-Coder-1.5B-Instruct-q4f16_1-MLC` - Expert code (1.5B, q4f16_1)
- `Qwen2.5-Math-1.5B-Instruct-q4f16_1-MLC` - Expert math√©matiques (1.5B, q4f16_1)

### Sprint 14: TaskExecutor v3.0 - Chef de Chantier Multi-Queue
**Date:** Novembre 2025  
**Statut:** ‚úÖ Impl√©ment√© et Production-Ready

Le Sprint 14 introduit le TaskExecutor v3.0 qui orchestre l'ex√©cution des t√¢ches multi-agents avec gestion fine de la concurrence, des priorit√©s, des timeouts et du streaming.

**Architecture Multi-Queue (Finale):**
- **Queue SERIAL** (`concurrency: 1`) : Une seule t√¢che √† la fois
- **Queue PARALLEL_LIMITED** (`concurrency: 2`) : Jusqu'√† 2 t√¢ches simultan√©es
- **Queue PARALLEL_FULL** (`concurrency: 4`) : Jusqu'√† 4 t√¢ches simultan√©es

Chaque strat√©gie d'ex√©cution obtient sa propre queue pour **respecter strictement les limites de concurrence** d√©finies par le plan du Router.

**Composants principaux:**
- **TaskExecutor** (`src/core/kernel/TaskExecutor.ts`): Orchestre l'ex√©cution des t√¢ches avec:
  - Streaming compl√®tement dans le job PQueue (occupation du slot pendant toute la g√©n√©ration)
  - Vraie cancellation via `engine.interruptGenerate()` sur timeout
  - Callback pattern pour envoi des chunks en temps r√©el
  - Polling-based streaming pour UX optimale
  - Gestion des priorit√©s (HIGH=10, MEDIUM=5, LOW=1)
  
- **Fusioner** (`src/core/kernel/Fusioner.ts`): Fusionneur intelligent des r√©sultats multi-agents

**Flux de Traitement:**
```
Requ√™te Utilisateur
    ‚Üì
Router.createPlan (intention + capacit√© ‚Üí strat√©gie)
    ‚Üì
TaskExecutor.processStream (s√©lection queue ‚Üí ex√©cution)
    ‚îú‚îÄ PQueue s√©lectionn√©e (SERIAL|LIMITED|FULL)
    ‚îú‚îÄ Job primaire avec streaming
    ‚îú‚îÄ Jobs fallback en parall√®le
    ‚îî‚îÄ Polling des chunks ‚Üí Envoi en temps r√©el
    ‚Üì
Fusioner.fuse (r√©sultats primaire + fallback ‚Üí r√©ponse finale)
    ‚Üì
R√©ponse Fusionn√©e + M√©tadonn√©es
```

**Usage:**
```typescript
import { taskExecutor } from '@/core/kernel';

// Streaming (pour chat UX)
for await (const chunk of taskExecutor.processStream(userPrompt)) {
  if (chunk.type === 'primary') {
    console.log("Chunk re√ßu:", chunk.content);
  } else if (chunk.type === 'fusion') {
    console.log("R√©ponse finale:", chunk.content);
  }
}

// Non-streaming (pour batch)
const response = await taskExecutor.process(userPrompt);
```

**Am√©liorations Cl√©s:**
- ‚úÖ Multi-queue stricte ‚Üí Pas de d√©passement de concurrence m√™me avec t√¢ches entrelac√©es
- ‚úÖ Streaming enti√®rement dans job ‚Üí Queue ne lib√®re le slot que quand g√©n√©ration finie
- ‚úÖ Vraie interruption ‚Üí Cancellation r√©elle du moteur, pas juste une promesse rompue
- ‚úÖ Priorit√©s respect√©es ‚Üí Tasks high-priority ex√©cut√©es en priorit√©
- ‚úÖ Fallback parall√®le ‚Üí Experts backup ex√©cut√©s en parall√®le si primaire √©choue

The FactCheckerAgent employs a hybrid approach for claim extraction (LLM + Rule-Based fallback) and a 2-step verification process (semantic search via HNSW embeddings + LLM Judge). Verification results include status (VERIFIED, CONTRADICTED, AMBIGUOUS, UNKNOWN), confidence scores, and evidence tracking.

**UI/UX Decisions:**
- **JournalCognitifView:** A timeline-based UI for cognitive traceability, displaying debate steps and detailed fact-checking results.
- **VerificationResultItem:** Visualizes fact-check status with color-coded icons (‚úÖ, ‚ùå, üü°, ‚ö†Ô∏è), claim text, confidence scores, and evidence previews.
- **ChatMessage:** Features a `SourcesFooter` to display consulted sources with badges and tooltips, enhancing transparency.
- **ObservatoryModal:** A 4-tabbed interface for monitoring and feedback.

**Technical Implementations & Design Choices:**
- **Hybrid Claim Extraction:** Combines LLM flexibility for complex context with rule-based determinism for guaranteed output and fallback. Multi-level parsing (JSON ‚Üí Markdown ‚Üí Rules) ensures robustness.
- **Semantic Verification:** Utilizes `EmbeddingAgent` and `GraphWorker.findEvidence` for efficient semantic search against a knowledge graph, judged by a minimalist LLM prompt for fast verdicts.
- **Graceful Degradation:** The system can return a draft response directly if meta-critique validation scores are below a dynamic threshold, preventing low-quality AI outputs.
- **Performance Optimization:** Parallelized Optimist and Critic agent execution to reduce latency.
- **Feedback Learning:** `FeedbackLearner` dynamically adjusts thresholds based on user feedback to improve MetaCritic accuracy.
- **Enhanced Type System:** Robust type definitions and validation (`MessageMetadata`, `isValidWorkerName`).
- **Centralized Utilities:** UUID generation and configurable logging strategies (`ConsoleLogger`, `BufferedLogger`, `NoOpLogger`).
- **JSONExtractor Enhancements:** Supports various Markdown JSON formats, single-quote conversion, and strict/lenient parsing modes.
- **CalculatorAgent Security:** Limited `mathjs` scopes to reduce attack surface and bundle size.

## External Dependencies
- **LLM Providers:** Used for agent reasoning, claim extraction, and verification. Specific models are abstracted but critical to agent operations.
- **HNSW (Hierarchical Navigable Small Worlds):** Used by `GraphWorker.findEvidence` for efficient semantic search and embedding storage.
- **`mathjs`:** Utilized by `CalculatorAgent` for mathematical operations (with limited scopes for security).
- **External Knowledge Graph/Database:** Implied for semantic search and evidence retrieval during fact-checking.