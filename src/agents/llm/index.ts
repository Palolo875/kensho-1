// src/agents/llm/index.ts
import { runAgent } from '../../core/agent-system/defineAgent';
import { AgentRuntime, AgentStreamEmitter } from '../../core/agent-system/AgentRuntime';
import { ModelLoader } from '../../core/models/ModelLoader';
import { DownloadManager } from '../../core/downloads/DownloadManager';
import * as webllm from '@mlc-ai/web-llm';

const MODEL_ID = 'Phi-3-mini-4k-instruct-q4f32_1-MLC'; // Upgrade Sprint 3 : Phi-3 pour une meilleure qualit√©
const DOWNLOAD_ID = 'llm-model';

// Param√®tres de g√©n√©ration par d√©faut
const DEFAULT_GENERATION_PARAMS = {
    temperature: 0.7,
    max_tokens: 1024,
    top_p: 0.95,
};

// System prompt par d√©faut pour guider le mod√®le
const DEFAULT_SYSTEM_PROMPT = `Tu es un assistant IA intelligent et serviable. 
R√©ponds de mani√®re claire, pr√©cise et dans la m√™me langue que la question pos√©e.
Si tu ne sais pas quelque chose, admets-le honn√™tement.`;

export interface GenerationParams {
    temperature?: number;
    max_tokens?: number;
    top_p?: number;
    system_prompt?: string;
}

let engine: webllm.MLCEngine | null = null;
let modelLoadingPromise: Promise<void> | null = null;
let isDownloadCancelled = false;
const dm = DownloadManager.getInstance();

// Le ModelLoader enverra ses mises √† jour au thread principal via postMessage
const modelLoader = new ModelLoader((progress) => {
    // V√©rifier si l'utilisateur a annul√©
    if (isDownloadCancelled) {
        console.log('[MainLLMAgent] ‚õî T√©l√©chargement annul√© par l\'utilisateur');
        return;
    }
    
    self.postMessage({ type: 'MODEL_PROGRESS', payload: progress });
    // Mettre √† jour le DownloadManager aussi
    dm.updateProgress(DOWNLOAD_ID, {
        id: DOWNLOAD_ID,
        type: 'llm',
        name: 'Mod√®le LLM (Phi-3)',
        status: progress.phase === 'ready' ? 'completed' : 'downloading' as any,
        progress: progress.progress,
    });
}, { allowPause: true });

// G√©rer les messages de pause/reprise/start/cancel du t√©l√©chargement
self.addEventListener('message', (event) => {
    if (event.data.type === 'PAUSE_DOWNLOAD') {
        modelLoader.pause();
        dm.pause(DOWNLOAD_ID);
    } else if (event.data.type === 'RESUME_DOWNLOAD') {
        modelLoader.resume();
        dm.resume(DOWNLOAD_ID);
    } else if (event.data.type === 'CANCEL_DOWNLOAD') {
        // Annuler compl√®tement le t√©l√©chargement
        console.log('[MainLLMAgent] ‚õî Annulation du t√©l√©chargement demand√©e');
        isDownloadCancelled = true;
        modelLoader.cancel();
        dm.markCancelled(DOWNLOAD_ID);
        modelLoadingPromise = null;
        engine = null;
        self.postMessage({
            type: 'MODEL_PROGRESS',
            payload: { phase: 'idle', progress: 0, text: 'T√©l√©chargement annul√©' }
        });
    } else if (event.data.type === 'START_DOWNLOAD') {
        // R√©initialiser isDownloadCancelled si c'est un nouveau t√©l√©chargement
        isDownloadCancelled = false;
        // D√©marrer le t√©l√©chargement √† la demande de l'utilisateur
        if (!modelLoadingPromise) {
            console.log('[MainLLMAgent] üöÄ D√©marrage du chargement du mod√®le (sur demande):', MODEL_ID);
            dm.register(DOWNLOAD_ID, 'llm', 'Mod√®le LLM (Phi-3)', (progress) => {
                console.log(`[MainLLMAgent] üì• ${progress.name}: ${Math.round(progress.progress * 100)}%`);
            });
            modelLoadingPromise = modelLoader.loadModel(MODEL_ID).then(() => {
                engine = modelLoader.getEngine();
                console.log('[MainLLMAgent] ‚úÖ Moteur LLM pr√™t et op√©rationnel');
                dm.unregister(DOWNLOAD_ID);
                self.postMessage({
                    type: 'MODEL_PROGRESS',
                    payload: { phase: 'ready', progress: 1, text: 'Mod√®le pr√™t.' }
                });
            }).catch((error) => {
                console.error('[MainLLMAgent] ‚ùå √âchec du chargement du mod√®le:', error);
                dm.unregister(DOWNLOAD_ID);
                self.postMessage({
                    type: 'MODEL_ERROR',
                    payload: { message: error.message }
                });
            });
        }
    }
});

// NE PAS charger automatiquement le mod√®le
// L'utilisateur d√©cidera quand d√©marrer le t√©l√©chargement
console.log('[MainLLMAgent] ‚è≥ Pr√™t √† recevoir la commande de t√©l√©chargement du mod√®le');
self.postMessage({
    type: 'MODEL_PROGRESS',
    payload: { phase: 'idle', progress: 0, text: 'En attente du d√©marrage du t√©l√©chargement...' }
});

runAgent({
    name: 'MainLLMAgent',
    init: (runtime: AgentRuntime) => {
        console.log('[MainLLMAgent] üöÄ Initialisation...');
        runtime.log('info', `LLM Agent initialis√©. Chargement du mod√®le ${MODEL_ID}...`);
        console.log('[MainLLMAgent] ‚úÖ Pr√™t √† recevoir des requ√™tes de g√©n√©ration');

        // Exposer une m√©thode pour obtenir les capacit√©s du syst√®me
        runtime.registerMethod('getSystemCapabilities', async () => {
            return await ModelLoader.getSystemCapabilities();
        });

        // Exposer la m√©thode de streaming 'generateResponse'
        runtime.registerStreamMethod(
            'generateResponse',
            async (payload: any, stream: AgentStreamEmitter) => {
                console.log('[MainLLMAgent] üì® Requ√™te de g√©n√©ration re√ßue:', payload);

                const [prompt, customParams] = payload.args || [payload, {}];

                if (!engine) {
                    const error = new Error('Le moteur LLM n\'est pas encore pr√™t. Veuillez patienter...');
                    console.error('[MainLLMAgent] ‚ùå Moteur non pr√™t');
                    runtime.log('error', error.message);
                    stream.error(error);
                    return;
                }

                console.log('[MainLLMAgent] ‚úÖ Moteur disponible');

                // Valider le prompt
                if (!prompt || typeof prompt !== 'string' || prompt.trim().length === 0) {
                    const error = new Error('Le prompt doit √™tre une cha√Æne de caract√®res non vide.');
                    console.error('[MainLLMAgent] ‚ùå Prompt invalide:', prompt);
                    runtime.log('error', error.message);
                    stream.error(error);
                    return;
                }

                console.log('[MainLLMAgent] ‚úÖ Prompt valide:', prompt.substring(0, 50) + '...');

                // Sprint 7: Enrichir le system prompt avec contexte projet si actif
                let systemPrompt = customParams?.system_prompt ?? DEFAULT_SYSTEM_PROMPT;
                if (typeof window !== 'undefined') {
                    const kenshoStore = (window as any)?.useKenshoStore?.getState?.();
                    if (kenshoStore?.activeProjectId && kenshoStore?.projects) {
                        const activeProject = kenshoStore.projects.find((p: any) => p.id === kenshoStore.activeProjectId);
                        const projectTasks = kenshoStore.projectTasks?.get(kenshoStore.activeProjectId) || [];
                        if (activeProject) {
                            const ProjectContextBuilder = require('../../core/oie/ProjectContextBuilder').ProjectContextBuilder;
                            const projectContext = ProjectContextBuilder.buildProjectContext(activeProject, projectTasks);
                            systemPrompt = DEFAULT_SYSTEM_PROMPT + '\n' + projectContext;
                        }
                    }
                }

                // Fusionner les param√®tres par d√©faut avec les param√®tres personnalis√©s
                const params: Required<GenerationParams> = {
                    temperature: customParams?.temperature ?? DEFAULT_GENERATION_PARAMS.temperature,
                    max_tokens: customParams?.max_tokens ?? DEFAULT_GENERATION_PARAMS.max_tokens,
                    top_p: customParams?.top_p ?? DEFAULT_GENERATION_PARAMS.top_p,
                    system_prompt: systemPrompt,
                };

                // Valider les param√®tres
                if (params.temperature < 0 || params.temperature > 2) {
                    const error = new Error('Temperature doit √™tre entre 0 et 2.');
                    runtime.log('error', error.message);
                    stream.error(error);
                    return;
                }

                if (params.max_tokens < 1 || params.max_tokens > 4096) {
                    const error = new Error('max_tokens doit √™tre entre 1 et 4096.');
                    runtime.log('error', error.message);
                    stream.error(error);
                    return;
                }

                try {
                    console.log('[MainLLMAgent] üîÑ D√©but de la g√©n√©ration...');
                    runtime.log('info', `D√©but de la g√©n√©ration pour le prompt: "${String(prompt).substring(0, 50)}..." (temp: ${params.temperature}, max_tokens: ${params.max_tokens})`);

                    // Construire les messages avec le system prompt
                    const messages: any[] = [
                        { role: 'system', content: params.system_prompt },
                        { role: 'user', content: String(prompt) }
                    ];

                    console.log('[MainLLMAgent] ü§ñ Appel du moteur LLM...');
                    const streamIterator = await engine.chat.completions.create({
                        messages,
                        stream: true,
                        temperature: params.temperature,
                        max_tokens: params.max_tokens,
                        top_p: params.top_p,
                    });

                    console.log('[MainLLMAgent] üì° Stream d√©marr√©, attente des chunks...');
                    let totalChunks = 0;
                    for await (const chunk of streamIterator) {
                        const textChunk = (chunk as any).choices?.[0]?.delta?.content || '';
                        if (textChunk) {
                            totalChunks++;
                            if (totalChunks === 1) {
                                console.log('[MainLLMAgent] üì¶ Premier chunk re√ßu');
                            }
                            // Envoyer chaque morceau de texte via le stream
                            stream.chunk({ text: textChunk });
                        }
                    }

                    console.log(`[MainLLMAgent] ‚úÖ G√©n√©ration termin√©e. ${totalChunks} chunks envoy√©s.`);
                    runtime.log('info', `G√©n√©ration termin√©e. ${totalChunks} chunks envoy√©s.`);
                    stream.end({ totalChunks }); // Signaler la fin du stream

                } catch (error) {
                    const err = error instanceof Error ? error : new Error('Erreur inconnue durant l\'inf√©rence');
                    console.error('[MainLLMAgent] ‚ùå Erreur d\'inf√©rence:', err);
                    runtime.log('error', `Erreur d'inf√©rence: ${err.message}`);
                    stream.error(err);
                }
            }
        );

        // Exposer une m√©thode request/response pour g√©n√©rer une r√©ponse compl√®te (sans streaming)
        // Utilis√©e par le LLMPlanner pour g√©n√©rer les plans
        runtime.registerMethod('generateSingleResponse', async (payload: any) => {
            console.log('[MainLLMAgent] üì® Requ√™te de g√©n√©ration unique re√ßue:', payload);

            const [prompt, customParams] = payload.args || [payload, {}];

            if (!engine) {
                const error = new Error('Le moteur LLM n\'est pas encore pr√™t. Veuillez patienter...');
                console.error('[MainLLMAgent] ‚ùå Moteur non pr√™t');
                runtime.log('error', error.message);
                throw error;
            }

            console.log('[MainLLMAgent] ‚úÖ Moteur disponible');

            // Valider le prompt
            if (!prompt || typeof prompt !== 'string' || prompt.trim().length === 0) {
                const error = new Error('Le prompt doit √™tre une cha√Æne de caract√®res non vide.');
                console.error('[MainLLMAgent] ‚ùå Prompt invalide:', prompt);
                runtime.log('error', error.message);
                throw error;
            }

            console.log('[MainLLMAgent] ‚úÖ Prompt valide pour g√©n√©ration unique');

            // Fusionner les param√®tres par d√©faut avec les param√®tres personnalis√©s
            const params: Required<GenerationParams> = {
                temperature: customParams?.temperature ?? DEFAULT_GENERATION_PARAMS.temperature,
                max_tokens: customParams?.max_tokens ?? DEFAULT_GENERATION_PARAMS.max_tokens,
                top_p: customParams?.top_p ?? DEFAULT_GENERATION_PARAMS.top_p,
                system_prompt: customParams?.system_prompt ?? DEFAULT_SYSTEM_PROMPT,
            };

            // Valider les param√®tres
            if (params.temperature < 0 || params.temperature > 2) {
                throw new Error('Temperature doit √™tre entre 0 et 2.');
            }

            if (params.max_tokens < 1 || params.max_tokens > 4096) {
                throw new Error('max_tokens doit √™tre entre 1 et 4096.');
            }

            try {
                console.log('[MainLLMAgent] üîÑ D√©but de la g√©n√©ration unique...');
                runtime.log('info', `G√©n√©ration unique pour: "${String(prompt).substring(0, 50)}..." (temp: ${params.temperature})`);

                // Construire les messages avec le system prompt
                const messages: any[] = [
                    { role: 'system', content: params.system_prompt },
                    { role: 'user', content: String(prompt) }
                ];

                console.log('[MainLLMAgent] ü§ñ Appel du moteur LLM (mode non-stream)...');
                const response = await engine.chat.completions.create({
                    messages,
                    stream: false, // Mode request/response
                    temperature: params.temperature,
                    max_tokens: params.max_tokens,
                    top_p: params.top_p,
                });

                const textResponse = (response as any).choices?.[0]?.message?.content || '';
                
                console.log(`[MainLLMAgent] ‚úÖ G√©n√©ration unique termin√©e. ${textResponse.length} caract√®res.`);
                runtime.log('info', `G√©n√©ration unique termin√©e. ${textResponse.length} caract√®res g√©n√©r√©s.`);
                
                return textResponse;

            } catch (error) {
                const err = error instanceof Error ? error : new Error('Erreur inconnue durant l\'inf√©rence');
                console.error('[MainLLMAgent] ‚ùå Erreur d\'inf√©rence:', err);
                runtime.log('error', `Erreur d'inf√©rence (mode unique): ${err.message}`);
                throw err;
            }
        });

        // M√©thode de streaming pour synth√©tiser un d√©bat (Sprint 6)
        // Prend la r√©ponse initiale de l'Optimiste et la critique d'Ath√©na,
        // et g√©n√®re une r√©ponse finale √©quilibr√©e
        runtime.registerStreamMethod(
            'synthesizeDebate',
            async (payload: any, stream: AgentStreamEmitter) => {
                console.log('[MainLLMAgent] üß† Synth√®se de d√©bat demand√©e');
                
                const { originalQuery, draftResponse, critique } = payload.args?.[0] || payload;
                
                if (!engine) {
                    const error = new Error('Le moteur LLM n\'est pas encore pr√™t. Veuillez patienter...');
                    console.error('[MainLLMAgent] ‚ùå Moteur non pr√™t');
                    runtime.log('error', error.message);
                    stream.error(error);
                    return;
                }
                
                // Construire le prompt de synth√®se
                const SYNTHESIS_PROMPT = `Tu es un assistant IA qui doit synth√©tiser un d√©bat interne pour fournir une r√©ponse √©quilibr√©e et nuanc√©e.

**CONTEXTE :**
Question originale : "${originalQuery}"

**ANALYSE OPTIMISTE (L√©o) :**
${draftResponse}

**CRITIQUE (Ath√©na) :**
${typeof critique === 'object' ? JSON.stringify(critique, null, 2) : critique}

**TA MISSION :**
Synth√©tise ces deux perspectives pour fournir une r√©ponse finale qui :
1. Reconna√Æt les points forts identifi√©s par L√©o
2. Int√®gre les pr√©occupations l√©gitimes d'Ath√©na
3. Fournit une recommandation √©quilibr√©e et nuanc√©e
4. Reste claire et actionnable pour l'utilisateur

**R√àGLES :**
- Ne mentionne PAS L√©o ni Ath√©na dans ta r√©ponse
- Parle directement √† l'utilisateur
- Sois concis (moins de 250 mots)
- Fournis une r√©ponse pratique et √©quilibr√©e

**TA R√âPONSE FINALE :`;

                try {
                    console.log('[MainLLMAgent] üîÑ D√©but de la synth√®se...');
                    runtime.log('info', 'Synth√®se du d√©bat en cours...');

                    const messages: any[] = [
                        { role: 'system', content: DEFAULT_SYSTEM_PROMPT },
                        { role: 'user', content: SYNTHESIS_PROMPT }
                    ];

                    console.log('[MainLLMAgent] ü§ñ Appel du moteur LLM pour la synth√®se...');
                    const streamIterator = await engine.chat.completions.create({
                        messages,
                        stream: true,
                        temperature: 0.7,
                        max_tokens: 1024,
                        top_p: 0.95,
                    });

                    console.log('[MainLLMAgent] üì° Stream de synth√®se d√©marr√©...');
                    let totalChunks = 0;
                    for await (const chunk of streamIterator) {
                        const textChunk = (chunk as any).choices?.[0]?.delta?.content || '';
                        if (textChunk) {
                            totalChunks++;
                            stream.chunk({ text: textChunk });
                        }
                    }

                    console.log(`[MainLLMAgent] ‚úÖ Synth√®se termin√©e. ${totalChunks} chunks envoy√©s.`);
                    runtime.log('info', `Synth√®se du d√©bat termin√©e. ${totalChunks} chunks envoy√©s.`);
                    stream.end({ totalChunks });

                } catch (error) {
                    const err = error instanceof Error ? error : new Error('Erreur inconnue durant la synth√®se');
                    console.error('[MainLLMAgent] ‚ùå Erreur de synth√®se:', err);
                    runtime.log('error', `Erreur de synth√®se: ${err.message}`);
                    stream.error(err);
                }
            }
        );

        // M√©thode pour reset le moteur si n√©cessaire
        runtime.registerMethod('resetEngine', async () => {
            runtime.log('info', 'Reset du moteur LLM...');
            engine = null;
            await modelLoader.loadModel(MODEL_ID);
            engine = modelLoader.getEngine();
            return { success: true };
        });

        // M√©thode pour obtenir les stats du mod√®le
        runtime.registerMethod('getModelStats', async () => {
            if (!engine) {
                return { ready: false, model: MODEL_ID };
            }
            return {
                ready: true,
                model: MODEL_ID,
                // Ajouter d'autres stats si disponibles via web-llm
            };
        });
    }
});
