/**
 * ModelManager v4.0 - Transformers.js + Qwen3-0.6B-ONNX
 * 
 * Utilise AutoTokenizer et AutoModelForCausalLM pour charger
 * le mod√®le Qwen3-0.6B-ONNX depuis Hugging Face avec streaming.
 * T√©l√©chargement optionnel - demande √† l'utilisateur la permission.
 */

import { AutoTokenizer, AutoModelForCausalLM, env } from '@xenova/transformers';
import { sseStreamer } from '../streaming/SSEStreamer';

// Configuration pour permettre le chargement depuis Hugging Face
env.allowLocalModels = false;
env.allowRemoteModels = true;

console.log("üß†‚ú® Initialisation du ModelManager v4.0 (Transformers.js + Qwen3-0.6B-ONNX)...");

export type ModelType = 'gpt2' | 'mock';

export interface ModelInfo {
  id: ModelType;
  name: string;
  huggingFaceId: string;
  size: string;
  description: string;
  isDownloaded: boolean;
}

export interface DownloadProgress {
  percent: number;
  speed: number; // bytes/sec
  timeRemaining: number; // ms
  loaded: number; // bytes
  total: number; // bytes
  file?: string;
}

export type DownloadCallback = (progress: DownloadProgress) => void;

export class ModelManager {
  private tokenizer: any | null = null;
  private model: any | null = null;
  private _ready!: Promise<void>;
  private _resolveReady!: () => void;
  private _rejectReady!: (error: any) => void;
  private isInitialized = false;
  private isInitializing = false;
  private currentModelKey: ModelType = 'mock';
  private downloadedModels: Set<ModelType> = new Set();
  private downloadController: AbortController | null = null;
  private downloadStartTime = 0;
  private downloadedBytes = 0;
  private pausedProgress = 0;

  constructor() {
    this.resetReadyPromise();
  }

  private resetReadyPromise() {
    this._ready = new Promise<void>((resolve, reject) => {
      this._resolveReady = resolve;
      this._rejectReady = reject;
    });
  }

  public get ready(): Promise<void> {
    return this._ready;
  }

  /**
   * Retourne la liste des mod√®les disponibles
   */
  public getAvailableModels(): ModelInfo[] {
    return [
      {
        id: 'gpt2',
        name: 'GPT-2 (Xenova Int4)',
        huggingFaceId: 'Xenova/gpt2',
        size: '~150MB',
        description: 'GPT-2 quantis√© en int4 - Ultra l√©ger et rapide',
        isDownloaded: this.downloadedModels.has('gpt2')
      },
      {
        id: 'mock',
        name: 'Mode Simulation',
        huggingFaceId: 'mock',
        size: 'Aucun',
        description: 'R√©ponses simul√©es (pas de IA)',
        isDownloaded: true
      }
    ];
  }

  /**
   * Retourne le mod√®le actuellement actif
   */
  public getCurrentModel(): ModelType {
    return this.currentModelKey;
  }

  /**
   * Initialise avec le mode simulation (pas de t√©l√©chargement)
   */
  public async initMockMode() {
    this.currentModelKey = 'mock';
    this.downloadedModels.add('mock');
    this.isInitialized = true;
    this.isInitializing = false;
    this._resolveReady();
    console.log("‚úÖ [ModelManager] Mode Simulation activ√©");
  }

  /**
   * Annule le t√©l√©chargement en cours
   */
  public cancelDownload() {
    if (this.downloadController) {
      this.downloadController.abort();
      this.downloadController = null;
    }
    this.isInitializing = false;
    console.log("[ModelManager] T√©l√©chargement annul√©");
  }

  /**
   * Pause/Resume du t√©l√©chargement via un callback
   */
  public pauseDownload() {
    console.log("[ModelManager] Pause du t√©l√©chargement...");
    if (this.downloadController && !this.downloadController.signal.aborted) {
      this.pausedProgress = this.downloadedBytes;
    }
  }

  public resumeDownload() {
    console.log("[ModelManager] Reprise du t√©l√©chargement...");
    // La reprise est g√©r√©e en interne
  }

  /**
   * T√©l√©charge et initialise le mod√®le DistilGPT-2
   * √Ä appeler UNIQUEMENT si l'utilisateur le demande
   */
  public async downloadAndInitQwen3(onProgress?: DownloadCallback) {
    if (this.downloadedModels.has('gpt2')) {
      console.log("[ModelManager] GPT-2 d√©j√† t√©l√©charg√©");
      return;
    }

    if (this.isInitializing && this.currentModelKey === 'gpt2') {
      console.warn("[ModelManager] GPT-2 en cours de t√©l√©chargement, attente...");
      await this.ready;
      return;
    }

    this.isInitializing = true;
    this.downloadController = new AbortController();
    this.downloadStartTime = Date.now();
    this.downloadedBytes = 0;
    this.pausedProgress = 0;
    const modelKey = "Xenova/gpt2";
    const estimatedTotalBytes = 150 * 1024 * 1024; // 150MB (int4 quantized)

    try {
      console.log(`[ModelManager] Chargement de GPT-2 (Xenova)...`);
      sseStreamer.streamInfo(`Chargement du tokenizer...`);
      
      // Charger le tokenizer
      this.tokenizer = await AutoTokenizer.from_pretrained(modelKey);
      
      console.log(`[ModelManager] ‚úÖ Tokenizer pr√™t. Chargement du mod√®le...`);
      sseStreamer.streamInfo(`Chargement du mod√®le...`);
      
      // Charger le mod√®le avec gestion de progression
      try {
        this.model = await AutoModelForCausalLM.from_pretrained(modelKey, {
          quantized: true,
          progress_callback: (progress: any) => {
            if (this.downloadController?.signal.aborted) {
              throw new Error('Download cancelled');
            }

            // Calculer le pourcentage correctement
            let percent = 0;
            if (progress.progress !== undefined) {
              // Si progress est d√©j√† un nombre entre 0-1, le multiplier par 100
              if (progress.progress <= 1) {
                percent = Math.round(progress.progress * 100);
              } else {
                // Sinon, le prendre directement (d√©j√† en pourcentage)
                percent = Math.min(99, Math.round(progress.progress));
              }
            }

            const now = Date.now();
            const elapsedMs = now - this.downloadStartTime;
            const elapsedSec = Math.max(elapsedMs / 1000, 0.1);
            
            const currentBytes = (percent / 100) * estimatedTotalBytes;
            const speed = currentBytes / elapsedSec;
            const remainingBytes = estimatedTotalBytes - currentBytes;
            const timeRemainingMs = speed > 0 ? (remainingBytes / speed) * 1000 : 0;

            this.downloadedBytes = currentBytes;
            
            const progressData: DownloadProgress = {
              percent: Math.min(99, percent),
              speed: Math.max(0, speed),
              timeRemaining: Math.max(0, timeRemainingMs),
              loaded: Math.round(currentBytes),
              total: estimatedTotalBytes,
              file: progress.file || 'GPT-2 model files'
            };
            
            onProgress?.(progressData);
            console.log(`[ModelManager] Progression: ${percent}%`);
            sseStreamer.streamInfo(`T√©l√©chargement: ${percent}%`);
          }
        });
      } catch (error) {
        if ((error as any)?.message === 'Download cancelled') {
          throw error;
        }
        console.warn("[ModelManager] Erreur mineure lors du t√©l√©chargement, continuant...", error);
      }

      this.currentModelKey = 'gpt2';
      this.downloadedModels.add('gpt2');
      this.isInitialized = true;
      this.isInitializing = false;
      this.downloadController = null;
      
      // Envoyer 100%
      onProgress?.({
        percent: 100,
        speed: 0,
        timeRemaining: 0,
        loaded: estimatedTotalBytes,
        total: estimatedTotalBytes,
        file: 'GPT-2 - Pr√™t!'
      });

      this._resolveReady();
      console.log(`‚úÖ [ModelManager] GPT-2 pr√™t pour g√©n√©rer du texte.`);
      sseStreamer.streamInfo(`Mod√®le pr√™t!`);

    } catch (error) {
      if ((error as any)?.name === 'AbortError' || (error as any)?.message === 'Download cancelled') {
        console.log("[ModelManager] T√©l√©chargement annul√© par l'utilisateur");
        this.isInitializing = false;
        this.downloadController = null;
        this.resetReadyPromise();
        return;
      }
      console.error("[ModelManager] Erreur d'initialisation:", error);
      this.isInitializing = false;
      this.downloadController = null;
      this._rejectReady(error);
      this.resetReadyPromise();
      sseStreamer.streamError(error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  /**
   * Change le mod√®le actif
   */
  public async switchToModel(modelKey: ModelType): Promise<void> {
    if (modelKey === 'mock') {
      this.currentModelKey = 'mock';
      console.log("[ModelManager] Switched to mock mode");
      return;
    }

    if (modelKey === 'gpt2') {
      if (!this.downloadedModels.has('gpt2')) {
        throw new Error("GPT-2 n'a pas √©t√© t√©l√©charg√©. Appelez downloadAndInitQwen3() d'abord.");
      }
      if (!this.model || !this.tokenizer) {
        throw new Error("GPT-2 n'a pas pu √™tre initialis√©");
      }
      this.currentModelKey = 'gpt2';
      return;
    }
  }

  /**
   * Obtient le tokenizer et le mod√®le une fois pr√™ts
   */
  public async getModelAndTokenizer(): Promise<{ model: any, tokenizer: any }> {
    await this.ready;
    
    if (this.currentModelKey === 'mock') {
      throw new Error("Mode mock activ√© - pas de vrai mod√®le");
    }

    if (!this.model || !this.tokenizer) {
      throw new Error("Le mod√®le ou le tokenizer ne sont pas initialis√©s.");
    }
    return { model: this.model, tokenizer: this.tokenizer };
  }

  /**
   * G√©n√®re du texte avec streaming via callback
   * Utilise le mod√®le actuellement charg√©
   */
  public async generateStreaming(
    prompt: string,
    onToken: (token: string) => void,
    maxNewTokens: number = 256
  ): Promise<string> {
    if (this.currentModelKey === 'mock') {
      throw new Error("Mode mock - utilise DialoguePluginMock");
    }

    const { model, tokenizer } = await this.getModelAndTokenizer();
    
    try {
      console.log(`[ModelManager] G√©n√©ration d√©marr√©e pour le prompt: "${prompt.substring(0, 50)}..."`);
      
      const inputs = tokenizer(prompt, { return_tensors: "pt" });
      
      let fullResponse = "";
      let lastDecodedLength = 0;
      
      const outputs = await model.generate({
        ...inputs,
        max_new_tokens: maxNewTokens,
        callback_function: (beams: any) => {
          try {
            const decoded = tokenizer.decode(beams[0].output_token_ids, { skip_special_tokens: true });
            
            if (decoded.length > lastDecodedLength) {
              const newToken = decoded.substring(lastDecodedLength);
              lastDecodedLength = decoded.length;
              
              onToken(newToken);
              fullResponse += newToken;
            }
          } catch (e) {
            console.error("[ModelManager] Erreur dans callback:", e);
          }
        }
      });
      
      const finalOutput = tokenizer.decode(outputs[0], { skip_special_tokens: true });
      console.log(`[ModelManager] ‚úÖ G√©n√©ration termin√©e`);
      
      return finalOutput;
    } catch (error) {
      console.error("[ModelManager] Erreur de g√©n√©ration:", error);
      throw error;
    }
  }

  /**
   * V√©rifie si un mod√®le est d√©j√† t√©l√©charg√©
   */
  public isModelDownloaded(modelKey: ModelType): boolean {
    return this.downloadedModels.has(modelKey);
  }
}

// Instance singleton
export const modelManager = new ModelManager();
