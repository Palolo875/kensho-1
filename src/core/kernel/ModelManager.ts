/**
 * ModelManager v4.0 - Transformers.js + Qwen3-0.6B-ONNX
 * 
 * Utilise AutoTokenizer et AutoModelForCausalLM pour charger
 * le mod√®le Qwen3-0.6B-ONNX depuis Hugging Face avec streaming.
 */

import { AutoTokenizer, AutoModelForCausalLM, env } from '@xenova/transformers';
import { sseStreamer } from '../streaming/SSEStreamer';

// Configuration pour permettre le chargement depuis Hugging Face
env.allowLocalModels = false;
env.allowRemoteModels = true;

console.log("üß†‚ú® Initialisation du ModelManager v4.0 (Transformers.js + Qwen3-0.6B-ONNX)...");

export class ModelManager {
  private tokenizer: any | null = null;
  private model: any | null = null;
  private _ready!: Promise<void>;
  private _resolveReady!: () => void;
  private _rejectReady!: (error: any) => void;
  private isInitialized = false;
  private isInitializing = false;

  constructor() {
    this.resetReadyPromise();
  }

  private resetReadyPromise() {
    this._ready = new Promise<void>((resolve, reject) => {
      this._resolveReady = resolve;
      this._rejectReady = reject;
    });
  }

  public get ready(): Promise<void> {
    return this._ready;
  }

  /**
   * Initialise et pr√©charge le mod√®le Qwen3 0.6B et son tokenizer
   */
  public async init(modelKey: string = "onnx-community/Qwen3-0.6B-ONNX") {
    if (this.isInitialized) {
      console.warn("[ModelManager] Init d√©j√† appel√©, ignor√©.");
      return;
    }

    if (this.isInitializing) {
      console.warn("[ModelManager] Init en cours, attente...");
      await this.ready;
      return;
    }

    this.isInitializing = true;

    try {
      console.log(`[ModelManager] Pr√©-chargement du tokenizer...`);
      sseStreamer.streamInfo(`Chargement du tokenizer...`);
      
      // Charger le tokenizer
      this.tokenizer = await AutoTokenizer.from_pretrained(modelKey);
      
      console.log(`[ModelManager] ‚úÖ Tokenizer charg√©. Chargement du mod√®le...`);
      sseStreamer.streamInfo(`Chargement du mod√®le ${modelKey}...`);
      
      // Charger le mod√®le avec callbacks de progression
      this.model = await AutoModelForCausalLM.from_pretrained(modelKey, {
        quantized: true,
        progress_callback: (progress: any) => {
          const percent = Math.round((progress.progress || 0) * 100);
          console.log(`[ModelManager] Progression: ${progress.file} (${percent}%)`);
          sseStreamer.streamInfo(`T√©l√©chargement: ${percent}%`);
        }
      });
      
      this.isInitialized = true;
      this.isInitializing = false;
      
      this._resolveReady();
      console.log(`‚úÖ [ModelManager] ${modelKey} est pr√™t pour g√©n√©rer du texte.`);
      sseStreamer.streamInfo(`Mod√®le pr√™t!`);

    } catch (error) {
      console.error("[ModelManager] Erreur d'initialisation:", error);
      this.isInitializing = false;
      this._rejectReady(error);
      this.resetReadyPromise();
      sseStreamer.streamError(error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  /**
   * Obtient le tokenizer et le mod√®le une fois pr√™ts
   */
  public async getModelAndTokenizer(): Promise<{ model: any, tokenizer: any }> {
    await this.ready;
    if (!this.model || !this.tokenizer) {
      throw new Error("Le mod√®le ou le tokenizer ne sont pas initialis√©s.");
    }
    return { model: this.model, tokenizer: this.tokenizer };
  }

  /**
   * G√©n√®re du texte avec streaming via callback
   */
  public async generateStreaming(
    prompt: string,
    onToken: (token: string) => void,
    maxNewTokens: number = 256
  ): Promise<string> {
    const { model, tokenizer } = await this.getModelAndTokenizer();
    
    try {
      console.log(`[ModelManager] G√©n√©ration d√©marr√©e pour le prompt: "${prompt.substring(0, 50)}..."`);
      
      // Tokeniser le prompt
      const inputs = tokenizer(prompt, { return_tensors: "pt" });
      
      let fullResponse = "";
      const promptLength = prompt.length;
      let lastDecodedLength = 0;
      
      // G√©n√©rer avec callback
      const outputs = await model.generate({
        ...inputs,
        max_new_tokens: maxNewTokens,
        callback_function: (beams: any) => {
          try {
            // D√©coder la s√©quence compl√®te
            const decoded = tokenizer.decode(beams[0].output_token_ids, { skip_special_tokens: true });
            
            // Extraire uniquement le nouveau token
            if (decoded.length > lastDecodedLength) {
              const newToken = decoded.substring(lastDecodedLength);
              lastDecodedLength = decoded.length;
              
              // Envoyer le token √† l'UI
              onToken(newToken);
              fullResponse += newToken;
            }
          } catch (e) {
            console.error("[ModelManager] Erreur dans callback:", e);
          }
        }
      });
      
      // D√©codage final
      const finalOutput = tokenizer.decode(outputs[0], { skip_special_tokens: true });
      console.log(`[ModelManager] ‚úÖ G√©n√©ration termin√©e`);
      
      return finalOutput;
    } catch (error) {
      console.error("[ModelManager] Erreur de g√©n√©ration:", error);
      throw error;
    }
  }
}

// Instance singleton
export const modelManager = new ModelManager();
