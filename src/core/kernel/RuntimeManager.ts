/**
 * RuntimeManager - Production Implementation
 *
 * G√®re le cycle de vie des runtimes d'inf√©rence (WebLLM, Transformers.js).
 * Responsable de :
 * - Initialisation et destruction des moteurs
 * - Gestion de la m√©moire GPU/CPU
 * - D√©tection automatique de WebGPU
 * - Basculement entre diff√©rents backends
 * - Monitoring des performances d'inf√©rence
 * - Retry avec backoff exponentiel
 * - Fallback automatique GPU ‚Üí CPU
 * - Pool de moteurs pour multi-mod√®les
 *
 * En mode d√©veloppement, les appels aux vrais moteurs peuvent √™tre
 * remplac√©s par des mocks via l'injection de d√©pendances.
 */

import { createLogger } from '../../lib/logger';
import { logger } from './monitoring/LoggerService';
import { storageManager, CompiledGraph, COMPILED_GRAPH_VERSION } from './StorageManager';
import { resourceManager } from './ResourceManager';
import { memoryManager } from './MemoryManager';
import {
  MockWebLLMEngine,
  MockTransformersJSEngine,
  createMockEngine,
} from '../runtime/mocks/mock-engines';
import { MockGPUEngine } from './engine/MockGPU.engine';
import { MockCPUEngine } from './engine/MockCPU.engine';
import { sseStreamer } from '../streaming/SSEStreamer';

const log = createLogger('RuntimeManager');

logger.info('RuntimeManager', 'üöÄ RuntimeManager (Production) initialis√©.');

// Types pour les diff√©rents backends support√©s
export type RuntimeBackend = 'webllm' | 'transformers' | 'mock' | 'auto';

export interface RuntimeConfig {
  backend: RuntimeBackend;
  modelId: string;
  options?: InferenceOptions;
}

export interface InferenceOptions {
  maxTokens?: number;
  temperature?: number;
  topP?: number;
  useGPU?: boolean;
}

export interface RuntimeStatus {
  isReady: boolean;
  backend: RuntimeBackend | null;
  modelId: string | null;
  memoryUsage: number;
  lastInferenceTime: number | null;
  totalInferences: number;
  gpuAvailable: boolean;
  gpuInfo: GPUInfo | null;
}

export interface GPUInfo {
  vendor: string;
  architecture: string;
  device: string;
  description: string;
}

export interface InferenceResult {
  text: string;
  tokensGenerated: number;
  timeMs: number;
  finishReason: 'stop' | 'length' | 'error';
}

/**
 * Interface pour les √©v√©nements de progression
 */
interface ProgressEvent {
  modelKey: string;
  stage: string;
  progress: number;
}

/**
 * Interface pour les callbacks de progression
 */
export interface ProgressCallback {
  (progress: { phase: string; progress: number; text: string }): void;
}

/**
 * Interface pour l'injection de d√©pendances (permet les mocks)
 */
export interface IInferenceEngine {
  load(modelId: string, onProgress?: ProgressCallback): Promise<void>;
  generate(prompt: string, options?: InferenceOptions): Promise<InferenceResult>;
  generateStream(
    prompt: string,
    onChunk: (chunk: string) => void,
    options?: InferenceOptions
  ): Promise<InferenceResult>;
  unload(): Promise<void>;
  isLoaded(): boolean;
  getModelId(): string | null;
}

/**
 * Configuration pour le retry avec backoff
 */
export interface RetryConfig {
  maxRetries: number;
  initialDelayMs: number;
  maxDelayMs: number;
  backoffMultiplier: number;
  retryableErrors: string[];
}

/**
 * M√©triques de performance
 */
export interface PerformanceMetrics {
  totalInferences: number;
  successfulInferences: number;
  failedInferences: number;
  totalRetries: number;
  fallbacksTriggered: number;
  averageLatencyMs: number;
  averageTokensPerSecond: number;
  p50LatencyMs: number;
  p95LatencyMs: number;
  p99LatencyMs: number;
  lastHourInferences: number;
  errorRate: number;
  gpuUtilization: number;
}

/**
 * Entr√©e du pool de moteurs
 */
interface EnginePoolEntry {
  engine: IInferenceEngine;
  modelId: string;
  backend: RuntimeBackend;
  lastUsed: number;
  useCount: number;
  isActive: boolean;
}

/**
 * Enregistrement d'une inf√©rence pour les m√©triques
 */
interface InferenceRecord {
  timestamp: number;
  latencyMs: number;
  tokensGenerated: number;
  success: boolean;
  retries: number;
  usedFallback: boolean;
}

/**
 * √âv√©nement de progression de compilation
 * Utilis√© pour informer l'UI de l'avancement de la pr√©-compilation
 */
export interface CompilationProgress {
  modelKey: string;
  stage: 'checking' | 'loading' | 'parsing' | 'linking' | 'optimizing' | 'compiling' | 'caching' | 'done';
  progress: number; // 0.0 - 1.0
  estimatedRemainingMs: number;
  message: string;
}

/**
 * Callback pour la progression de compilation
 */
export type CompilationProgressCallback = (progress: CompilationProgress) => void;

/**
 * Stats du cache de graphes compil√©s
 */
export interface CompiledGraphStats {
  inMemory: number;
  inStorage: number;
  cacheHits: number;
  cacheMisses: number;
  lastCompilationMs: number | null;
}

const DEFAULT_RETRY_CONFIG: RetryConfig = {
  maxRetries: 3,
  initialDelayMs: 500,
  maxDelayMs: 10000,
  backoffMultiplier: 2,
  retryableErrors: ['TIMEOUT', 'NETWORK_ERROR', 'OUT_OF_MEMORY', 'GPU_ERROR'],
};

/**
 * RuntimeManager - Gestionnaire principal des runtimes d'inf√©rence
 */
class RuntimeManager {
  private engine: IInferenceEngine | null = null;
  private currentBackend: RuntimeBackend | null = null;
  private gpuAvailable: boolean | null = null;
  private gpuInfo: GPUInfo | null = null;

  // Pool de moteurs pour multi-mod√®les
  private enginePool: Map<string, EnginePoolEntry> = new Map();
  private readonly MAX_POOL_SIZE = 3;

  // Configuration retry
  private retryConfig: RetryConfig = DEFAULT_RETRY_CONFIG;

  // M√©triques de performance
  private inferenceRecords: InferenceRecord[] = [];
  private readonly MAX_RECORDS = 1000;
  private metricsCache: PerformanceMetrics | null = null;
  private metricsCacheTime: number = 0;
  private readonly METRICS_CACHE_TTL = 5000; // 5 secondes

  // ============================================================================
  // PR√â-COMPILATION: Cache des graphes compil√©s pour d√©marrage instantan√©
  // ============================================================================

  /**
   * Cache m√©moire des graphes compil√©s (LRU)
   * Premier niveau de cache pour acc√®s instantan√©
   */
  private loadedCompiledGraphs: Map<string, CompiledGraph> = new Map();
  private readonly MAX_CACHED_GRAPHS = 3;

  /**
   * Stats du cache de graphes
   */
  private graphStats: {
    cacheHits: number;
    cacheMisses: number;
    lastCompilationMs: number | null;
  } = {
      cacheHits: 0,
      cacheMisses: 0,
      lastCompilationMs: null,
    };

  /**
   * Flag pour √©viter le warming concurrent
   */
  private isWarmingUp: boolean = false;
  private warmupPromise: Promise<void> | null = null;

  private status: RuntimeStatus = {
    isReady: false,
    backend: null,
    modelId: null,
    memoryUsage: 0,
    lastInferenceTime: null,
    totalInferences: 0,
    gpuAvailable: false,
    gpuInfo: null,
  };

  // --- Logique du Circuit Breaker ---
  private failureCount = 0;
  private successCount = 0; // ‚úÖ Nouveau
  private rejectionCount = 0; // ‚úÖ Nouveau
  private readonly FAILURE_THRESHOLD = 3;
  private readonly SUCCESS_THRESHOLD = 2; // ‚úÖ Succ√®s n√©cessaires pour fermer
  private readonly REJECTION_THRESHOLD = 5; // ‚úÖ Seuil de rejets
  private readonly FALLBACK_DURATION = 60_000;
  private readonly HALF_OPEN_TIMEOUT = 5_000; // ‚úÖ Timeout pour un test
  private fallbackUntil: number = 0;
  private gpuEngine: MockGPUEngine;
  private cpuEngine: MockCPUEngine;
  private circuitState: 'CLOSED' | 'OPEN' | 'HALF_OPEN' = 'CLOSED'; // ‚úÖ Nouvel √©tat

  // Ajout des propri√©t√©s pour le pr√©chauffage
  private prewarmingModels: Map<string, AbortController> = new Map();
  private prewarmMetrics = {
    totalPrewarms: 0,
    successfulPrewarms: 0,
    cancelledPrewarms: 0,
    hitRate: 0 // % des prewarms qui ont √©t√© utilis√©s
  };

  constructor() {
    log.info('RuntimeManager cr√©√©');
    // D√©tecter WebGPU au d√©marrage
    this.detectWebGPU().then((available) => {
      this.status.gpuAvailable = available;
      this.status.gpuInfo = this.gpuInfo;
      log.info(`WebGPU disponible: ${available}`);
    });

    // Lancer le nettoyage des graphes obsol√®tes en arri√®re-plan
    this.initializeGraphCache();

    this.gpuEngine = new MockGPUEngine();
    this.cpuEngine = new MockCPUEngine();
    logger.info('RuntimeManager', 'üöÄ RuntimeManager (Production) initialis√©.'); // ‚úÖ Nouveau
  }

  /**
   * Initialise le cache de graphes (nettoyage au boot)
   */
  private async initializeGraphCache(): Promise<void> {
    try {
      const cleanup = await storageManager.cleanupObsoleteGraphs(30);
      if (cleanup.deleted > 0) {
        log.info(`[Pre-Compile] Nettoyage initial: ${cleanup.deleted} graphes obsol√®tes supprim√©s`);
      }
    } catch {
      // Ignorer les erreurs de nettoyage
    }
  }

  /**
   * D√©tecte la disponibilit√© de WebGPU et r√©cup√®re les informations GPU
   */
  public async detectWebGPU(): Promise<boolean> {
    if (this.gpuAvailable !== null) {
      return this.gpuAvailable;
    }

    try {
      if (typeof navigator === 'undefined' || !navigator.gpu) {
        log.info('WebGPU API non disponible dans cet environnement');
        this.gpuAvailable = false;
        return false;
      }

      const adapter = await navigator.gpu.requestAdapter();
      if (!adapter) {
        log.warn('WebGPU: Aucun adaptateur GPU trouv√©');
        this.gpuAvailable = false;
        return false;
      }

      // R√©cup√©rer les informations sur le GPU
      const adapterInfo = await adapter.requestAdapterInfo();
      this.gpuInfo = {
        vendor: adapterInfo.vendor || 'Unknown',
        architecture: adapterInfo.architecture || 'Unknown',
        device: adapterInfo.device || 'Unknown',
        description: adapterInfo.description || 'Unknown GPU',
      };

      log.info('WebGPU d√©tect√©:', this.gpuInfo);
      this.gpuAvailable = true;
      return true;
    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error));
      log.warn('Erreur lors de la d√©tection WebGPU:', err);
      this.gpuAvailable = false;
      return false;
    }
  }

  /**
   * S√©lectionne automatiquement le meilleur backend disponible
   */
  public async autoSelectBackend(): Promise<RuntimeBackend> {
    const hasGPU = await this.detectWebGPU();

    if (hasGPU) {
      log.info('‚úÖ WebGPU disponible - S√©lection du backend GPU (webllm)');
      return 'webllm';
    } else {
      log.info('‚ö†Ô∏è WebGPU non disponible - S√©lection du backend CPU (transformers)');
      return 'transformers';
    }
  }

  /**
   * Configure les param√®tres de retry
   */
  public setRetryConfig(config: Partial<RetryConfig>): void {
    this.retryConfig = { ...this.retryConfig, ...config };
    log.info('Configuration retry mise √† jour:', this.retryConfig);
  }

  /**
   * Calcule le d√©lai de retry avec backoff exponentiel
   */
  private calculateRetryDelay(attempt: number): number {
    const delay = Math.min(
      this.retryConfig.initialDelayMs * Math.pow(this.retryConfig.backoffMultiplier, attempt),
      this.retryConfig.maxDelayMs
    );
    // Ajouter un jitter de ¬±20%
    const jitter = delay * 0.2 * (Math.random() * 2 - 1);
    return Math.round(delay + jitter);
  }

  /**
   * V√©rifie si une erreur est retryable
   */
  private isRetryableError(error: Error): boolean {
    const errorMessage = error.message.toUpperCase();
    return this.retryConfig.retryableErrors.some((e) => errorMessage.includes(e));
  }

  /**
   * Ex√©cute une op√©ration avec retry et backoff
   */
  private async withRetry<T>(
    operation: () => Promise<T>,
    operationName: string,
    enableFallback: boolean = true
  ): Promise<{ result: T; retries: number; usedFallback: boolean }> {
    let lastError: Error | null = null;
    let retries = 0;
    let usedFallback = false;

    for (let attempt = 0; attempt <= this.retryConfig.maxRetries; attempt++) {
      try {
        const result = await operation();
        return { result, retries, usedFallback };
      } catch (error) {
        lastError = error instanceof Error ? error : new Error(String(error));
        retries = attempt;

        if (attempt < this.retryConfig.maxRetries && this.isRetryableError(lastError)) {
          const delay = this.calculateRetryDelay(attempt);
          log.warn(
            `${operationName} √©chou√© (tentative ${attempt + 1}/${this.retryConfig.maxRetries + 1}), ` +
            `retry dans ${delay}ms: ${lastError.message}`
          );
          await new Promise((resolve) => setTimeout(resolve, delay));
        } else if (
          enableFallback &&
          this.currentBackend === 'webllm' &&
          lastError.message.includes('GPU')
        ) {
          // Fallback GPU ‚Üí CPU
          log.warn('Erreur GPU d√©tect√©e, fallback vers CPU...');
          try {
            await this.switchBackend('transformers');
            usedFallback = true;
            const result = await operation();
            return { result, retries, usedFallback };
          } catch (fallbackError) {
            log.error('Fallback CPU √©galement √©chou√©:', fallbackError as Error);
          }
        }
      }
    }

    throw lastError || new Error(`${operationName} √©chou√© apr√®s ${retries + 1} tentatives`);
  }

  /**
   * Initialise le runtime avec un backend sp√©cifique
   */
  public async initialize(
    config: RuntimeConfig,
    onProgress?: ProgressCallback
  ): Promise<boolean> {
    try {
      let backend = config.backend;

      // Si 'auto', s√©lectionner le meilleur backend
      if (backend === 'auto') {
        backend = await this.autoSelectBackend();
        log.info(`Backend auto-s√©lectionn√©: ${backend}`);
      }

      log.info(`Initialisation du runtime: ${backend} / ${config.modelId}`);

      // V√©rifier les ressources disponibles
      const deviceStatus = await resourceManager.getStatus();
      if (deviceStatus.memory.usageRatio > 0.85) {
        log.warn('M√©moire insuffisante pour charger un mod√®le');
        return false;
      }

      // V√©rifier si le mod√®le peut √™tre charg√© (sauf pour mock)
      if (backend !== 'mock' && !memoryManager.canLoadModel(config.modelId)) {
        log.warn(`VRAM insuffisante pour le mod√®le: ${config.modelId}`);
        // En mode auto ou webllm, on peut fallback sur CPU
        if (backend === 'webllm') {
          log.info('Fallback vers le backend CPU (transformers)');
          backend = 'transformers';
        }
      }

      // V√©rifier si le mod√®le est d√©j√† dans le pool
      const poolKey = `${backend}:${config.modelId}`;
      const pooledEngine = this.enginePool.get(poolKey);

      if (pooledEngine && pooledEngine.engine.isLoaded()) {
        log.info(`Mod√®le ${config.modelId} r√©cup√©r√© depuis le pool`);
        this.engine = pooledEngine.engine;
        this.currentBackend = backend;
        pooledEngine.lastUsed = Date.now();
        pooledEngine.useCount++;
        pooledEngine.isActive = true;

        this.updateStatus(backend, config.modelId);
        return true;
      }

      // D√©charger l'ancien moteur si n√©cessaire
      if (this.engine?.isLoaded()) {
        await this.releaseToPool();
      }

      // Cr√©er le moteur appropri√©
      this.engine = await this.createEngine(backend);
      this.currentBackend = backend;

      // Charger le mod√®le avec retry
      await this.withRetry(
        () => this.engine!.load(config.modelId, onProgress),
        `Chargement du mod√®le ${config.modelId}`,
        true
      );

      // Ajouter au pool
      this.addToPool(poolKey, this.engine, config.modelId, backend);

      this.updateStatus(backend, config.modelId);

      // V√©rifier le cache OPFS
      const storageReady = await storageManager.ensureReady();
      if (storageReady) {
        log.info('Cache OPFS disponible pour les mod√®les');
      }

      log.info(`Runtime initialis√© avec succ√®s: ${backend}/${config.modelId}`);
      return true;
    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error));
      log.error("Erreur d'initialisation du runtime:", err);
      this.status.isReady = false;
      return false;
    }
  }

  /**
   * Met √† jour le statut apr√®s initialisation
   */
  private updateStatus(backend: RuntimeBackend, modelId: string): void {
    this.status = {
      isReady: true,
      backend: backend,
      modelId: modelId,
      memoryUsage: 0,
      lastInferenceTime: null,
      totalInferences: this.status.totalInferences,
      gpuAvailable: this.gpuAvailable ?? false,
      gpuInfo: this.gpuInfo,
    };
  }

  /**
   * Ajoute un moteur au pool
   */
  private addToPool(
    key: string,
    engine: IInferenceEngine,
    modelId: string,
    backend: RuntimeBackend
  ): void {
    // √âviction LRU si n√©cessaire
    if (this.enginePool.size >= this.MAX_POOL_SIZE) {
      let oldestKey: string | null = null;
      let oldestTime = Infinity;

      for (const [k, entry] of this.enginePool.entries()) {
        if (!entry.isActive && entry.lastUsed < oldestTime) {
          oldestTime = entry.lastUsed;
          oldestKey = k;
        }
      }

      if (oldestKey) {
        const evicted = this.enginePool.get(oldestKey);
        if (evicted) {
          evicted.engine.unload().catch((e) => log.warn('Erreur lors de l\'√©viction:', e));
          this.enginePool.delete(oldestKey);
          log.info(`Pool: √©viction de ${oldestKey}`);
        }
      }
    }

    this.enginePool.set(key, {
      engine,
      modelId,
      backend,
      lastUsed: Date.now(),
      useCount: 1,
      isActive: true,
    });
  }

  /**
   * Lib√®re le moteur actuel vers le pool (sans le d√©charger)
   */
  private async releaseToPool(): Promise<void> {
    if (!this.engine || !this.currentBackend || !this.status.modelId) return;

    const poolKey = `${this.currentBackend}:${this.status.modelId}`;
    const entry = this.enginePool.get(poolKey);

    if (entry) {
      entry.isActive = false;
      entry.lastUsed = Date.now();
    }

    this.engine = null;
  }

  /**
   * Initialise le runtime avec s√©lection automatique du backend
   */
  public async initializeAuto(
    modelId: string,
    onProgress?: ProgressCallback
  ): Promise<boolean> {
    return this.initialize(
      {
        backend: 'auto',
        modelId,
      },
      onProgress
    );
  }

  /**
   * Cr√©e une instance du moteur d'inf√©rence appropri√©
   */
  private async createEngine(backend: RuntimeBackend): Promise<IInferenceEngine> {
    switch (backend) {
      case 'mock':
        log.info('Utilisation du MockInferenceEngine (g√©n√©rique)');
        return createMockEngine(this.gpuAvailable ? 'GPU' : 'CPU');

      case 'webllm':
        log.info("WebLLM backend demand√© - utilisation du mock GPU pour l'instant");
        // TODO: Impl√©menter WebLLMEngine quand pr√™t
        // const { WebLLMEngine } = await import('../runtime/webllm/WebLLMEngine');
        // return new WebLLMEngine();
        return new MockWebLLMEngine();

      case 'transformers':
        log.info("Transformers.js backend demand√© - utilisation du mock CPU pour l'instant");
        // TODO: Impl√©menter TransformersEngine quand pr√™t
        // const { TransformersEngine } = await import('../runtime/transformers/TransformersEngine');
        // return new TransformersEngine();
        return new MockTransformersJSEngine();

      case 'auto': {
        const selectedBackend = await this.autoSelectBackend();
        return this.createEngine(selectedBackend);
      }

      default:
        throw new Error(`Backend non support√©: ${backend}`);
    }
  }

  /**
   * Enregistre une inf√©rence dans les m√©triques
   */
  private recordInference(
    latencyMs: number,
    tokensGenerated: number,
    success: boolean,
    retries: number,
    usedFallback: boolean
  ): void {
    const record: InferenceRecord = {
      timestamp: Date.now(),
      latencyMs,
      tokensGenerated,
      success,
      retries,
      usedFallback,
    };

    this.inferenceRecords.push(record);

    // Limiter la taille
    if (this.inferenceRecords.length > this.MAX_RECORDS) {
      this.inferenceRecords = this.inferenceRecords.slice(-this.MAX_RECORDS);
    }

    // Invalider le cache des m√©triques
    this.metricsCache = null;
  }

  /**
   * G√©n√®re une r√©ponse (mode non-streaming) avec retry
   */
  public async generate(
    prompt: string,
    options?: InferenceOptions
  ): Promise<InferenceResult> {
    if (!this.engine || !this.status.isReady) {
      throw new Error('Runtime non initialis√©');
    }

    const startTime = performance.now();
    let retries = 0;
    let usedFallback = false;

    try {
      const { result, retries: r, usedFallback: f } = await this.withRetry(
        () => this.engine!.generate(prompt, options),
        'Inf√©rence',
        true
      );
      retries = r;
      usedFallback = f;

      const latencyMs = performance.now() - startTime;
      this.status.lastInferenceTime = latencyMs;
      this.status.totalInferences++;

      // Enregistrer les m√©triques
      this.recordInference(latencyMs, result.tokensGenerated, true, retries, usedFallback);

      log.debug(
        `Inf√©rence compl√®te: ${result.tokensGenerated} tokens en ${result.timeMs.toFixed(0)}ms` +
        (retries > 0 ? ` (${retries} retries)` : '') +
        (usedFallback ? ' (fallback CPU)' : '')
      );

      return result;
    } catch (error) {
      const latencyMs = performance.now() - startTime;
      this.recordInference(latencyMs, 0, false, retries, usedFallback);

      const err = error instanceof Error ? error : new Error(String(error));
      log.error("Erreur d'inf√©rence:", err);
      throw err;
    }
  }

  /**
   * G√©n√®re une r√©ponse en streaming avec retry
   */
  public async generateStream(
    prompt: string,
    onChunk: (chunk: string) => void,
    options?: InferenceOptions
  ): Promise<InferenceResult> {
    if (!this.engine || !this.status.isReady) {
      throw new Error('Runtime non initialis√©');
    }

    const startTime = performance.now();
    let retries = 0;
    let usedFallback = false;

    try {
      const { result, retries: r, usedFallback: f } = await this.withRetry(
        () => this.engine!.generateStream(prompt, onChunk, options),
        'Streaming',
        true
      );
      retries = r;
      usedFallback = f;

      const latencyMs = performance.now() - startTime;
      this.status.lastInferenceTime = latencyMs;
      this.status.totalInferences++;

      // Enregistrer les m√©triques
      this.recordInference(latencyMs, result.tokensGenerated, true, retries, usedFallback);

      log.debug(
        `Streaming compl√©t√©: ${result.tokensGenerated} tokens en ${result.timeMs.toFixed(0)}ms` +
        (retries > 0 ? ` (${retries} retries)` : '') +
        (usedFallback ? ' (fallback CPU)' : '')
      );

      return result;
    } catch (error) {
      const latencyMs = performance.now() - startTime;
      this.recordInference(latencyMs, 0, false, retries, usedFallback);

      const err = error instanceof Error ? error : new Error(String(error));
      log.error('Erreur de streaming:', err);
      throw err;
    }
  }

  /**
   * Calcule les percentiles pour les m√©triques de latence
   */
  private calculatePercentile(values: number[], percentile: number): number {
    if (values.length === 0) return 0;
    const sorted = [...values].sort((a, b) => a - b);
    const index = Math.ceil((percentile / 100) * sorted.length) - 1;
    return sorted[Math.max(0, index)];
  }

  /**
   * Obtient les m√©triques de performance
   */
  public getPerformanceMetrics(): PerformanceMetrics {
    // Utiliser le cache si valide
    if (this.metricsCache && Date.now() - this.metricsCacheTime < this.METRICS_CACHE_TTL) {
      return this.metricsCache;
    }

    const now = Date.now();
    const oneHourAgo = now - 3600000;

    const records = this.inferenceRecords;
    const recentRecords = records.filter((r) => r.timestamp > oneHourAgo);

    const successfulRecords = records.filter((r) => r.success);
    const latencies = successfulRecords.map((r) => r.latencyMs);

    const totalTokens = successfulRecords.reduce((sum, r) => sum + r.tokensGenerated, 0);
    const totalTime = successfulRecords.reduce((sum, r) => sum + r.latencyMs, 0);
    const avgTokensPerSecond = totalTime > 0 ? (totalTokens / totalTime) * 1000 : 0;

    const totalRetries = records.reduce((sum, r) => sum + r.retries, 0);
    const fallbacksTriggered = records.filter((r) => r.usedFallback).length;

    const metrics: PerformanceMetrics = {
      totalInferences: records.length,
      successfulInferences: successfulRecords.length,
      failedInferences: records.filter((r) => !r.success).length,
      totalRetries,
      fallbacksTriggered,
      averageLatencyMs:
        latencies.length > 0 ? latencies.reduce((a, b) => a + b, 0) / latencies.length : 0,
      averageTokensPerSecond: Math.round(avgTokensPerSecond * 100) / 100,
      p50LatencyMs: this.calculatePercentile(latencies, 50),
      p95LatencyMs: this.calculatePercentile(latencies, 95),
      p99LatencyMs: this.calculatePercentile(latencies, 99),
      lastHourInferences: recentRecords.length,
      errorRate: records.length > 0 ? records.filter((r) => !r.success).length / records.length : 0,
      gpuUtilization: this.currentBackend === 'webllm' ? 0.7 : 0, // Placeholder
    };

    // Mettre en cache
    this.metricsCache = metrics;
    this.metricsCacheTime = now;

    return metrics;
  }

  /**
   * R√©initialise les m√©triques
   */
  public resetMetrics(): void {
    this.inferenceRecords = [];
    this.metricsCache = null;
    log.info('M√©triques r√©initialis√©es');
  }

  /**
   * Change de mod√®le sans changer de backend
   */
  public async switchModel(modelId: string, onProgress?: ProgressCallback): Promise<boolean> {
    if (!this.currentBackend) {
      log.error('Aucun backend actif');
      return false;
    }

    return this.initialize(
      {
        backend: this.currentBackend,
        modelId,
      },
      onProgress
    );
  }

  /**
   * Change de backend
   */
  public async switchBackend(
    backend: RuntimeBackend,
    modelId?: string,
    onProgress?: ProgressCallback
  ): Promise<boolean> {
    const model = modelId ?? this.status.modelId;
    if (!model) {
      log.error('Aucun mod√®le sp√©cifi√©');
      return false;
    }

    return this.initialize(
      {
        backend,
        modelId: model,
      },
      onProgress
    );
  }

  /**
   * Obtient les informations sur le pool de moteurs
   */
  public getPoolInfo(): {
    size: number;
    maxSize: number;
    engines: Array<{
      key: string;
      modelId: string;
      backend: RuntimeBackend;
      isActive: boolean;
      useCount: number;
      lastUsed: number;
    }>;
  } {
    const engines = Array.from(this.enginePool.entries()).map(([key, entry]) => ({
      key,
      modelId: entry.modelId,
      backend: entry.backend,
      isActive: entry.isActive,
      useCount: entry.useCount,
      lastUsed: entry.lastUsed,
    }));

    return {
      size: this.enginePool.size,
      maxSize: this.MAX_POOL_SIZE,
      engines,
    };
  }

  /**
   * Pr√©charge un mod√®le dans le pool
   */
  public async preloadModel(
    modelId: string,
    backend: RuntimeBackend = 'auto',
    onProgress?: ProgressCallback
  ): Promise<boolean> {
    const resolvedBackend = backend === 'auto' ? await this.autoSelectBackend() : backend;
    const poolKey = `${resolvedBackend}:${modelId}`;

    if (this.enginePool.has(poolKey)) {
      log.info(`Mod√®le ${modelId} d√©j√† dans le pool`);
      return true;
    }

    try {
      const engine = await this.createEngine(resolvedBackend);
      await engine.load(modelId, onProgress);
      this.addToPool(poolKey, engine, modelId, resolvedBackend);
      log.info(`Mod√®le ${modelId} pr√©charg√© dans le pool`);
      return true;
    } catch (error) {
      log.error(`Erreur de pr√©chargement du mod√®le ${modelId}:`, error as Error);
      return false;
    }
  }

  // ============================================================================
  // M√âTHODES DE PR√â-COMPILATION (D√©marrage instantan√©)
  // ============================================================================

  /**
   * Charge un mod√®le avec le syst√®me de pr√©-compilation 3-niveaux:
   * 1. Cache m√©moire (instantan√©)
   * 2. Cache OPFS (< 200ms)
   * 3. Compilation compl√®te (~ 4 secondes simul√©es)
   */
  public async loadCompiledModel(
    modelKey: string,
    onProgress?: CompilationProgressCallback,
    signal?: AbortSignal
  ): Promise<CompiledGraph> {
    const emitProgress = (
      stage: CompilationProgress['stage'],
      progress: number,
      estimatedRemainingMs: number,
      message: string
    ) => {
      onProgress?.({
        modelKey,
        stage,
        progress,
        estimatedRemainingMs,
        message,
      });
    };

    emitProgress('checking', 0, 0, 'V√©rification du cache m√©moire...');

    // ============ NIVEAU 1: Cache m√©moire (instantan√©) ============
    const cachedInMemory = this.loadedCompiledGraphs.get(modelKey);
    if (cachedInMemory) {
      this.graphStats.cacheHits++;
      log.info(`[Pre-Compile] ‚úÖ ${modelKey} trouv√© en m√©moire (instantan√©)`);
      emitProgress('done', 1, 0, 'Graphe en m√©moire, d√©marrage instantan√©');
      return cachedInMemory;
    }

    // ============ NIVEAU 2: Cache OPFS (< 200ms) ============
    emitProgress('loading', 0.1, 200, 'Recherche du graphe pr√©-compil√© dans le stockage...');

    const cachedInStorage = await storageManager.getCompiledGraph(modelKey);
    if (cachedInStorage) {
      this.graphStats.cacheHits++;
      log.info(`[Pre-Compile] üì¶ ${modelKey} trouv√© dans OPFS, chargement rapide...`);

      // Simuler le temps de chargement depuis OPFS vers la m√©moire (< 200ms)
      emitProgress('loading', 0.5, 100, 'Transfert du graphe vers la VRAM...');
      await this.simulateDelay(150, 50); // 150ms ¬± 50ms

      // Ajouter au cache m√©moire
      this.evictOldestGraphIfNeeded();
      this.loadedCompiledGraphs.set(modelKey, cachedInStorage);

      emitProgress('done', 1, 0, 'Graphe charg√© depuis le stockage (d√©marrage √† froid √©vit√©)');
      log.info(`[Pre-Compile] ‚úÖ ${modelKey} pr√™t (warm start depuis OPFS)`);
      return cachedInStorage;
    }

    // ============ NIVEAU 3: Compilation compl√®te (~ 4 secondes) ============
    this.graphStats.cacheMisses++;
    log.warn(`[Pre-Compile] ‚ö†Ô∏è Aucun graphe pour ${modelKey}, compilation n√©cessaire...`);

    const compilationStart = performance.now();

    // Simulation des diff√©rentes phases de compilation
    const phases = [
      { stage: 'parsing' as const, duration: 800, msg: 'Parsing du mod√®le...' },
      { stage: 'linking' as const, duration: 600, msg: 'Linking des d√©pendances...' },
      { stage: 'optimizing' as const, duration: 1000, msg: 'Optimisation du graphe...' },
      { stage: 'compiling' as const, duration: 1400, msg: 'Compilation des shaders WebGPU...' },
      { stage: 'caching' as const, duration: 200, msg: 'Mise en cache du graphe compil√©...' },
    ];

    const totalEstimated = phases.reduce((sum, p) => sum + p.duration, 0);
    let elapsed = 0;

    for (const phase of phases) {
      emitProgress(phase.stage, elapsed / totalEstimated, totalEstimated - elapsed, phase.msg);

      // Ajouter un jitter r√©aliste (¬±15%)
      await this.simulateDelay(phase.duration, phase.duration * 0.15);
      elapsed += phase.duration;

      // V√©rifier l'annulation
      if (signal?.aborted) {
        throw new Error('Pr√©chauffage annul√©');
      }
    }

    const compilationTimeMs = performance.now() - compilationStart;
    this.graphStats.lastCompilationMs = compilationTimeMs;

    // Cr√©er le nouveau graphe compil√©
    const newGraph: CompiledGraph = {
      id: modelKey,
      version: COMPILED_GRAPH_VERSION,
      schemaHash: this.generateSchemaHash(modelKey),
      compiledAt: Date.now(),
      compilationTimeMs: Math.round(compilationTimeMs),
      metadata: {
        modelName: modelKey,
        backend: this.currentBackend || 'mock',
        optimizationLevel: 'balanced',
      },
    };

    // Sauvegarder dans OPFS pour les prochains lancements
    await storageManager.saveCompiledGraph(modelKey, newGraph);

    // Ajouter au cache m√©moire
    this.evictOldestGraphIfNeeded();
    this.loadedCompiledGraphs.set(modelKey, newGraph);

    emitProgress('done', 1, 0, `Compilation termin√©e (${Math.round(compilationTimeMs)}ms)`);
    log.info(`[Pre-Compile] ‚úÖ ${modelKey} compil√© et pr√™t (${Math.round(compilationTimeMs)}ms)`);

    return newGraph;
  }

  /**
   * Pr√©-compile une liste de mod√®les en arri√®re-plan (warming)
   * Non-bloquant, id√©al pour ex√©cuter pendant l'onboarding utilisateur
   */
  public async warmupModels(
    modelKeys: string[],
    onProgress?: (current: string, index: number, total: number) => void
  ): Promise<{ success: string[]; failed: string[] }> {
    // √âviter le warming concurrent
    if (this.isWarmingUp && this.warmupPromise) {
      log.info('[Warmup] Warming d√©j√† en cours, attente...');
      await this.warmupPromise;
    }

    this.isWarmingUp = true;
    const success: string[] = [];
    const failed: string[] = [];

    log.info(`[Warmup] D√©marrage du pr√©-chauffage de ${modelKeys.length} mod√®le(s)...`);

    this.warmupPromise = (async () => {
      for (let i = 0; i < modelKeys.length; i++) {
        const modelKey = modelKeys[i];
        onProgress?.(modelKey, i, modelKeys.length);

        try {
          // Utiliser requestIdleCallback si disponible pour ne pas bloquer l'UI
          if (typeof requestIdleCallback !== 'undefined') {
            await new Promise<void>((resolve) => {
              requestIdleCallback(() => resolve(), { timeout: 100 });
            });
          }

          // Charger silencieusement (sans callback de progression visible)
          await this.loadCompiledModel(modelKey);
          success.push(modelKey);
          log.debug(`[Warmup] ${modelKey} pr√™t (${i + 1}/${modelKeys.length})`);
        } catch (error) {
          const err = error instanceof Error ? error : new Error(String(error));
          log.warn(`[Warmup] √âchec pour ${modelKey}: ${err.message}`);
          failed.push(modelKey);
        }
      }
    })();

    await this.warmupPromise;
    this.isWarmingUp = false;
    this.warmupPromise = null;

    log.info(`[Warmup] Termin√©: ${success.length} succ√®s, ${failed.length} √©checs`);
    return { success, failed };
  }

  /**
   * Pr√©chauffe un mod√®le en arri√®re-plan.
   * C'est une op√©ration non bloquante.
   */
  public prewarmModel(modelKey: string): void {
    this.prewarmMetrics.totalPrewarms++;
    
    // Annule les autres pr√©chauffages en cours
    for (const [key, controller] of this.prewarmingModels.entries()) {
      if (key !== modelKey) {
        logger.info('RuntimeManager', `Annulation du pr√©chauffage de ${key}`);
        controller.abort();
        this.prewarmingModels.delete(key);
        this.prewarmMetrics.cancelledPrewarms++;
      }
    }

    // Si d√©j√† en cours pour ce mod√®le, ne rien faire
    if (this.prewarmingModels.has(modelKey)) {
      logger.debug('RuntimeManager', `${modelKey} d√©j√† en pr√©chauffage`);
      return;
    }

    // Si d√©j√† charg√©, ne rien faire
    if (this.loadedCompiledGraphs.has(modelKey)) {
      logger.debug('RuntimeManager', `${modelKey} d√©j√† charg√©`);
      this.prewarmMetrics.successfulPrewarms++;
      return;
    }

    // Lance le pr√©chauffage avec AbortController
    const controller = new AbortController();
    this.prewarmingModels.set(modelKey, controller);

    logger.info('RuntimeManager', `üî• Pr√©chauffage de ${modelKey}...`);

    this.loadCompiledModel(modelKey, undefined, controller.signal)
      .then(() => {
        logger.info('RuntimeManager', `‚úÖ ${modelKey} pr√©chauff√© et pr√™t`);
        this.prewarmingModels.delete(modelKey);
        this.prewarmMetrics.successfulPrewarms++;
      })
      .catch(err => {
        if (err.name === 'AbortError') {
          logger.debug('RuntimeManager', `Pr√©chauffage de ${modelKey} annul√©`);
          this.prewarmMetrics.cancelledPrewarms++;
        } else {
          logger.error('RuntimeManager', `√âchec du pr√©chauffage`, err);
        }
        this.prewarmingModels.delete(modelKey);
      });
  }

  public getMetrics() {
    return {
      ...this.prewarmMetrics,
      hitRate: (this.prewarmMetrics.successfulPrewarms / this.prewarmMetrics.totalPrewarms * 100).toFixed(1) + '%'
    };
  }

  /**
   * √âviction LRU du cache m√©moire si n√©cessaire
   */
  private evictOldestGraphIfNeeded(): void {
    if (this.loadedCompiledGraphs.size >= this.MAX_CACHED_GRAPHS) {
      // L'ordre d'insertion de Map pr√©serve l'ordre, donc le premier est le plus ancien
      const oldestKey = this.loadedCompiledGraphs.keys().next().value;
      if (oldestKey) {
        this.loadedCompiledGraphs.delete(oldestKey);
        log.debug(`[Pre-Compile] √âviction LRU: ${oldestKey}`);
      }
    }
  }

  /**
   * Retourne les statistiques du cache de graphes compil√©s
   */
  public async getCompiledGraphStats(): Promise<CompiledGraphStats> {
    const storedGraphs = await storageManager.listCompiledGraphs();

    return {
      inMemory: this.loadedCompiledGraphs.size,
      inStorage: storedGraphs.length,
      cacheHits: this.graphStats.cacheHits,
      cacheMisses: this.graphStats.cacheMisses,
      lastCompilationMs: this.graphStats.lastCompilationMs,
    };
  }

  /**
   * Async generator for pipelined token generation (Task #16)
   * This method simulates CPU/GPU pipelining where the CPU prepares the next token
   * while the GPU computes the current one, reducing idle time.
   */
  public async *generate(prompt: string, modelKey: string): AsyncGenerator<string> {
    log.info(`[RuntimeManager] D√©but de la g√©n√©ration pour ${modelKey} avec pipelining...`);
    
    // Get the engine for this model
    const engine = await this.getEngineFor(modelKey);
    
    // If the engine has the new generate method, use it
    if (typeof (engine as any).generate === 'function' && (engine as any).generate.constructor.name === 'AsyncGeneratorFunction') {
      // Use the engine's built-in pipelining
      for await (const chunk of (engine as any).generate(prompt, modelKey)) {
        yield chunk;
      }
    } else {
      // Fallback to traditional streaming
      let fullResponse = '';
      await new Promise<void>((resolve) => {
        engine.generateStream(
          prompt,
          (chunk: string) => {
            fullResponse += chunk;
          }
        ).then(() => resolve());
      });
      
      // Split response into tokens and yield them
      const tokens = fullResponse.split(' ');
      for (const token of tokens) {
        yield token + ' ';
      }
    }
    
    log.info(`[RuntimeManager] Fin de la g√©n√©ration pour ${modelKey}.`);
  }

  /**
   * R√©cup√®re un moteur d'inf√©rence, en s'assurant que le graphe est pr√©-compil√©
   */
  public async getEngineFor(
    modelKey: string,
    onProgress?: CompilationProgressCallback
  ): Promise<IInferenceEngine> {
    // S'assurer que le graphe est charg√©
    await this.loadCompiledModel(modelKey, onProgress);

    // Renvoyer le moteur actuel (la logique de g√©n√©ration est s√©par√©e)
    if (!this.engine) {
      throw new Error(`Aucun moteur disponible pour ${modelKey}`);
    }

    return this.engine;
  }

  /**
   * V√©rifie si un mod√®le a un graphe pr√©-compil√© disponible
   */
  public isGraphCached(modelKey: string): boolean {
    return this.loadedCompiledGraphs.has(modelKey);
  }

  /**
   * Vide le cache de graphes compil√©s (m√©moire + OPFS)
   */
  public async clearCompiledGraphs(): Promise<void> {
    this.loadedCompiledGraphs.clear();
    this.graphStats.cacheHits = 0;
    this.graphStats.cacheMisses = 0;
    this.graphStats.lastCompilationMs = null;

    // Vider le r√©pertoire graphs dans OPFS
    await storageManager.deleteDirectory('graphs', true);

    log.info('[Pre-Compile] Cache de graphes vid√©');
  }

  /**
   * Utilitaire: simule un d√©lai avec jitter
   */
  private async simulateDelay(baseMs: number, jitterMs: number): Promise<void> {
    const jitter = jitterMs * (Math.random() * 2 - 1);
    const delay = Math.max(0, baseMs + jitter);
    await new Promise((resolve) => setTimeout(resolve, delay));
  }

  /**
   * Utilitaire: g√©n√®re un hash de sch√©ma simple
   */
  private generateSchemaHash(modelKey: string): string {
    // Hash simple bas√© sur le nom et la version
    const str = `${modelKey}:${COMPILED_GRAPH_VERSION}:${Date.now()}`;
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash;
    }
    return Math.abs(hash).toString(16);
  }

  /**
   * Arr√™te le runtime et lib√®re les ressources
   */
  public async shutdown(): Promise<void> {
    // D√©charger tous les moteurs du pool
    for (const [key, entry] of this.enginePool.entries()) {
      try {
        await entry.engine.unload();
        log.debug(`Pool: d√©chargement de ${key}`);
      } catch (error) {
        log.warn(`Erreur lors du d√©chargement de ${key}:`, error as Error);
      }
    }
    this.enginePool.clear();

    this.engine = null;
    this.currentBackend = null;
    this.status = {
      isReady: false,
      backend: null,
      modelId: null,
      memoryUsage: 0,
      lastInferenceTime: null,
      totalInferences: this.status.totalInferences,
      gpuAvailable: this.gpuAvailable ?? false,
      gpuInfo: this.gpuInfo,
    };

    log.info('Runtime arr√™t√© proprement');
  }

  /**
   * Retourne le statut actuel du runtime
   */
  public getStatus(): RuntimeStatus {
    return { ...this.status };
  }

  /**
   * V√©rifie si le runtime est pr√™t
   */
  public isReady(): boolean {
    return this.status.isReady && this.engine?.isLoaded() === true;
  }

  /**
   * Retourne le backend actuel
   */
  public getCurrentBackend(): RuntimeBackend | null {
    return this.currentBackend;
  }

  /**
   * V√©rifie si WebGPU est disponible
   */
  public isGPUAvailable(): boolean {
    return this.gpuAvailable ?? false;
  }

  /**
   * Retourne les informations GPU
   */
  public getGPUInfo(): GPUInfo | null {
    return this.gpuInfo;
  }

  /**
   * Injecte un moteur personnalis√© (pour les tests)
   */
  public setEngine(engine: IInferenceEngine): void {
    this.engine = engine;
    log.info("Moteur d'inf√©rence inject√© manuellement");
  }

  /**
   * V√©rifie si un mod√®le est dans le pool
   */
  public isModelInPool(modelId: string, backend?: RuntimeBackend): boolean {
    if (backend) {
      return this.enginePool.has(`${backend}:${modelId}`);
    }
    for (const [key] of this.enginePool.entries()) {
      if (key.endsWith(`:${modelId}`)) {
        return true;
      }
    }
    return false;
  }

  /**
   * Vide le pool de moteurs
   */
  public async clearPool(): Promise<void> {
    for (const [key, entry] of this.enginePool.entries()) {
      if (!entry.isActive) {
        try {
          await entry.engine.unload();
          this.enginePool.delete(key);
          log.debug(`Pool: suppression de ${key}`);
        } catch (error) {
          log.warn(`Erreur lors de la suppression de ${key}:`, error as Error);
        }
      }
    }
    log.info('Pool vid√© (moteurs inactifs)');
  }

  /**
   * Obtient le moteur appropri√© selon l'√©tat du Circuit Breaker
   */
  public async getEngineFor(task: any): Promise<MockGPUEngine | MockCPUEngine> {
    // Met √† jour les m√©triques
    this.metrics.state = this.circuitState;
    this.metrics.fallbackUntil = this.fallbackUntil;

    switch (this.circuitState) {
      case 'CLOSED':
        // Op√©ration normale
        return this.gpuEngine;

      case 'OPEN':
        if (Date.now() < this.fallbackUntil) {
          logger.warn('RuntimeManager', 'Circuit OPEN. Fallback CPU.'); // ‚úÖ Nouveau
          return this.cpuEngine;
        }
        // Le temps est √©coul√©, passe en HALF_OPEN
        this.circuitState = 'HALF_OPEN';
        this.successCount = 0;
        logger.info('RuntimeManager', 'Circuit HALF_OPEN. Test du GPU...'); // ‚úÖ Nouveau
        sseStreamer.streamStatus("Test de stabilit√© du moteur principal...");
        // Continue vers HALF_OPEN ‚Üì

      case 'HALF_OPEN':
        // On teste le GPU avec un timeout strict
        return this.gpuEngine;
    }
  }

  /**
   * Ex√©cute une promesse avec un timeout
   */
  private async executeWithTimeout<T>(
    promise: Promise<T>,
    timeoutMs: number
  ): Promise<T> {
    return Promise.race([
      promise,
      new Promise<T>((_, reject) =>
        setTimeout(() => reject(new Error('Timeout')), timeoutMs)
      )
    ]);
  }

  /**
   * Notifie le Circuit Breaker d'un succ√®s.
   */
  public handleSuccess(): void {
    this.metrics.totalSuccesses++;
    this.metrics.lastSuccessTime = Date.now();

    if (this.circuitState === 'HALF_OPEN') {
      this.successCount++;
      logger.info('RuntimeManager', `Test GPU r√©ussi (${this.successCount}/${this.SUCCESS_THRESHOLD})`); // ‚úÖ Nouveau

      if (this.successCount >= this.SUCCESS_THRESHOLD) {
        this.closeCircuit();
      }
    } else if (this.circuitState === 'CLOSED') {
      // Reset le compteur d'√©checs si on √©tait en √©tat normal
      this.failureCount = Math.max(0, this.failureCount - 1);
    }
  }

  /**
   * Notifie le Circuit Breaker d'un √©chec.
   */
  public handleFailure(): void {
    this.metrics.totalFailures++;
    this.metrics.lastFailureTime = Date.now();

    if (this.circuitState === 'HALF_OPEN') {
      // √âchec pendant le test ‚Üí retour imm√©diat en OPEN
      logger.error('RuntimeManager', 'Test GPU √©chou√©. Retour en OPEN.', new Error('Test GPU failed')); // ‚úÖ Nouveau
      this.tripCircuitBreaker();
      return;
    }

    this.failureCount++;
    logger.warn('RuntimeManager', `√âchec GPU (${this.failureCount}/${this.FAILURE_THRESHOLD})`); // ‚úÖ Nouveau

    if (this.failureCount >= this.FAILURE_THRESHOLD) {
      this.tripCircuitBreaker();
    }
  }

  /**
   * Enregistre un rejet de t√¢che (backpressure)
   */
  public registerRejection(): void {
    this.rejectionCount++;
    this.metrics.rejectionCount = this.rejectionCount;
    logger.warn('RuntimeManager', `Rejet enregistr√© (${this.rejectionCount}/${this.REJECTION_THRESHOLD})`); // ‚úÖ Nouveau
    
    if (this.rejectionCount >= this.REJECTION_THRESHOLD) {
      this.tripCircuitBreakerHard();
    }
  }

  /**
   * Ouvre le circuit de mani√®re stricte (hard-open)
   */
  private tripCircuitBreakerHard(): void {
    logger.error('RuntimeManager', 'Tous les moteurs satur√©s, hard-open mode.', new Error('All engines saturated')); // ‚úÖ Nouveau
    this.circuitState = 'OPEN';
    this.fallbackUntil = Date.now() + this.FALLBACK_DURATION;
    this.metrics.fallbackUntil = this.fallbackUntil;
    sseStreamer.streamStatus('Syst√®me en surcharge. Mise en pause temporaire.');
  }

  /**
   * Ouvre le circuit et passe en mode fallback.
   */
  private tripCircuitBreaker(): void {
    logger.error('RuntimeManager', 'üö® CIRCUIT OPEN ! Fallback CPU.', new Error('Circuit breaker opened')); // ‚úÖ Nouveau
    this.circuitState = 'OPEN';
    this.fallbackUntil = Date.now() + this.FALLBACK_DURATION;
    this.failureCount = 0; // Reset pour le prochain cycle
    this.metrics.fallbackUntil = this.fallbackUntil;
    sseStreamer.streamStatus("Mode d√©grad√© activ√© (CPU).");
  }

  /**
   * Ferme le circuit.
   */
  private closeCircuit(): void {
    logger.info('RuntimeManager', '‚úÖ Circuit CLOSED. GPU stable.'); // ‚úÖ Nouveau
    this.circuitState = 'CLOSED';
    this.failureCount = 0;
    this.successCount = 0;
    this.rejectionCount = 0;
    sseStreamer.streamStatus("Moteur principal r√©tabli (GPU).";
  }

  /**
   * V√©rifie si le syst√®me est en mode fallback
   */
  public isInFallbackMode(): boolean {
    return this.circuitState === 'OPEN' && Date.now() < this.fallbackUntil;
  }

  /**
   * Obtient les m√©triques du Circuit Breaker.
   */
  public getMetrics(): CircuitMetrics {
    return {
      ...this.metrics,
      state: this.circuitState,
      fallbackUntil: this.fallbackUntil,
      rejectionCount: this.rejectionCount
    };
  }

  // M√©thode pour les tests
  public forceGpuFailure(fail: boolean) {
    this.gpuEngine.forceFailure(fail);
  }
}

// Export singleton
export const runtimeManager = new RuntimeManager();