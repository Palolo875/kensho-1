/**
 * RuntimeManager - Production Implementation
 * 
 * G√®re le cycle de vie des runtimes d'inf√©rence (WebLLM, Transformers.js).
 * Responsable de :
 * - Initialisation et destruction des moteurs
 * - Gestion de la m√©moire GPU/CPU
 * - Basculement entre diff√©rents backends
 * - Monitoring des performances d'inf√©rence
 * 
 * En mode d√©veloppement, les appels aux vrais moteurs peuvent √™tre
 * remplac√©s par des mocks via l'injection de d√©pendances.
 */

import { createLogger } from '../../lib/logger';
import { storageManager } from './StorageManager';
import { resourceManager } from './ResourceManager';
import { memoryManager } from './MemoryManager';

const log = createLogger('RuntimeManager');

log.info('üöÄ RuntimeManager (Production) initialis√©.');

// Types pour les diff√©rents backends support√©s
export type RuntimeBackend = 'webllm' | 'transformers' | 'mock';

export interface RuntimeConfig {
  backend: RuntimeBackend;
  modelId: string;
  options?: {
    maxTokens?: number;
    temperature?: number;
    topP?: number;
    useGPU?: boolean;
  };
}

export interface RuntimeStatus {
  isReady: boolean;
  backend: RuntimeBackend | null;
  modelId: string | null;
  memoryUsage: number;
  lastInferenceTime: number | null;
  totalInferences: number;
}

export interface InferenceResult {
  text: string;
  tokensGenerated: number;
  timeMs: number;
  finishReason: 'stop' | 'length' | 'error';
}

export interface ProgressCallback {
  (progress: { phase: string; progress: number; text: string }): void;
}

// Interface pour l'injection de d√©pendances (permet les mocks)
export interface IInferenceEngine {
  load(modelId: string, onProgress?: ProgressCallback): Promise<void>;
  generate(prompt: string, options?: RuntimeConfig['options']): Promise<InferenceResult>;
  generateStream(
    prompt: string, 
    onChunk: (chunk: string) => void,
    options?: RuntimeConfig['options']
  ): Promise<InferenceResult>;
  unload(): Promise<void>;
  isLoaded(): boolean;
  getModelId(): string | null;
}

/**
 * Mock Engine pour le d√©veloppement et les tests
 */
class MockInferenceEngine implements IInferenceEngine {
  private loaded = false;
  private currentModelId: string | null = null;

  async load(modelId: string, onProgress?: ProgressCallback): Promise<void> {
    log.info(`[Mock] Chargement simul√© du mod√®le: ${modelId}`);
    
    // Simuler les √©tapes de chargement
    const phases = ['downloading', 'loading', 'compiling', 'ready'];
    for (let i = 0; i < phases.length; i++) {
      await new Promise(resolve => setTimeout(resolve, 100));
      onProgress?.({
        phase: phases[i],
        progress: (i + 1) / phases.length,
        text: `[Mock] ${phases[i]}...`
      });
    }

    this.loaded = true;
    this.currentModelId = modelId;
    log.info(`[Mock] Mod√®le ${modelId} charg√© avec succ√®s`);
  }

  async generate(prompt: string, _options?: RuntimeConfig['options']): Promise<InferenceResult> {
    if (!this.loaded) {
      throw new Error('[Mock] Aucun mod√®le charg√©');
    }

    const startTime = performance.now();
    
    // Simuler un d√©lai d'inf√©rence
    await new Promise(resolve => setTimeout(resolve, 50 + Math.random() * 100));

    const mockResponse = this.generateMockResponse(prompt);
    const timeMs = performance.now() - startTime;

    return {
      text: mockResponse,
      tokensGenerated: mockResponse.split(' ').length,
      timeMs,
      finishReason: 'stop'
    };
  }

  async generateStream(
    prompt: string,
    onChunk: (chunk: string) => void,
    _options?: RuntimeConfig['options']
  ): Promise<InferenceResult> {
    if (!this.loaded) {
      throw new Error('[Mock] Aucun mod√®le charg√©');
    }

    const startTime = performance.now();
    const mockResponse = this.generateMockResponse(prompt);
    const words = mockResponse.split(' ');

    // Simuler le streaming mot par mot
    for (const word of words) {
      await new Promise(resolve => setTimeout(resolve, 20 + Math.random() * 30));
      onChunk(word + ' ');
    }

    const timeMs = performance.now() - startTime;

    return {
      text: mockResponse,
      tokensGenerated: words.length,
      timeMs,
      finishReason: 'stop'
    };
  }

  async unload(): Promise<void> {
    log.info(`[Mock] D√©chargement du mod√®le: ${this.currentModelId}`);
    this.loaded = false;
    this.currentModelId = null;
  }

  isLoaded(): boolean {
    return this.loaded;
  }

  getModelId(): string | null {
    return this.currentModelId;
  }

  private generateMockResponse(prompt: string): string {
    const responses = [
      "Je suis un assistant IA en mode simulation. Je peux vous aider avec vos questions.",
      "Voici une r√©ponse simul√©e pour tester le syst√®me sans t√©l√©charger de mod√®le.",
      "En mode mock, je g√©n√®re des r√©ponses pr√©d√©finies pour le d√©veloppement.",
      "Cette r√©ponse est g√©n√©r√©e par le MockInferenceEngine pour les tests.",
    ];
    
    // S√©lectionner une r√©ponse bas√©e sur le hash du prompt
    const hash = prompt.split('').reduce((a, b) => a + b.charCodeAt(0), 0);
    return responses[hash % responses.length];
  }
}

/**
 * RuntimeManager - Gestionnaire principal des runtimes d'inf√©rence
 */
class RuntimeManager {
  private engine: IInferenceEngine | null = null;
  private currentBackend: RuntimeBackend | null = null;
  private status: RuntimeStatus = {
    isReady: false,
    backend: null,
    modelId: null,
    memoryUsage: 0,
    lastInferenceTime: null,
    totalInferences: 0
  };

  constructor() {
    log.info('RuntimeManager cr√©√©');
  }

  /**
   * Initialise le runtime avec un backend sp√©cifique
   */
  public async initialize(
    config: RuntimeConfig,
    onProgress?: ProgressCallback
  ): Promise<boolean> {
    try {
      log.info(`Initialisation du runtime: ${config.backend} / ${config.modelId}`);

      // V√©rifier les ressources disponibles
      const deviceStatus = await resourceManager.getStatus();
      if (deviceStatus.memory.usageRatio > 0.85) {
        log.warn('M√©moire insuffisante pour charger un mod√®le');
        return false;
      }

      // V√©rifier si le mod√®le peut √™tre charg√©
      if (config.backend !== 'mock' && !memoryManager.canLoadModel(config.modelId)) {
        log.warn(`VRAM insuffisante pour le mod√®le: ${config.modelId}`);
        return false;
      }

      // D√©charger l'ancien moteur si n√©cessaire
      if (this.engine?.isLoaded()) {
        await this.shutdown();
      }

      // Cr√©er le moteur appropri√©
      this.engine = await this.createEngine(config.backend);
      this.currentBackend = config.backend;

      // Charger le mod√®le
      await this.engine.load(config.modelId, onProgress);

      // Mettre √† jour le statut
      this.status = {
        isReady: true,
        backend: config.backend,
        modelId: config.modelId,
        memoryUsage: 0,
        lastInferenceTime: null,
        totalInferences: 0
      };

      // V√©rifier le cache OPFS
      const storageReady = await storageManager.ensureReady();
      if (storageReady) {
        log.info('Cache OPFS disponible pour les mod√®les');
      }

      log.info(`Runtime initialis√© avec succ√®s: ${config.backend}/${config.modelId}`);
      return true;

    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error));
      log.error('Erreur d\'initialisation du runtime:', err);
      this.status.isReady = false;
      return false;
    }
  }

  /**
   * Cr√©e une instance du moteur d'inf√©rence appropri√©
   */
  private async createEngine(backend: RuntimeBackend): Promise<IInferenceEngine> {
    switch (backend) {
      case 'mock':
        log.info('Utilisation du MockInferenceEngine');
        return new MockInferenceEngine();

      case 'webllm':
        // En production, on importerait dynamiquement WebLLM
        log.info('WebLLM backend demand√© - utilisation du mock pour l\'instant');
        // TODO: Impl√©menter WebLLMEngine quand pr√™t
        // return new WebLLMEngine();
        return new MockInferenceEngine();

      case 'transformers':
        // En production, on importerait dynamiquement Transformers.js
        log.info('Transformers.js backend demand√© - utilisation du mock pour l\'instant');
        // TODO: Impl√©menter TransformersEngine quand pr√™t
        // return new TransformersEngine();
        return new MockInferenceEngine();

      default:
        throw new Error(`Backend non support√©: ${backend}`);
    }
  }

  /**
   * G√©n√®re une r√©ponse (mode non-streaming)
   */
  public async generate(
    prompt: string,
    options?: RuntimeConfig['options']
  ): Promise<InferenceResult> {
    if (!this.engine || !this.status.isReady) {
      throw new Error('Runtime non initialis√©');
    }

    const startTime = performance.now();
    
    try {
      const result = await this.engine.generate(prompt, options);
      
      this.status.lastInferenceTime = performance.now() - startTime;
      this.status.totalInferences++;

      log.debug(`Inf√©rence compl√®te: ${result.tokensGenerated} tokens en ${result.timeMs.toFixed(0)}ms`);
      
      return result;
    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error));
      log.error('Erreur d\'inf√©rence:', err);
      throw err;
    }
  }

  /**
   * G√©n√®re une r√©ponse en streaming
   */
  public async generateStream(
    prompt: string,
    onChunk: (chunk: string) => void,
    options?: RuntimeConfig['options']
  ): Promise<InferenceResult> {
    if (!this.engine || !this.status.isReady) {
      throw new Error('Runtime non initialis√©');
    }

    const startTime = performance.now();

    try {
      const result = await this.engine.generateStream(prompt, onChunk, options);
      
      this.status.lastInferenceTime = performance.now() - startTime;
      this.status.totalInferences++;

      log.debug(`Streaming compl√©t√©: ${result.tokensGenerated} tokens en ${result.timeMs.toFixed(0)}ms`);
      
      return result;
    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error));
      log.error('Erreur de streaming:', err);
      throw err;
    }
  }

  /**
   * Change de mod√®le sans changer de backend
   */
  public async switchModel(
    modelId: string,
    onProgress?: ProgressCallback
  ): Promise<boolean> {
    if (!this.currentBackend) {
      log.error('Aucun backend actif');
      return false;
    }

    return this.initialize({
      backend: this.currentBackend,
      modelId
    }, onProgress);
  }

  /**
   * Arr√™te le runtime et lib√®re les ressources
   */
  public async shutdown(): Promise<void> {
    if (this.engine) {
      try {
        await this.engine.unload();
        log.info('Runtime arr√™t√© proprement');
      } catch (error) {
        log.error('Erreur lors de l\'arr√™t du runtime:', error as Error);
      }
    }

    this.engine = null;
    this.currentBackend = null;
    this.status = {
      isReady: false,
      backend: null,
      modelId: null,
      memoryUsage: 0,
      lastInferenceTime: null,
      totalInferences: 0
    };
  }

  /**
   * Retourne le statut actuel du runtime
   */
  public getStatus(): RuntimeStatus {
    return { ...this.status };
  }

  /**
   * V√©rifie si le runtime est pr√™t
   */
  public isReady(): boolean {
    return this.status.isReady && this.engine?.isLoaded() === true;
  }

  /**
   * Retourne le backend actuel
   */
  public getCurrentBackend(): RuntimeBackend | null {
    return this.currentBackend;
  }

  /**
   * Injecte un moteur personnalis√© (pour les tests)
   */
  public setEngine(engine: IInferenceEngine): void {
    this.engine = engine;
    log.info('Moteur d\'inf√©rence inject√© manuellement');
  }
}

// Export singleton
export const runtimeManager = new RuntimeManager();

