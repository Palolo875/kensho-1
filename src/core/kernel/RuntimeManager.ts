/**
 * RuntimeManager - Production Implementation
 *
 * G√®re le cycle de vie des runtimes d'inf√©rence (WebLLM, Transformers.js).
 * Responsable de :
 * - Initialisation et destruction des moteurs
 * - Gestion de la m√©moire GPU/CPU
 * - D√©tection automatique de WebGPU
 * - Basculement entre diff√©rents backends
 * - Monitoring des performances d'inf√©rence
 *
 * En mode d√©veloppement, les appels aux vrais moteurs peuvent √™tre
 * remplac√©s par des mocks via l'injection de d√©pendances.
 */

import { createLogger } from '../../lib/logger';
import { storageManager } from './StorageManager';
import { resourceManager } from './ResourceManager';
import { memoryManager } from './MemoryManager';
import {
  MockWebLLMEngine,
  MockTransformersJSEngine,
  createMockEngine,
} from '../runtime/mocks/mock-engines';

const log = createLogger('RuntimeManager');

log.info('üöÄ RuntimeManager (Production) initialis√©.');

// Types pour les diff√©rents backends support√©s
export type RuntimeBackend = 'webllm' | 'transformers' | 'mock' | 'auto';

export interface RuntimeConfig {
  backend: RuntimeBackend;
  modelId: string;
  options?: {
    maxTokens?: number;
    temperature?: number;
    topP?: number;
    useGPU?: boolean;
  };
}

export interface RuntimeStatus {
  isReady: boolean;
  backend: RuntimeBackend | null;
  modelId: string | null;
  memoryUsage: number;
  lastInferenceTime: number | null;
  totalInferences: number;
  gpuAvailable: boolean;
  gpuInfo: GPUInfo | null;
}

export interface GPUInfo {
  vendor: string;
  architecture: string;
  device: string;
  description: string;
}

export interface InferenceResult {
  text: string;
  tokensGenerated: number;
  timeMs: number;
  finishReason: 'stop' | 'length' | 'error';
}

export interface ProgressCallback {
  (progress: { phase: string; progress: number; text: string }): void;
}

// Interface pour l'injection de d√©pendances (permet les mocks)
export interface IInferenceEngine {
  load(modelId: string, onProgress?: ProgressCallback): Promise<void>;
  generate(prompt: string, options?: RuntimeConfig['options']): Promise<InferenceResult>;
  generateStream(
    prompt: string,
    onChunk: (chunk: string) => void,
    options?: RuntimeConfig['options']
  ): Promise<InferenceResult>;
  unload(): Promise<void>;
  isLoaded(): boolean;
  getModelId(): string | null;
}

/**
 * RuntimeManager - Gestionnaire principal des runtimes d'inf√©rence
 */
class RuntimeManager {
  private engine: IInferenceEngine | null = null;
  private currentBackend: RuntimeBackend | null = null;
  private gpuAvailable: boolean | null = null;
  private gpuInfo: GPUInfo | null = null;
  private status: RuntimeStatus = {
    isReady: false,
    backend: null,
    modelId: null,
    memoryUsage: 0,
    lastInferenceTime: null,
    totalInferences: 0,
    gpuAvailable: false,
    gpuInfo: null,
  };

  constructor() {
    log.info('RuntimeManager cr√©√©');
    // D√©tecter WebGPU au d√©marrage
    this.detectWebGPU().then(available => {
      this.status.gpuAvailable = available;
      this.status.gpuInfo = this.gpuInfo;
      log.info(`WebGPU disponible: ${available}`);
    });
  }

  /**
   * D√©tecte la disponibilit√© de WebGPU et r√©cup√®re les informations GPU
   */
  public async detectWebGPU(): Promise<boolean> {
    if (this.gpuAvailable !== null) {
      return this.gpuAvailable;
    }

    try {
      if (typeof navigator === 'undefined' || !navigator.gpu) {
        log.info('WebGPU API non disponible dans cet environnement');
        this.gpuAvailable = false;
        return false;
      }

      const adapter = await navigator.gpu.requestAdapter();
      if (!adapter) {
        log.warn('WebGPU: Aucun adaptateur GPU trouv√©');
        this.gpuAvailable = false;
        return false;
      }

      // R√©cup√©rer les informations sur le GPU
      const adapterInfo = await adapter.requestAdapterInfo();
      this.gpuInfo = {
        vendor: adapterInfo.vendor || 'Unknown',
        architecture: adapterInfo.architecture || 'Unknown',
        device: adapterInfo.device || 'Unknown',
        description: adapterInfo.description || 'Unknown GPU',
      };

      log.info('WebGPU d√©tect√©:', this.gpuInfo);
      this.gpuAvailable = true;
      return true;
    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error));
      log.warn('Erreur lors de la d√©tection WebGPU:', err);
      this.gpuAvailable = false;
      return false;
    }
  }

  /**
   * S√©lectionne automatiquement le meilleur backend disponible
   */
  public async autoSelectBackend(): Promise<RuntimeBackend> {
    const hasGPU = await this.detectWebGPU();

    if (hasGPU) {
      log.info('‚úÖ WebGPU disponible - S√©lection du backend GPU (webllm)');
      return 'webllm';
    } else {
      log.info('‚ö†Ô∏è WebGPU non disponible - S√©lection du backend CPU (transformers)');
      return 'transformers';
    }
  }

  /**
   * Initialise le runtime avec un backend sp√©cifique
   */
  public async initialize(
    config: RuntimeConfig,
    onProgress?: ProgressCallback
  ): Promise<boolean> {
    try {
      let backend = config.backend;

      // Si 'auto', s√©lectionner le meilleur backend
      if (backend === 'auto') {
        backend = await this.autoSelectBackend();
        log.info(`Backend auto-s√©lectionn√©: ${backend}`);
      }

      log.info(`Initialisation du runtime: ${backend} / ${config.modelId}`);

      // V√©rifier les ressources disponibles
      const deviceStatus = await resourceManager.getStatus();
      if (deviceStatus.memory.usageRatio > 0.85) {
        log.warn('M√©moire insuffisante pour charger un mod√®le');
        return false;
      }

      // V√©rifier si le mod√®le peut √™tre charg√© (sauf pour mock)
      if (backend !== 'mock' && !memoryManager.canLoadModel(config.modelId)) {
        log.warn(`VRAM insuffisante pour le mod√®le: ${config.modelId}`);
        // En mode auto ou transformers, on peut fallback sur CPU
        if (backend === 'webllm') {
          log.info('Fallback vers le backend CPU (transformers)');
          backend = 'transformers';
        }
      }

      // D√©charger l'ancien moteur si n√©cessaire
      if (this.engine?.isLoaded()) {
        await this.shutdown();
      }

      // Cr√©er le moteur appropri√©
      this.engine = await this.createEngine(backend);
      this.currentBackend = backend;

      // Charger le mod√®le
      await this.engine.load(config.modelId, onProgress);

      // Mettre √† jour le statut
      this.status = {
        isReady: true,
        backend: backend,
        modelId: config.modelId,
        memoryUsage: 0,
        lastInferenceTime: null,
        totalInferences: 0,
        gpuAvailable: this.gpuAvailable ?? false,
        gpuInfo: this.gpuInfo,
      };

      // V√©rifier le cache OPFS
      const storageReady = await storageManager.ensureReady();
      if (storageReady) {
        log.info('Cache OPFS disponible pour les mod√®les');
      }

      log.info(`Runtime initialis√© avec succ√®s: ${backend}/${config.modelId}`);
      return true;
    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error));
      log.error("Erreur d'initialisation du runtime:", err);
      this.status.isReady = false;
      return false;
    }
  }

  /**
   * Initialise le runtime avec s√©lection automatique du backend
   */
  public async initializeAuto(
    modelId: string,
    onProgress?: ProgressCallback
  ): Promise<boolean> {
    return this.initialize(
      {
        backend: 'auto',
        modelId,
      },
      onProgress
    );
  }

  /**
   * Cr√©e une instance du moteur d'inf√©rence appropri√©
   */
  private async createEngine(backend: RuntimeBackend): Promise<IInferenceEngine> {
    switch (backend) {
      case 'mock':
        log.info('Utilisation du MockInferenceEngine (g√©n√©rique)');
        return createMockEngine(this.gpuAvailable ? 'GPU' : 'CPU');

      case 'webllm':
        // En production, on importerait dynamiquement WebLLM
        log.info('WebLLM backend demand√© - utilisation du mock GPU pour l\'instant');
        // TODO: Impl√©menter WebLLMEngine quand pr√™t
        // const { WebLLMEngine } = await import('../runtime/webllm/WebLLMEngine');
        // return new WebLLMEngine();
        return new MockWebLLMEngine();

      case 'transformers':
        // En production, on importerait dynamiquement Transformers.js
        log.info('Transformers.js backend demand√© - utilisation du mock CPU pour l\'instant');
        // TODO: Impl√©menter TransformersEngine quand pr√™t
        // const { TransformersEngine } = await import('../runtime/transformers/TransformersEngine');
        // return new TransformersEngine();
        return new MockTransformersJSEngine();

      case 'auto':
        // Ne devrait pas arriver ici car 'auto' est r√©solu dans initialize()
        const selectedBackend = await this.autoSelectBackend();
        return this.createEngine(selectedBackend);

      default:
        throw new Error(`Backend non support√©: ${backend}`);
    }
  }

  /**
   * G√©n√®re une r√©ponse (mode non-streaming)
   */
  public async generate(
    prompt: string,
    options?: RuntimeConfig['options']
  ): Promise<InferenceResult> {
    if (!this.engine || !this.status.isReady) {
      throw new Error('Runtime non initialis√©');
    }

    const startTime = performance.now();

    try {
      const result = await this.engine.generate(prompt, options);

      this.status.lastInferenceTime = performance.now() - startTime;
      this.status.totalInferences++;

      log.debug(
        `Inf√©rence compl√®te: ${result.tokensGenerated} tokens en ${result.timeMs.toFixed(0)}ms`
      );

      return result;
    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error));
      log.error("Erreur d'inf√©rence:", err);
      throw err;
    }
  }

  /**
   * G√©n√®re une r√©ponse en streaming
   */
  public async generateStream(
    prompt: string,
    onChunk: (chunk: string) => void,
    options?: RuntimeConfig['options']
  ): Promise<InferenceResult> {
    if (!this.engine || !this.status.isReady) {
      throw new Error('Runtime non initialis√©');
    }

    const startTime = performance.now();

    try {
      const result = await this.engine.generateStream(prompt, onChunk, options);

      this.status.lastInferenceTime = performance.now() - startTime;
      this.status.totalInferences++;

      log.debug(
        `Streaming compl√©t√©: ${result.tokensGenerated} tokens en ${result.timeMs.toFixed(0)}ms`
      );

      return result;
    } catch (error) {
      const err = error instanceof Error ? error : new Error(String(error));
      log.error('Erreur de streaming:', err);
      throw err;
    }
  }

  /**
   * Change de mod√®le sans changer de backend
   */
  public async switchModel(modelId: string, onProgress?: ProgressCallback): Promise<boolean> {
    if (!this.currentBackend) {
      log.error('Aucun backend actif');
      return false;
    }

    return this.initialize(
      {
        backend: this.currentBackend,
        modelId,
      },
      onProgress
    );
  }

  /**
   * Change de backend
   */
  public async switchBackend(
    backend: RuntimeBackend,
    modelId?: string,
    onProgress?: ProgressCallback
  ): Promise<boolean> {
    const model = modelId ?? this.status.modelId;
    if (!model) {
      log.error('Aucun mod√®le sp√©cifi√©');
      return false;
    }

    return this.initialize(
      {
        backend,
        modelId: model,
      },
      onProgress
    );
  }

  /**
   * Arr√™te le runtime et lib√®re les ressources
   */
  public async shutdown(): Promise<void> {
    if (this.engine) {
      try {
        await this.engine.unload();
        log.info('Runtime arr√™t√© proprement');
      } catch (error) {
        log.error("Erreur lors de l'arr√™t du runtime:", error as Error);
      }
    }

    this.engine = null;
    this.currentBackend = null;
    this.status = {
      isReady: false,
      backend: null,
      modelId: null,
      memoryUsage: 0,
      lastInferenceTime: null,
      totalInferences: 0,
      gpuAvailable: this.gpuAvailable ?? false,
      gpuInfo: this.gpuInfo,
    };
  }

  /**
   * Retourne le statut actuel du runtime
   */
  public getStatus(): RuntimeStatus {
    return { ...this.status };
  }

  /**
   * V√©rifie si le runtime est pr√™t
   */
  public isReady(): boolean {
    return this.status.isReady && this.engine?.isLoaded() === true;
  }

  /**
   * Retourne le backend actuel
   */
  public getCurrentBackend(): RuntimeBackend | null {
    return this.currentBackend;
  }

  /**
   * V√©rifie si WebGPU est disponible
   */
  public isGPUAvailable(): boolean {
    return this.gpuAvailable ?? false;
  }

  /**
   * Retourne les informations GPU
   */
  public getGPUInfo(): GPUInfo | null {
    return this.gpuInfo;
  }

  /**
   * Injecte un moteur personnalis√© (pour les tests)
   */
  public setEngine(engine: IInferenceEngine): void {
    this.engine = engine;
    log.info("Moteur d'inf√©rence inject√© manuellement");
  }
}

// Export singleton
export const runtimeManager = new RuntimeManager();
