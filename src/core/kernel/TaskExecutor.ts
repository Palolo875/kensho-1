/**
 * TaskExecutor v3.0 - Chef de Chantier Intelligent
 * 
 * Architecture FINALE valid√©e:
 * ‚úÖ Queue globale avec TOUTE l'ex√©cution dans le job (streaming inclus)
 * ‚úÖ Streaming via callback pattern (onChunk) 
 * ‚úÖ Vraie cancellation avec engine.interruptGenerate()
 * ‚úÖ Concurrence strictement respect√©e (SERIAL/LIMITED/FULL)
 * 
 * BUGFIX Architectural:
 * - Le streaming loop s'ex√©cute maintenant ENTI√àREMENT dans le job PQueue
 * - Les chunks sont envoy√©s via callbacks, pas via async generator externe
 * - La queue ne lib√®re le slot QUE quand toute la g√©n√©ration est termin√©e
 */

import PQueue from 'p-queue';
import { MLCEngine } from '@mlc-ai/web-llm';
import { modelManager } from './ModelManager';
import { Router } from '../router/Router';
import { 
  ExecutionPlan, 
  Task, 
  TaskResult, 
  StreamChunk, 
  ExecutionStrategy
} from '../router/RouterTypes';
import { fusioner } from './Fusioner';

console.log("üë∑ TaskExecutor v3.0 - Chef de Chantier Intelligent initialis√©");

export class TaskExecutor {
  private router: Router;
  private globalQueue: PQueue;
  private activeRequests = 0;

  constructor() {
    this.router = new Router();
    // Queue GLOBALE : limite totale de workers simultan√©s
    this.globalQueue = new PQueue({ 
      concurrency: 4,
      timeout: 120000
    });
  }

  /**
   * Process avec streaming pour UX optimale
   * 
   * ARCHITECTURE CRITIQUE:
   * - TOUT le travail (y compris streaming) se passe dans le job PQueue
   * - On utilise des callbacks pour envoyer les chunks pendant que le job tourne
   * - La queue ne lib√®re le slot QUE quand la g√©n√©ration est compl√®te
   */
  public async *processStream(userPrompt: string): AsyncGenerator<StreamChunk> {
    this.activeRequests++;
    const requestId = crypto.randomUUID().substring(0, 8);
    console.log(`[TaskExecutor] üöÄ Requ√™te #${requestId} (${this.activeRequests} active(s)): "${userPrompt.substring(0, 50)}..."`);

    try {
      // 1. Obtenir le plan du Router
      const plan = await this.router.createPlan(userPrompt);
      console.log(`[TaskExecutor #${requestId}] üìã Plan: ${plan.strategy}, ${plan.fallbackTasks.length + 1} t√¢che(s)`);
      await this.router.validatePlan(plan);

      // 2. Buffer pour collecter les chunks stream√©s DEPUIS le job PQueue
      const chunks: StreamChunk[] = [];
      let primaryResult: TaskResult | null = null;

      // 3. Callback qui sera appel√© DEPUIS le job PQueue pour envoyer les chunks
      const onChunk = (chunk: StreamChunk) => {
        chunks.push(chunk);
      };

      // 4. Lancer la t√¢che primaire via la queue globale
      //    TOUT le streaming se passe DANS ce job
      const primaryPromise = this.globalQueue.add(async () => {
        return await this.executeStreamingTaskInQueue(
          plan.primaryTask,
          userPrompt,
          onChunk
        );
      }, { priority: 100 }); // Priorit√© max

      // 5. Lancer les t√¢ches fallback en parall√®le
      const fallbackPromises = plan.fallbackTasks.map((task) =>
        this.globalQueue.add(
          () => this.executeTaskWithTimeout(task, userPrompt),
          { priority: this.getPriorityValue(task.priority) }
        )
      );

      // 6. Streamer les chunks au fur et √† mesure qu'ils arrivent
      //    On poll le buffer de chunks pendant que le job tourne
      const pollInterval = 50; // ms
      let lastIndex = 0;

      // Polling loop: envoyer les chunks pendant que le job tourne
      while (!primaryResult) {
        // Envoyer les nouveaux chunks
        for (let i = lastIndex; i < chunks.length; i++) {
          yield chunks[i];
          lastIndex = i + 1;
        }
        
        // V√©rifier si le job est termin√©
        try {
          primaryResult = await Promise.race([
            primaryPromise,
            new Promise<null>(resolve => setTimeout(() => resolve(null), pollInterval))
          ]);
        } catch (e) {
          // Erreur dans le job, on sortira au prochain tour
        }
      }

      // 7. Attendre et envoyer les chunks restants
      if (!primaryResult) {
        primaryResult = await primaryPromise;
      }
      
      for (let i = lastIndex; i < chunks.length; i++) {
        yield chunks[i];
      }

      // 8. Attendre les r√©sultats fallback
      let expertResults: TaskResult[] = [];
      if (fallbackPromises.length > 0) {
        console.log(`[TaskExecutor #${requestId}] ‚è≥ Attente de ${fallbackPromises.length} fallback(s)...`);
        expertResults = await Promise.all(fallbackPromises);
        
        const successCount = expertResults.filter(r => r.status === 'success').length;
        console.log(`[TaskExecutor #${requestId}] üìä Fallback: ${successCount}/${expertResults.length} r√©ussis`);
      }

      // 9. Fusionner et envoyer le r√©sultat final
      const finalResponse = await fusioner.fuse({
        primaryResult,
        expertResults
      });

      yield { 
        type: 'fusion', 
        content: finalResponse,
        expertResults 
      };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Erreur inconnue';
      console.error(`[TaskExecutor #${requestId}] ‚ùå Erreur:`, error);
      
      yield {
        type: 'status',
        status: `Erreur: ${errorMessage}`
      };
      
      throw error;
    } finally {
      this.activeRequests--;
    }
  }

  /**
   * Process classique (sans streaming)
   */
  public async process(userPrompt: string): Promise<string> {
    let finalContent = '';
    
    for await (const chunk of this.processStream(userPrompt)) {
      if (chunk.type === 'fusion' && chunk.content) {
        finalContent = chunk.content;
      }
    }

    return finalContent;
  }

  /**
   * Ex√©cute une t√¢che avec streaming EN ENTIER dans le job PQueue
   * 
   * ARCHITECTURE CRITIQUE:
   * - Toute la g√©n√©ration (y compris streaming) se passe DANS cette fonction
   * - Les chunks sont envoy√©s via onChunk pendant que le job tourne
   * - La fonction ne retourne QUE quand toute la g√©n√©ration est termin√©e
   * - La queue garde le slot occup√© pendant TOUTE la dur√©e
   */
  private async executeStreamingTaskInQueue(
    task: Task,
    userPrompt: string,
    onChunk: (chunk: StreamChunk) => void
  ): Promise<TaskResult> {
    const engine = await modelManager.getEngine();
    const startTime = performance.now();
    let timedOut = false;
    const timeoutMs = task.timeout;

    // Setup timeout avec VRAIE interruption
    const timeoutId = setTimeout(async () => {
      timedOut = true;
      console.warn(`   [Worker] ‚è±Ô∏è  Timeout ${task.agentName}, interruption...`);
      await engine.interruptGenerate(); // VRAIE cancellation
    }, timeoutMs);

    try {
      console.log(`   [Worker] ‚ñ∂Ô∏è  ${task.agentName} d√©marr√© (queue: ${this.globalQueue.pending}/${this.globalQueue.concurrency})`);
      
      // Envoyer status
      onChunk({ type: 'status', status: `Ex√©cution de ${task.agentName}...` });

      // Charger le mod√®le si n√©cessaire
      if (!modelManager.isModelLoaded(task.modelKey)) {
        console.log(`   [Worker] üîÑ Chargement du mod√®le ${task.modelKey}...`);
        onChunk({ type: 'status', status: `Chargement du mod√®le ${task.modelKey}...` });
        await modelManager.switchModel(task.modelKey);
      }

      const prompt = task.prompt || userPrompt;

      // Streaming de la g√©n√©ration
      const stream = await engine.chat.completions.create({
        messages: [{ role: 'user', content: prompt }],
        stream: true,
        temperature: task.temperature
      });

      let accumulatedContent = '';

      // CRITIQUE: Cette boucle s'ex√©cute DANS le job PQueue
      // La queue ne lib√®re le slot QUE quand cette boucle est termin√©e
      for await (const chunk of stream) {
        if (timedOut) {
          break;
        }

        const content = chunk.choices[0]?.delta?.content;
        if (content) {
          accumulatedContent += content;
          // Envoyer le chunk via callback (pendant que le job tourne)
          onChunk({ type: 'primary', content });
        }
      }

      clearTimeout(timeoutId);

      const duration = performance.now() - startTime;
      const status = timedOut ? 'timeout' : 'success';
      
      console.log(`   [Worker] ‚úÖ ${task.agentName} ${status} (${duration.toFixed(0)}ms, ${accumulatedContent.length} chars)`);
      
      onChunk({ type: 'status', status: `${task.agentName} termin√©` });

      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        result: accumulatedContent,
        status,
        duration
      };

    } catch (error) {
      clearTimeout(timeoutId);
      const duration = performance.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      onChunk({ type: 'status', status: `Erreur: ${errorMessage}` });
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        error: errorMessage,
        status: timedOut ? 'timeout' : 'error',
        duration
      };
    }
  }

  /**
   * Ex√©cute une t√¢che SANS streaming (pour fallback)
   */
  private async executeTaskWithTimeout(task: Task, userPrompt: string): Promise<TaskResult> {
    const engine = await modelManager.getEngine();
    const startTime = performance.now();
    const timeoutMs = task.timeout;
    let timedOut = false;

    const timeoutId = setTimeout(async () => {
      timedOut = true;
      console.warn(`   [Worker] ‚è±Ô∏è  Timeout ${task.agentName}...`);
      await engine.interruptGenerate();
    }, timeoutMs);

    try {
      console.log(`   [Worker] ‚ñ∂Ô∏è  ${task.agentName} d√©marr√©`);
      
      if (!modelManager.isModelLoaded(task.modelKey)) {
        await modelManager.switchModel(task.modelKey);
      }

      const prompt = task.prompt || userPrompt;

      const response = await engine.chat.completions.create({
        messages: [{ role: 'user', content: prompt }],
        temperature: task.temperature
      });

      clearTimeout(timeoutId);

      const content = response.choices[0]?.message?.content || "";
      const duration = performance.now() - startTime;
      
      console.log(`   [Worker] ‚úÖ ${task.agentName} termin√© (${duration.toFixed(0)}ms)`);
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        result: content,
        status: timedOut ? 'timeout' : 'success',
        duration
      };

    } catch (error) {
      clearTimeout(timeoutId);
      const duration = performance.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        error: errorMessage,
        status: timedOut ? 'timeout' : 'error',
        duration
      };
    }
  }

  private getPriorityValue(priority: string): number {
    switch (priority) {
      case 'HIGH': return 10;
      case 'MEDIUM': return 5;
      case 'LOW': return 1;
      default: return 1;
    }
  }

  public getActiveRequestCount(): number {
    return this.activeRequests;
  }

  public getQueueStats() {
    return {
      activeRequests: this.activeRequests,
      pending: this.globalQueue.pending,
      size: this.globalQueue.size,
      concurrency: this.globalQueue.concurrency
    };
  }
}

export const taskExecutor = new TaskExecutor();
