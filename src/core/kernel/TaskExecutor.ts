/**
 * TaskExecutor v3.0 - Chef de Chantier Intelligent
 * 
 * ARCHITECTURE MULTI-QUEUE (FINALE):
 * ‚úÖ Queue s√©par√©e par strat√©gie (respect strict de concurrence)
 * ‚úÖ Streaming enti√®rement dans le job PQueue
 * ‚úÖ Vraie cancellation via engine.interruptGenerate()
 * ‚úÖ Callback pattern pour envoyer les chunks en temps r√©el
 * 
 * Strat√©gies:
 * - SERIAL: 1 t√¢che √† la fois
 * - PARALLEL_LIMITED: max 2 t√¢ches simultan√©es  
 * - PARALLEL_FULL: max 4 t√¢ches simultan√©es
 */

import PQueue from 'p-queue';
import { MLCEngine } from '@mlc-ai/web-llm';
import { modelManager } from './ModelManager';
import { Router } from '../router/Router';
import { 
  ExecutionPlan, 
  Task, 
  TaskResult, 
  StreamChunk, 
  ExecutionStrategy
} from '../router/RouterTypes';
import { fusioner } from './Fusioner';

console.log("üë∑ TaskExecutor v3.0 - Chef de Chantier Intelligent (Multi-Queue)");

export class TaskExecutor {
  private router: Router;
  private queueSerial: PQueue;
  private queueParallelLimited: PQueue;
  private queueParallelFull: PQueue;
  private activeRequests = 0;

  constructor() {
    this.router = new Router();
    
    // Queue pour SERIAL: 1 t√¢che √† la fois
    this.queueSerial = new PQueue({ 
      concurrency: 1,
      timeout: 120000
    });
    
    // Queue pour PARALLEL_LIMITED: max 2 t√¢ches
    this.queueParallelLimited = new PQueue({ 
      concurrency: 2,
      timeout: 120000
    });
    
    // Queue pour PARALLEL_FULL: max 4 t√¢ches
    this.queueParallelFull = new PQueue({ 
      concurrency: 4,
      timeout: 120000
    });
  }

  /**
   * Obtient la queue appropri√©e selon la strat√©gie
   */
  private getQueue(strategy: ExecutionStrategy): PQueue {
    switch (strategy) {
      case 'SERIAL':
        return this.queueSerial;
      case 'PARALLEL_LIMITED':
        return this.queueParallelLimited;
      case 'PARALLEL_FULL':
        return this.queueParallelFull;
      default:
        return this.queueSerial;
    }
  }

  /**
   * Process avec streaming pour UX optimale
   */
  public async *processStream(userPrompt: string): AsyncGenerator<StreamChunk> {
    this.activeRequests++;
    const requestId = crypto.randomUUID().substring(0, 8);
    console.log(`[TaskExecutor] üöÄ Requ√™te #${requestId} (${this.activeRequests} active(s))`);

    try {
      // 1. Obtenir le plan du Router
      const plan = await this.router.createPlan(userPrompt);
      console.log(`[TaskExecutor #${requestId}] üìã Plan: ${plan.strategy}, ${plan.fallbackTasks.length + 1} t√¢che(s)`);
      await this.router.validatePlan(plan);

      // 2. S√©lectionner la queue appropri√©e
      const queue = this.getQueue(plan.strategy);
      console.log(`[TaskExecutor #${requestId}] ‚öôÔ∏è  Strat√©gie: ${plan.strategy}`);

      // 3. Buffer pour les chunks stream√©s
      const chunks: StreamChunk[] = [];
      const onChunk = (chunk: StreamChunk) => {
        chunks.push(chunk);
      };

      // 4. Ex√©cuter la t√¢che primaire via la queue (avec streaming)
      const primaryPromise = queue.add(
        () => this.executeStreamingTaskInQueue(plan.primaryTask, userPrompt, onChunk),
        { priority: 100 }
      );

      // 5. Ex√©cuter les t√¢ches fallback via la m√™me queue
      const fallbackPromises = plan.fallbackTasks.map((task) =>
        queue.add(
          () => this.executeTaskWithTimeout(task, userPrompt),
          { priority: this.getPriorityValue(task.priority) }
        )
      );

      // 6. Polling: streamer les chunks pendant que les jobs tournent
      let primaryResult: TaskResult | null = null;
      let lastIndex = 0;
      const pollInterval = 50;

      while (!primaryResult) {
        // Envoyer les nouveaux chunks
        for (let i = lastIndex; i < chunks.length; i++) {
          yield chunks[i];
          lastIndex = i + 1;
        }

        // V√©rifier si le job primaire est termin√© (avec timeout)
        try {
          primaryResult = await Promise.race([
            primaryPromise,
            new Promise<null>(resolve => 
              setTimeout(() => resolve(null), pollInterval)
            )
          ]);
        } catch (e) {
          // Le job a √©chou√©, on le saura au prochain tour
        }
      }

      // 7. Envoyer les chunks restants
      for (let i = lastIndex; i < chunks.length; i++) {
        yield chunks[i];
      }

      // 8. Attendre les t√¢ches fallback
      let expertResults: TaskResult[] = [];
      if (fallbackPromises.length > 0) {
        console.log(`[TaskExecutor #${requestId}] ‚è≥ Attente de ${fallbackPromises.length} fallback(s)...`);
        expertResults = await Promise.all(fallbackPromises);
        
        const successCount = expertResults.filter(r => r.status === 'success').length;
        console.log(`[TaskExecutor #${requestId}] üìä Fallback: ${successCount}/${expertResults.length}`);
      }

      // 9. Fusionner et envoyer le r√©sultat final
      const finalResponse = await fusioner.fuse({
        primaryResult: primaryResult!,
        expertResults
      });

      yield { 
        type: 'fusion', 
        content: finalResponse,
        expertResults 
      };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Erreur inconnue';
      console.error(`[TaskExecutor] ‚ùå Erreur:`, error);
      
      yield {
        type: 'status',
        status: `Erreur: ${errorMessage}`
      };
      
      throw error;
    } finally {
      this.activeRequests--;
    }
  }

  /**
   * Process classique (sans streaming)
   */
  public async process(userPrompt: string): Promise<string> {
    let finalContent = '';
    
    for await (const chunk of this.processStream(userPrompt)) {
      if (chunk.type === 'fusion' && chunk.content) {
        finalContent = chunk.content;
      }
    }

    return finalContent;
  }

  /**
   * Ex√©cute une t√¢che avec streaming EN ENTIER dans le job PQueue
   * 
   * CRITIQUE: Toute la g√©n√©ration (y compris streaming) se passe DANS cette fonction
   * La queue ne lib√®re le slot QUE quand la g√©n√©ration est compl√®te
   */
  private async executeStreamingTaskInQueue(
    task: Task,
    userPrompt: string,
    onChunk: (chunk: StreamChunk) => void
  ): Promise<TaskResult> {
    const engine = await modelManager.getEngine();
    const startTime = performance.now();
    let timedOut = false;
    const timeoutMs = task.timeout;

    // Setup timeout avec VRAIE interruption
    const timeoutId = setTimeout(async () => {
      timedOut = true;
      console.warn(`   [Worker] ‚è±Ô∏è  Timeout ${task.agentName}, interruption...`);
      await engine.interruptGenerate();
    }, timeoutMs);

    try {
      console.log(`   [Worker] ‚ñ∂Ô∏è  ${task.agentName} d√©marr√©`);
      
      onChunk({ type: 'status', status: `Ex√©cution de ${task.agentName}...` });

      // Charger le mod√®le si n√©cessaire
      if (!modelManager.isModelLoaded(task.modelKey)) {
        console.log(`   [Worker] üîÑ Chargement du mod√®le ${task.modelKey}...`);
        onChunk({ type: 'status', status: `Chargement du mod√®le ${task.modelKey}...` });
        await modelManager.switchModel(task.modelKey);
      }

      const prompt = task.prompt || userPrompt;

      // Streaming de la g√©n√©ration (DANS le job PQueue)
      const stream = await engine.chat.completions.create({
        messages: [{ role: 'user', content: prompt }],
        stream: true,
        temperature: task.temperature
      });

      let accumulatedContent = '';

      // CRITIQUE: Cette boucle s'ex√©cute ENTI√àREMENT dans le job
      for await (const chunk of stream) {
        if (timedOut) break;

        const content = chunk.choices[0]?.delta?.content;
        if (content) {
          accumulatedContent += content;
          onChunk({ type: 'primary', content });
        }
      }

      clearTimeout(timeoutId);

      const duration = performance.now() - startTime;
      const status = timedOut ? 'timeout' : 'success';
      
      console.log(`   [Worker] ‚úÖ ${task.agentName} ${status} (${duration.toFixed(0)}ms)`);
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        result: accumulatedContent,
        status,
        duration
      };

    } catch (error) {
      clearTimeout(timeoutId);
      const duration = performance.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        error: errorMessage,
        status: timedOut ? 'timeout' : 'error',
        duration
      };
    }
  }

  /**
   * Ex√©cute une t√¢che SANS streaming (pour fallback)
   */
  private async executeTaskWithTimeout(task: Task, userPrompt: string): Promise<TaskResult> {
    const engine = await modelManager.getEngine();
    const startTime = performance.now();
    const timeoutMs = task.timeout;
    let timedOut = false;

    const timeoutId = setTimeout(async () => {
      timedOut = true;
      await engine.interruptGenerate();
    }, timeoutMs);

    try {
      console.log(`   [Worker] ‚ñ∂Ô∏è  ${task.agentName} d√©marr√©`);
      
      if (!modelManager.isModelLoaded(task.modelKey)) {
        await modelManager.switchModel(task.modelKey);
      }

      const prompt = task.prompt || userPrompt;

      const response = await engine.chat.completions.create({
        messages: [{ role: 'user', content: prompt }],
        temperature: task.temperature
      });

      clearTimeout(timeoutId);

      const content = response.choices[0]?.message?.content || "";
      const duration = performance.now() - startTime;
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        result: content,
        status: timedOut ? 'timeout' : 'success',
        duration
      };

    } catch (error) {
      clearTimeout(timeoutId);
      const duration = performance.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        error: errorMessage,
        status: timedOut ? 'timeout' : 'error',
        duration
      };
    }
  }

  private getPriorityValue(priority: string): number {
    switch (priority) {
      case 'HIGH': return 10;
      case 'MEDIUM': return 5;
      case 'LOW': return 1;
      default: return 1;
    }
  }

  public getActiveRequestCount(): number {
    return this.activeRequests;
  }

  public getQueueStats() {
    return {
      activeRequests: this.activeRequests,
      serialQueue: { pending: this.queueSerial.pending, concurrency: this.queueSerial.concurrency },
      parallelLimitedQueue: { pending: this.queueParallelLimited.pending, concurrency: this.queueParallelLimited.concurrency },
      parallelFullQueue: { pending: this.queueParallelFull.pending, concurrency: this.queueParallelFull.concurrency }
    };
  }
}

export const taskExecutor = new TaskExecutor();
