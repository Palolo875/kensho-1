/**
 * TaskExecutor v3.0 - Chef de Chantier Intelligent
 * 
 * Am√©liorations v3.0:
 * ‚úÖ Gestion stricte de la concurrence avec p-queue
 * ‚úÖ Gestion native des priorit√©s
 * ‚úÖ Timeouts avec AbortController
 * ‚úÖ Streaming pour TTFT optimal
 * ‚úÖ Observabilit√© compl√®te (activeWorkers, queueSize)
 */

import PQueue from 'p-queue';
import { modelManager } from './ModelManager';
import { router } from '../router/Router';
import { 
  ExecutionPlan, 
  Task, 
  TaskResult, 
  StreamChunk, 
  ExecutionStrategy,
  RouterError
} from '../router/RouterTypes';
import { fusioner } from './Fusioner';

console.log("üë∑ TaskExecutor v3.0 - Chef de Chantier Intelligent initialis√©");

export class TaskExecutor {
  private queue: PQueue;
  private activeWorkers = 0;

  constructor() {
    this.queue = new PQueue({ 
      concurrency: 1,
      timeout: 60000 // Timeout global de 60s
    });
  }

  /**
   * Process avec streaming pour UX optimale
   * Retourne un AsyncGenerator qui streame les chunks au fur et √† mesure
   */
  public async *processStream(userPrompt: string): AsyncGenerator<StreamChunk> {
    console.log(`[TaskExecutor] üöÄ Nouvelle requ√™te: "${userPrompt.substring(0, 50)}..."`);

    try {
      // 1. Obtenir le plan du Router
      const plan = await router.createPlan(userPrompt);
      console.log(`[TaskExecutor] üìã Plan cr√©√© | Strat√©gie: ${plan.strategy} | T√¢ches: ${plan.fallbackTasks.length + 1}`);

      // Valider le plan
      await router.validatePlan(plan);

      // 2. Ajuster la concurrence dynamiquement
      this.queue.concurrency = this.getConcurrencyLimit(plan.strategy);
      console.log(`[TaskExecutor] ‚öôÔ∏è  Concurrence: ${this.queue.concurrency}`);

      // 3. Lancer les t√¢ches fallback en arri√®re-plan (avec priorit√© plus basse)
      const fallbackPromises = plan.fallbackTasks.map((task, index) =>
        this.queue.add(
          () => this.executeTaskWithTimeout(task, userPrompt),
          { priority: this.getPriorityValue(task.priority) }
        )
      );

      // 4. Streamer la t√¢che primaire (priorit√© maximale)
      yield { type: 'status', status: `Ex√©cution de ${plan.primaryTask.agentName}...` };

      const engine = await modelManager.getEngine();
      
      // Charger le mod√®le si n√©cessaire
      if (!modelManager.isModelLoaded(plan.primaryTask.modelKey)) {
        console.log(`[TaskExecutor] üîÑ Chargement du mod√®le ${plan.primaryTask.modelKey}...`);
        yield { type: 'status', status: `Chargement du mod√®le ${plan.primaryTask.modelKey}...` };
        await modelManager.switchModel(plan.primaryTask.modelKey);
      }

      // Cr√©er le prompt pour la t√¢che primaire
      const primaryPrompt = plan.primaryTask.prompt || userPrompt;

      // Streamer la r√©ponse
      console.log(`[TaskExecutor] ‚ñ∂Ô∏è  Streaming ${plan.primaryTask.agentName}...`);
      const primaryStream = await engine.chat.completions.create({
        messages: [{ role: 'user', content: primaryPrompt }],
        stream: true,
        temperature: plan.primaryTask.temperature
      });

      let primaryContent = '';
      const startTime = performance.now();

      for await (const chunk of primaryStream) {
        const content = chunk.choices[0]?.delta?.content;
        if (content) {
          primaryContent += content;
          yield { type: 'primary', content };
        }
      }

      const duration = performance.now() - startTime;
      console.log(`[TaskExecutor] ‚úÖ ${plan.primaryTask.agentName} termin√© (${duration.toFixed(0)}ms, ${primaryContent.length} chars)`);

      // 5. Attendre les r√©sultats des t√¢ches fallback
      let expertResults: TaskResult[] = [];
      if (fallbackPromises.length > 0) {
        console.log(`[TaskExecutor] ‚è≥ Attente des ${fallbackPromises.length} t√¢che(s) fallback...`);
        expertResults = await Promise.all(fallbackPromises);
        
        const successCount = expertResults.filter(r => r.status === 'success').length;
        console.log(`[TaskExecutor] üìä Fallback: ${successCount}/${expertResults.length} r√©ussis`);
      }

      // 6. Fusionner les r√©sultats
      const primaryResult: TaskResult = {
        agentName: plan.primaryTask.agentName,
        modelKey: plan.primaryTask.modelKey,
        result: primaryContent,
        status: 'success',
        duration
      };

      const finalResponse = await fusioner.fuse({
        primaryResult,
        expertResults
      });

      // 7. Envoyer le chunk de fusion final
      yield { 
        type: 'fusion', 
        content: finalResponse,
        expertResults 
      };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Erreur inconnue';
      console.error(`[TaskExecutor] ‚ùå Erreur lors du traitement:`, error);
      
      yield {
        type: 'status',
        status: `Erreur: ${errorMessage}`
      };
      
      throw error;
    }
  }

  /**
   * Process classique (sans streaming) pour compatibilit√©
   * Collecte tous les chunks et retourne la r√©ponse finale
   */
  public async process(userPrompt: string): Promise<string> {
    let finalContent = '';
    
    for await (const chunk of this.processStream(userPrompt)) {
      if (chunk.type === 'fusion' && chunk.content) {
        finalContent = chunk.content;
      } else if (chunk.type === 'primary' && chunk.content) {
        // Accumuler le contenu primaire si pas de fusion
        finalContent += chunk.content;
      }
    }

    return finalContent;
  }

  /**
   * Ex√©cute une t√¢che avec timeout et gestion d'erreurs robuste
   */
  private async executeTaskWithTimeout(task: Task, userPrompt: string): Promise<TaskResult> {
    const timeoutMs = task.timeout;
    const controller = new AbortController();
    const startTime = performance.now();

    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => {
        controller.abort();
        reject(new Error(`Timeout apr√®s ${timeoutMs}ms`));
      }, timeoutMs);
    });

    try {
      this.activeWorkers++;
      console.log(`   [Worker ${this.activeWorkers}] ‚ñ∂Ô∏è  ${task.agentName} d√©marr√© (${this.queue.pending} en attente)`);
      
      const result = await Promise.race([
        this.executeTask(task, userPrompt, controller.signal),
        timeoutPromise
      ]);
      
      this.activeWorkers--;
      return result;
      
    } catch (error) {
      this.activeWorkers--;
      const duration = performance.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      console.error(`   [Worker] ‚ùå ${task.agentName} √©chou√© apr√®s ${duration.toFixed(0)}ms:`, errorMessage);
      
      return { 
        agentName: task.agentName,
        modelKey: task.modelKey,
        error: errorMessage, 
        status: controller.signal.aborted ? 'timeout' : 'error',
        duration
      };
    }
  }

  /**
   * Ex√©cute une seule t√¢che (appel√© par executeTaskWithTimeout)
   */
  private async executeTask(task: Task, userPrompt: string, signal: AbortSignal): Promise<TaskResult> {
    const engine = await modelManager.getEngine();
    const startTime = performance.now();
    
    // Charger le mod√®le si n√©cessaire
    if (!modelManager.isModelLoaded(task.modelKey)) {
      console.log(`   [Worker] üîÑ Chargement du mod√®le ${task.modelKey}...`);
      await modelManager.switchModel(task.modelKey);
    }

    // Utiliser le prompt de la t√¢che ou le prompt utilisateur
    const prompt = task.prompt || userPrompt;

    const response = await engine.chat.completions.create({
      messages: [{ role: 'user', content: prompt }],
      temperature: task.temperature
    });

    if (signal.aborted) {
      throw new Error('Task aborted');
    }

    const content = response.choices[0]?.message?.content || "";
    const duration = performance.now() - startTime;
    
    console.log(`   [Worker] ‚úÖ ${task.agentName} termin√© (${duration.toFixed(0)}ms, ${content.length} chars)`);
    
    return { 
      agentName: task.agentName,
      modelKey: task.modelKey,
      result: content, 
      status: 'success',
      duration
    };
  }

  /**
   * D√©termine la limite de concurrence selon la strat√©gie
   */
  private getConcurrencyLimit(strategy: ExecutionStrategy): number {
    switch (strategy) {
      case 'SERIAL': 
        return 1;
      case 'PARALLEL_LIMITED': 
        return 2;
      case 'PARALLEL_FULL': 
        return 4;
      default: 
        return 1;
    }
  }

  /**
   * Convertit une priorit√© en valeur num√©rique pour p-queue
   * Plus le nombre est √©lev√©, plus la priorit√© est haute
   */
  private getPriorityValue(priority: string): number {
    switch (priority) {
      case 'HIGH': 
        return 10;
      case 'MEDIUM': 
        return 5;
      case 'LOW': 
        return 1;
      default: 
        return 1;
    }
  }

  /**
   * Observabilit√© : Nombre de workers actifs
   */
  public getActiveWorkerCount(): number {
    return this.activeWorkers;
  }

  /**
   * Observabilit√© : Taille de la file d'attente
   */
  public getQueueSize(): number {
    return this.queue.size;
  }

  /**
   * Observabilit√© : Nombre de t√¢ches en attente
   */
  public getPendingCount(): number {
    return this.queue.pending;
  }

  /**
   * Observabilit√© : Statistiques compl√®tes
   */
  public getStats() {
    return {
      activeWorkers: this.activeWorkers,
      queueSize: this.queue.size,
      pending: this.queue.pending,
      concurrency: this.queue.concurrency
    };
  }
}

// Instance singleton export√©e
export const taskExecutor = new TaskExecutor();
