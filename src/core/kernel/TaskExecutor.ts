/**
 * TaskExecutor v3.0 - Chef de Chantier Intelligent
 * 
 * ARCHITECTURE MULTI-QUEUE (FINALE):
 * ‚úÖ Queue s√©par√©e par strat√©gie (respect strict de concurrence)
 * ‚úÖ Streaming enti√®rement dans le job PQueue
 * ‚úÖ Vraie cancellation via engine.interruptGenerate()
 * ‚úÖ Callback pattern pour envoyer les chunks en temps r√©el
 * 
 * Strat√©gies:
 * - SERIAL: 1 t√¢che √† la fois
 * - PARALLEL_LIMITED: max 2 t√¢ches simultan√©es  
 * - PARALLEL_FULL: max 4 t√¢ches simultan√©es
 */

import PQueue from 'p-queue';
import { modelManager } from './ModelManager';
import { Router } from '../router/Router';
import { 
  ExecutionPlan, 
  Task, 
  TaskResult, 
  StreamChunk, 
  ExecutionStrategy
} from '../router/RouterTypes';
import { fusioner } from './Fusioner';
import { responseCache } from '../cache/ResponseCache';
import { sseStreamer } from '../streaming/SSEStreamer';

console.log("üë∑ TaskExecutor v3.1 - Chef de Chantier Intelligent (Cache-Aware + Streaming)");

export class TaskExecutor {
  private router: Router;
  private queueSerial: PQueue;
  private queueParallelLimited: PQueue;
  private queueParallelFull: PQueue;
  private activeRequests = 0;

  constructor() {
    this.router = new Router();
    
    // Queue pour SERIAL: 1 t√¢che √† la fois
    this.queueSerial = new PQueue({ 
      concurrency: 1,
      timeout: 120000
    });
    
    // Queue pour PARALLEL_LIMITED: max 2 t√¢ches
    this.queueParallelLimited = new PQueue({ 
      concurrency: 2,
      timeout: 120000
    });
    
    // Queue pour PARALLEL_FULL: max 4 t√¢ches
    this.queueParallelFull = new PQueue({ 
      concurrency: 4,
      timeout: 120000
    });
  }

  /**
   * Obtient la queue appropri√©e selon la strat√©gie
   */
  private getQueue(strategy: ExecutionStrategy): PQueue {
    switch (strategy) {
      case 'SERIAL':
        return this.queueSerial;
      case 'PARALLEL_LIMITED':
        return this.queueParallelLimited;
      case 'PARALLEL_FULL':
        return this.queueParallelFull;
      default:
        return this.queueSerial;
    }
  }

  /**
   * Process avec streaming pour UX optimale
   */
  public async *processStream(userPrompt: string): AsyncGenerator<StreamChunk> {
    this.activeRequests++;
    const requestId = crypto.randomUUID().substring(0, 8);
    console.log(`[TaskExecutor] üöÄ Requ√™te #${requestId} (${this.activeRequests} active(s))`);

    try {
      // ‚ú® 0. Obtenir le plan du Router AVANT de v√©rifier le cache
      const plan = await this.router.createPlan(userPrompt);
      console.log(`[TaskExecutor #${requestId}] üìã Plan: ${plan.strategy}, ${plan.fallbackTasks.length + 1} t√¢che(s)`);
      await this.router.validatePlan(plan);

      // ‚ú® 0.5 V√©rifier le cache AVANT d'ex√©cuter
      const primaryModelKey = plan.primaryTask.modelKey;
      const cached = responseCache.get(userPrompt, primaryModelKey);
      if (cached) {
        console.log(`[TaskExecutor #${requestId}] ‚ö° Cache HIT - R√©ponse trouv√©e.`);
        sseStreamer.streamInfo(`Response found in cache.`);
        // Streamer la r√©ponse cach√©e comme si elle venait juste de la GPU
        for (const char of cached.response) {
          yield { type: 'primary', content: char };
        }
        yield {
          type: 'fusion',
          content: cached.response,
          expertResults: [{ agentName: 'cache', modelKey: primaryModelKey, result: cached.response, status: 'success' }]
        };
        return;
      }

      sseStreamer.streamInfo(`Processing request...`);

      // 2. S√©lectionner la queue appropri√©e
      const queue = this.getQueue(plan.strategy);
      console.log(`[TaskExecutor #${requestId}] ‚öôÔ∏è  Strat√©gie: ${plan.strategy}`);
      sseStreamer.streamInfo(`Using strategy: ${plan.strategy}`);

      // 3. Buffer pour les chunks stream√©s
      const chunks: StreamChunk[] = [];
      const onChunk = (chunk: StreamChunk) => {
        chunks.push(chunk);
      };

      // 4. Ex√©cuter la t√¢che primaire via la queue (avec streaming)
      const primaryPromise = queue.add(
        () => this.executeStreamingTaskInQueue(plan.primaryTask, userPrompt, onChunk),
        { priority: 100 }
      );

      // 5. Ex√©cuter les t√¢ches fallback via la m√™me queue
      const fallbackPromises = plan.fallbackTasks.map((task) =>
        queue.add(
          () => this.executeTaskWithTimeout(task, userPrompt),
          { priority: this.getPriorityValue(task.priority) }
        )
      );

      // 6. Polling: streamer les chunks pendant que les jobs tournent
      let primaryResult: TaskResult | null = null;
      let lastIndex = 0;
      const pollInterval = 50;

      while (!primaryResult) {
        // Envoyer les nouveaux chunks
        for (let i = lastIndex; i < chunks.length; i++) {
          yield chunks[i];
          lastIndex = i + 1;
        }

        // V√©rifier si le job primaire est termin√© (avec timeout)
        try {
          primaryResult = await Promise.race([
            primaryPromise,
            new Promise<null>(resolve => 
              setTimeout(() => resolve(null), pollInterval)
            )
          ]);
        } catch (e) {
          // Le job a √©chou√©, on le saura au prochain tour
        }
      }

      // 7. Envoyer les chunks restants
      for (let i = lastIndex; i < chunks.length; i++) {
        yield chunks[i];
      }

      // 8. Attendre les t√¢ches fallback
      let expertResults: TaskResult[] = [];
      if (fallbackPromises.length > 0) {
        console.log(`[TaskExecutor #${requestId}] ‚è≥ Attente de ${fallbackPromises.length} fallback(s)...`);
        expertResults = await Promise.all(fallbackPromises);
        
        const successCount = expertResults.filter(r => r.status === 'success').length;
        console.log(`[TaskExecutor #${requestId}] üìä Fallback: ${successCount}/${expertResults.length}`);
      }

      // 9. Fusionner et envoyer le r√©sultat final
      const finalResponse = await fusioner.fuse({
        primaryResult: primaryResult!,
        expertResults
      });

      // ‚ú® Mettre en cache le r√©sultat AVANT de l'envoyer
      if (primaryResult) {
        responseCache.set(userPrompt, primaryModelKey, finalResponse, chunks.length);
        console.log(`[TaskExecutor #${requestId}] üíæ R√©sultat mis en cache.`);
      }

      yield { 
        type: 'fusion', 
        content: finalResponse,
        expertResults 
      };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Erreur inconnue';
      console.error(`[TaskExecutor] ‚ùå Erreur:`, error);
      
      // ‚ú® Notifier l'UI de l'erreur via SSE
      sseStreamer.streamError(error instanceof Error ? error : new Error(errorMessage));
      
      yield {
        type: 'status',
        status: `Erreur: ${errorMessage}`
      };
      
      throw error;
    } finally {
      this.activeRequests--;
    }
  }

  /**
   * Process classique (sans streaming)
   */
  public async process(userPrompt: string): Promise<string> {
    let finalContent = '';
    
    for await (const chunk of this.processStream(userPrompt)) {
      if (chunk.type === 'fusion' && chunk.content) {
        finalContent = chunk.content;
      }
    }

    return finalContent;
  }

  /**
   * Ex√©cute une t√¢che avec streaming EN ENTIER dans le job PQueue
   * Utilise Transformers.js pour g√©n√©rer du texte
   */
  private async executeStreamingTaskInQueue(
    task: Task,
    userPrompt: string,
    onChunk: (chunk: StreamChunk) => void
  ): Promise<TaskResult> {
    const startTime = performance.now();
    let timedOut = false;
    const timeoutMs = task.timeout;

    // Setup timeout
    const timeoutId = setTimeout(() => {
      timedOut = true;
      console.warn(`   [Worker] ‚è±Ô∏è  Timeout ${task.agentName}`);
    }, timeoutMs);

    try {
      console.log(`   [Worker] ‚ñ∂Ô∏è  ${task.agentName} d√©marr√©`);
      
      // ‚ú® Notifier l'UI via SSE
      sseStreamer.streamInfo(`Executing ${task.agentName}...`);
      onChunk({ type: 'status', status: `Ex√©cution de ${task.agentName}...` });

      const prompt = task.prompt || userPrompt;
      let fullResponse = "";

      // Streaming de la g√©n√©ration via Transformers.js
      const onToken = (token: string) => {
        if (!timedOut) {
          fullResponse += token;
          onChunk({ type: 'primary', content: token });
        }
      };

      // G√©n√©rer via ModelManager
      fullResponse = await modelManager.generateStreaming(
        prompt,
        onToken,
        256
      );

      clearTimeout(timeoutId);

      const duration = performance.now() - startTime;
      const status = timedOut ? 'timeout' : 'success';
      
      console.log(`   [Worker] ‚úÖ ${task.agentName} ${status} (${duration.toFixed(0)}ms)`);
      
      // Mettre en cache
      if (!timedOut) {
        responseCache.set(userPrompt, task.modelKey, fullResponse);
      }
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        result: fullResponse,
        status,
        duration
      };

    } catch (error) {
      clearTimeout(timeoutId);
      const duration = performance.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        error: {
          type: 'UnknownError' as const,
          message: errorMessage
        },
        status: timedOut ? 'timeout' : 'error',
        duration
      };
    }
  }

  /**
   * Ex√©cute une t√¢che SANS streaming (pour fallback)
   */
  private async executeTaskWithTimeout(task: Task, userPrompt: string): Promise<TaskResult> {
    const startTime = performance.now();
    const timeoutMs = task.timeout;
    let timedOut = false;

    const timeoutId = setTimeout(() => {
      timedOut = true;
      console.warn(`   [Worker] ‚è±Ô∏è  Timeout ${task.agentName}`);
    }, timeoutMs);

    try {
      console.log(`   [Worker] ‚ñ∂Ô∏è  ${task.agentName} d√©marr√©`);
      
      const prompt = task.prompt || userPrompt;
      let fullResponse = "";

      // G√©n√©ration sans streaming via Transformers.js
      const onToken = (token: string) => {
        fullResponse += token;
      };

      fullResponse = await modelManager.generateStreaming(
        prompt,
        onToken,
        256
      );

      clearTimeout(timeoutId);
      const duration = performance.now() - startTime;
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        result: fullResponse,
        status: timedOut ? 'timeout' : 'success',
        duration
      };

    } catch (error) {
      clearTimeout(timeoutId);
      const duration = performance.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : String(error);
      
      return {
        agentName: task.agentName,
        modelKey: task.modelKey,
        error: {
          type: 'UnknownError' as const,
          message: errorMessage
        },
        status: timedOut ? 'timeout' : 'error',
        duration
      };
    }
  }

  private getPriorityValue(priority: string): number {
    switch (priority) {
      case 'HIGH': return 10;
      case 'MEDIUM': return 5;
      case 'LOW': return 1;
      default: return 1;
    }
  }

  public getActiveRequestCount(): number {
    return this.activeRequests;
  }

  public getQueueStats() {
    return {
      activeRequests: this.activeRequests,
      serialQueue: { pending: this.queueSerial.pending, concurrency: this.queueSerial.concurrency },
      parallelLimitedQueue: { pending: this.queueParallelLimited.pending, concurrency: this.queueParallelLimited.concurrency },
      parallelFullQueue: { pending: this.queueParallelFull.pending, concurrency: this.queueParallelFull.concurrency }
    };
  }

  /**
   * Retry avec backoff exponentiel
   * Strat√©gie: 3 tentatives max, d√©lai: 100ms, 300ms, 900ms
   */
  public async processWithRetry(
    userPrompt: string,
    maxRetries = 3,
    initialBackoffMs = 100
  ): Promise<string> {
    let lastError: Error | null = null;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        console.log(`[TaskExecutor] Tentative ${attempt}/${maxRetries}`);
        return await this.process(userPrompt);
      } catch (error) {
        lastError = error instanceof Error ? error : new Error(String(error));
        
        if (attempt < maxRetries) {
          const backoffMs = initialBackoffMs * Math.pow(3, attempt - 1);
          console.warn(
            `[TaskExecutor] Tentative ${attempt} √©chou√©e, retry dans ${backoffMs}ms`
          );
          await new Promise(resolve => setTimeout(resolve, backoffMs));
        }
      }
    }

    throw new Error(`Failed after ${maxRetries} retries: ${lastError?.message}`);
  }
}

export const taskExecutor = new TaskExecutor();
